{
  "hash": "d42dd640f7faf07bafa4f7c3e7d4169d",
  "result": {
    "markdown": "---\ntitle: \"Numbers on a computer\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-10-12\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\nengine: knitr\n---\n\n\n\n[PDF](./unit8-numbers.pdf){.btn .btn-primary}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nReferences:\n\n-   Gentle, Computational Statistics, Chapter 2.\n-   [http://www.lahey.com/float.htm](http://www.lahey.com/float.htm)\n-   And for more gory detail, see Monahan, Chapter 2.\n\nA quick note that, as we've already seen, Python's version of scientific\nnotation is `XeY`, which means $X\\cdot10^{Y}$.\n\nA second note is that the concepts developed here apply outside of Python,\nbut we'll illustrate the principles of computer numbers using Python.\nPython usually makes use of the *double* type (8 bytes) in C for the underlying\nrepresentation of real-valued numbers in C variables, so what we'll really be\nseeing is how such types behave in C on most modern machines.\nIt's actually a bit more complicated in that one can use real-valued numbers\nthat use something other than 8 bytes in numpy by specifying a `dtype`.\n\nThe handling of integers is even more complicated. In numpy, the default\nis 8 byte integers, but other integer dtypes are available. And in Python\nitself, integers can be arbitrarily large.\n\n# 1. Basic representations\n\nEverything in computer memory or on disk is stored in terms of bits. A\n*bit* is essentially a switch than can be either on or off. Thus\neverything is encoded as numbers in base 2, i.e., 0s and 1s. 8 bits make\nup a *byte*. As discussed in Unit 2, for information stored as plain text (ASCII), each byte is\nused to encode a single character (as previously discussed, actually only 7 of the 8 bits are\nactually used, hence there are $2^{7}=128$ ASCII characters). One way to\nrepresent a byte is to write it in hexadecimal, rather than as 8 0/1\nbits. Since there are $2^{8}=256$ possible values in a byte, we can\nrepresent it more compactly as 2 base-16 numbers, such as \"3e\" or \"a0\"\nor \"ba\". A file format is nothing more than a way of interpreting the\nbytes in a file.\n\n\nWe'll create some helper functions to all us to look\nat the underlying binary representation.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom bitstring import Bits\n\ndef bits(x):\n    obj = Bits(float = x, length = 64)\n    return(obj.bin)\n\ndef dg(x, form = '.20f'):\n    print(format(x, form))\n```\n:::\n\n\n\nNote that 'b' is encoded as one\nmore than 'a', and similarly for '0', '1', and '2'.\nWe could check these against, say, the Wikipedia\ntable that shows the [ASCII encoding](https://en.wikipedia.org/wiki/ASCII).\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nBits(bytes=b'a').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01100001'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'b').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01100010'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'0').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110000'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'1').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110001'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'2').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110010'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'@').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01000000'\n```\n:::\n:::\n\n\n\n\nWe can think about how we'd store an integer in terms of bytes. With two\nbytes (16 bits), we could encode any value from $0,\\ldots,2^{16}-1=65535$. This is\nan *unsigned* integer representation. To store negative numbers as well,\nwe can use one bit for the sign, giving us the ability to encode\n-32767 - 32767 ($\\pm2^{15}-1$).\n\nNote that in general, rather than be stored\nsimply as the sign and then a number in base 2, integers (at least the\nnegative ones) are actually stored in different binary encoding to\nfacilitate arithmetic. \n\nHere's what a 64-bit integer representation \nthe actual bits.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.binary_repr(0, width=64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(1, width=64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000001'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(2, width=64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000010'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(-1, width=64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'1111111111111111111111111111111111111111111111111111111111111111'\n```\n:::\n:::\n\n\n\nWhat do I mean about facilitating arithmetic? As an example, consider adding\nthe binary representations of -1 and 1. Nice, right?\n\n\nFinally note that the set of computer integers is not closed under\narithmetic. We get an overflow (i.e., a result that is too\nlarge to be stored as an integer of the particular length):\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = np.int32(3423333)\na * a       # overflows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-1756921895\n\n<string>:1: RuntimeWarning: overflow encountered in int_scalars\n```\n:::\n\n```{.python .cell-code}\na = np.int64(3423333)\na * a       # doesn't overflow if we use 64 bit int\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11719208828889\n```\n:::\n\n```{.python .cell-code}\na = np.int64(34233332342343)\na * a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1001093889201452977\n\n<string>:1: RuntimeWarning: overflow encountered in long_scalars\n```\n:::\n:::\n\n\n\nThat said, if we use Python's `int` rather than numpy's integers,\nwe don't get overflow. But we do use more than 8 bytes that would be used\nby numpy.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = 34233332342343\na * a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1171921043261307270950729649\n```\n:::\n\n```{.python .cell-code}\nsys.getsizeof(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n32\n```\n:::\n\n```{.python .cell-code}\nsys.getsizeof(a*a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n36\n```\n:::\n:::\n\n\n\n\nIn C, one generally works with 8 byte real-valued numbers (aka *floating point* numbers or *floats*).\nHowever, many years ago, an initial standard representation used 4 bytes. Then\npeople started using 8 bytes, which became known as *double precision floating points*\nor *doubles*, whereas the 4-byte version became known as *single precision*.\nNow with GPUs, single precision is often used for speed and reduced memory use.\n\nLet's see how this plays out in terms of memory use in Python.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 100000)\nsys.getsizeof(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n800112\n```\n:::\n\n```{.python .cell-code}\nx = np.array(np.random.normal(size = 100000), dtype = \"float32\")\nsys.getsizeof(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n400112\n```\n:::\n\n```{.python .cell-code}\nx = np.array(np.random.normal(size = 100000), dtype = \"float16\")  \nsys.getsizeof(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n200112\n```\n:::\n:::\n\n\n\n\nWe can easily calculate the number of megabytes (MB) a vector of\nfloating points (in double precision) will use as the number of elements\ntimes 8 (bytes/double) divided by $10^{6}$ to convert from bytes to\nmegabytes. (In some cases when considering computer memory, a megabyte\nis $1,048,576=2^{20}=1024^{2}$ bytes (this is formally called a\n*mebibyte*) so slightly different than $10^{6}$ -- see [here for more\ndetails](https://en.wikipedia.org/wiki/Megabyte)).\n\nFinally, `numpy` has some helper functions that can tell us\nabout the characteristics of computer\nnumbers on the machine that Python is running.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.iinfo(np.int32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niinfo(min=-2147483648, max=2147483647, dtype=int32)\n```\n:::\n\n```{.python .cell-code}\nnp.iinfo(np.int64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(2147483647, width=32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01111111111111111111111111111111'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(-2147483648, width=32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'10000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(2147483648, width=32)  # strange\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'10000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(1, width=32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00000000000000000000000000000001'\n```\n:::\n\n```{.python .cell-code}\nnp.binary_repr(-1, width=32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'11111111111111111111111111111111'\n```\n:::\n:::\n\n\n\nSo the max for a 32-bit (4-byte) integer is $2147483647=2^{31}-1$, which\nis consistent with 4 bytes.  Since we have both negative and\npositive numbers, we have $2\\cdot2^{31}=2^{32}=(2^{8})^{4}$, i.e., 4\nbytes, with each byte having 8 bits.\n\n\n# 2. Floating point basics\n\n## Representing real numbers\n\n### Initial exploration\n\nReals (also called floating points) are stored on the computer as an\napproximation, albeit a very precise approximation. As an example, if we\nrepresent the distance from the earth to the sun using a double, the\nerror is around a millimeter. However, we need to be very careful if\nwe're trying to do a calculation that produces a very small (or very\nlarge number) and particularly when we want to see if numbers are equal\nto each other.\n\nIf you run the code here, the results may surprise you.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n0.3 - 0.2 == 0.1\n0.3\n0.2\n0.1 # Hmmm...\n\nnp.float64(0.3) - np.float64(0.2) == np.float64(0.1)\n\n0.75 - 0.5 == 0.25\n0.6 - 0.4 == 0.2\n## any ideas what is different about those two comparisons?\n```\n:::\n\n\n\nNext, let's consider the number of digits of accuracy\nwe have for a variety of numbers. We'll use `format` within\na handy wrapper function, `dg`, defined earlier, to view as many digits as we want:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = 0.3\nb = 0.2\ndg(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.29999999999999998890\n```\n:::\n\n```{.python .cell-code}\ndg(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.20000000000000001110\n```\n:::\n\n```{.python .cell-code}\ndg(a-b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.09999999999999997780\n```\n:::\n\n```{.python .cell-code}\ndg(0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.10000000000000000555\n```\n:::\n\n```{.python .cell-code}\ndg(1/3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.33333333333333331483\n```\n:::\n:::\n\n\n\nSo empirically, it looks like we're accurate up to the 16th decimal place\n\nBut actually, the key is the number of digits, not decimal places.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1234.1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12339999999994688551\n```\n:::\n\n```{.python .cell-code}\ndg(1234.123412341234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12341234123391586763\n```\n:::\n:::\n\n\n\nNotice that we can represent the result accurately only up to 16\nsignificant digits. This suggests no need to show more than 16\nsignificant digits and no need to print out any more when writing to a\nfile (except that if the number is bigger than $10^{16}$ then we need\nextra digits to correctly show the magnitude of the number if not using\nscientific notation). And of course, often we don't need anywhere near\nthat many.\n\nLet's return to our comparison, `0.75-0.5 == 0.25`.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.75000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(0.50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.50000000000000000000\n```\n:::\n:::\n\n\n\nWhat's different about the numbers 0.75 and 0.5 compared to 0.3, 0.2,\n0.1?\n\n### Machine epsilon\n\n\n*Machine epsilon* is the term used for indicating the\n(relative) accuracy of real numbers and it is defined as the smallest\nfloat, $x$, such that $1+x\\ne1$:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n1e-16 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n\n```{.python .cell-code}\nnp.array(1e-16) + np.array(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n\n```{.python .cell-code}\n1e-15 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.000000000000001\n```\n:::\n\n```{.python .cell-code}\nnp.array(1e-15) + np.array(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.000000000000001\n```\n:::\n\n```{.python .cell-code}\n2e-16 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0000000000000002\n```\n:::\n\n```{.python .cell-code}\nnp.finfo(np.float64).eps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.220446049250313e-16\n```\n:::\n\n```{.python .cell-code}\ndg(2e-16 + 1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.00000000000000022204\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n## What about in single precision, e.g. on a GPU?\nnp.finfo(np.float32).eps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.1920929e-07\n```\n:::\n:::\n\n\n\n\n### Floating point representation\n\n*Floating point* refers to the decimal point (or *radix* point since we'll\nbe working with base 2 and *decimal* relates to 10). \n\nTo proceed further we need to consider scientific notation, such as in writing Avogadro's\nnumber as $+6.023\\times10^{23}$. As a\nbaseline for what is about to follow note that we can express a decimal\nnumber in the following expansion\n$$6.037=6\\times10^{0}+0\\times10^{-1}+3\\times10^{-2}+7\\times10^{-3}$$ A real number on a\ncomputer is stored in what is basically scientific notation:\n$$\\pm d_{0}.d_{1}d_{2}\\ldots d_{p}\\times b^{e}\\label{eq:floatRep}$$\nwhere $b$ is the base, $e$ is an integer and $d_{i}\\in\\{0,\\ldots,b-1\\}$.\n$e$ is called the *exponent* and $d=d_{1}d_{2}\\ldots d_{p}$ is called the *mantissa*.\n\nLet's consider the choices that the computer pioneers needed to make\nin using this system to represent numbers on a computer using base 2 ($b=2$).\nFirst, we need to choose the number of bits to represent $e$ so that we\ncan represent sufficiently large and small numbers. Second we need to\nchoose the number of bits, $p$, to allocate to \n$d=d_{1}d_{2}\\ldots d_{p}$, which determines the accuracy of any\ncomputer representation of a real.\n\nThe great thing about floating points\nis that we can represent numbers that range from incredibly small to\nvery large while maintaining good precision. The floating point *floats*\nto adjust to the size of the number. Suppose we had only three digits to\nuse and were in base 10. In floating point notation we can express\n$0.12\\times0.12=0.0144$ as\n$(1.20\\times10^{-1})\\times(1.20\\times10^{-1})=1.44\\times10^{-2}$, but if\nwe had fixed the decimal point, we'd have $0.120\\times0.120=0.014$ and\nwe'd have lost a digit of accuracy. (Furthermore, we wouldn't be able\nto represent numbers bigger than $0.99$.)\n\nMore specifically, the actual storage of a number on a computer these\ndays is generally as a double in the form:\n$$(-1)^{S}\\times1.d\\times2^{e-1023}=(-1)^{S}\\times1.d_{1}d_{2}\\ldots d_{52}\\times2^{e-1023}$$\nwhere the computer uses base 2, $b=2$, (so $d_{i}\\in\\{0,1\\}$) because\nbase-2 arithmetic is faster than base-10 arithmetic. The leading 1\nnormalizes the number; i.e., ensures there is a unique representation\nfor a given computer number. This avoids representing any number in\nmultiple ways, e.g., either\n$1=1.0\\times2^{0}=0.1\\times2^{1}=0.01\\times2^{2}$. For a double, we have\n8 bytes=64 bits. Consider our representation as ($S,d,e$) where $S$ is\nthe sign. The leading 1 is the *hidden bit* and doesn't need to be\nstored because it is always present. In general $e$ is\nrepresented using 11 bits ($2^{11}=2048$), and the subtraction takes the\nplace of having a sign bit for the exponent. (Note that in our\ndiscussion we'll just think of $e$ in terms of its base 10\nrepresentation, although it is of course represented in base 2.) This\nleaves $p=52 = 64-1-11$ bits for $d$.\n\nIn this code I force storage as a double by tacking on a decimal place, `.0`.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbits(2.0**(-1)) # 1/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111111100000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**0)  # 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111111110000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**1)  # 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**1 + 2.0**0)  # 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000001000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**2)  # 4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(-2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'1100000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n:::\n\n\n\nLet's see that we can manually work out the bit-wise representation\nof 3.25:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbits(3.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000001010000000000000000000000000000000000000000000000000'\n```\n:::\n:::\n\n\n\nSo that is $1.101 \\times 2^{1024-1023} = 1\\times 2^{1} + 1\\times 2^{0} + 1\\times 2^{-2}$, where the 2nd through 12th\nbits are $10000000000$, which code for $1\\times 2^{10}=1024$.\n\n**Question**: Given a fixed number of bits for a number, what is the\ntradeoff between using bits for the $d$ part vs. bits for the $e$ part?\n\nLet's consider what can be represented exactly:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.10000000000000000555\n```\n:::\n\n```{.python .cell-code}\ndg(.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.50000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.25000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(.26)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.26000000000000000888\n```\n:::\n\n```{.python .cell-code}\ndg(1/32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03125000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(1/33)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03030303030303030387\n```\n:::\n:::\n\n\n\nSo why is 0.5 stored exactly and 0.1 not stored exactly? By analogy,\nconsider the difficulty with representing 1/3 in base 10.\n\n## Overflow and underflow\n\nThe largest and smallest numbers we can represent are $2^{e_{\\max}}$ and\n$2^{e_{\\min}}$ where $e_{\\max}$ and $e_{\\min}$ are the smallest and\nlargest possible values of the exponent. Let's consider the exponent and\nwhat we can infer about the range of possible numbers. With 11 bits for\n$e$, we can represent $\\pm2^{10}=\\pm1024$ different exponent values (see\n`np.finfo(np.float64).maxexp`) (why is `np.finfo(np.float64).minexp` only\n-1022?). So the largest number we could represent is $2^{1024}$. What\nis this in base 10?\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.float64(10)\nx**308\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1e+308\n```\n:::\n\n```{.python .cell-code}\nx**309\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninf\n\n<string>:1: RuntimeWarning: overflow encountered in double_scalars\n```\n:::\n\n```{.python .cell-code}\nnp.log10(2.0**1024)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: OverflowError: (34, 'Numerical result out of range')\n```\n:::\n\n```{.python .cell-code}\nnp.log10(2.0**1023)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n307.95368556425274\n```\n:::\n\n```{.python .cell-code}\nnp.finfo(np.float64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n```\n:::\n:::\n\n\n\nWe could have been smarter about that calculation:\n$\\log_{10}2^{1024}=\\log_{2}2^{1024}/\\log_{2}10=1024/3.32\\approx308$. The\nresult is analogous for the smallest number, so we have that floating\npoints can range between $1\\times10^{-308}$ and $1\\times10^{308}$,\nconsistent with what numpy reports above. Producing\nsomething larger or smaller in magnitude than these values is called\noverflow and underflow respectively.\n\nLet's see what happens when we underflow in numpy. Note that there is no warning.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx**(-308)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1e-308\n```\n:::\n\n```{.python .cell-code}\nx**(-330)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0\n```\n:::\n:::\n\n\n\nSomething subtle happens for numbers like $10^{-309}$ through $10^{-323}$. They can actually be represented despite what I said above. Investigating that may be an extra credit problem on a problem set.\n\n\n## Integers or floats?\n\nValues stored as integers should overflow if they exceed the maximum integer.\n\nShould $2^{65}$ overflow?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.log2(np.iinfo(np.int64).max)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n63.0\n```\n:::\n\n```{.python .cell-code}\nx = np.int64(2)\n# Yikes!\nx**64\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n```\n:::\n:::\n\n\n\nPython's `int` type doesn't overflow.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Interesting:\nprint(2**64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18446744073709551616\n```\n:::\n\n```{.python .cell-code}\nprint(2**100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1267650600228229401496703205376\n```\n:::\n:::\n\n\n\nOf course, doubles won't overflow until much larger values than 4- or 8-byte integers because we know they can be as big as $10^308$.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.float64(2)\ndg(x**64, '.2f')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18446744073709551616.00\n```\n:::\n\n```{.python .cell-code}\ndg(x**100, '.2f')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1267650600228229401496703205376.00\n```\n:::\n:::\n\n\n\nHowever we need to think about\nwhat integer-valued numbers can and can't be stored exactly in our base 2 representation of floating point numbers.\nIt turns out that integer-valued numbers can be stored exactly as doubles when their absolute\nvalue is less than $2^{53}$.\n\n> *Challenge*: Why $2^{53}$? Write out what integers can be stored exactly in our base 2 representation of floating point numbers.\n\nYou can force storage as integers or doubles in a few ways.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3; type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'int'>\n```\n:::\n\n```{.python .cell-code}\nx = np.float64(x); type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.float64'>\n```\n:::\n\n```{.python .cell-code}\nx = 3.0; type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'float'>\n```\n:::\n\n```{.python .cell-code}\nx = np.float64(3); type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.float64'>\n```\n:::\n:::\n\n\n\n## Precision\n\nConsider our representation as (*S, d, e*) where we have $p=52$ bits for\n$d$. Since we have $2^{52}\\approx0.5\\times10^{16}$, we can represent\nabout that many discrete values, which means we can accurately represent\nabout 16 digits (in base 10). The result is that floats on a computer\nare actually discrete (we have a finite number of bits), and if we get a\nnumber that is in one of the gaps (there are uncountably many reals),\nit's approximated by the nearest discrete value. The accuracy of our\nrepresentation is to within 1/2 of the gap between the two discrete\nvalues bracketing the true number. Let's consider the implications for\naccuracy in working with large and small numbers. By changing $e$ we can\nchange the magnitude of a number. So regardless of whether we have a\nvery large or small number, we have about 16 digits of accuracy, since\nthe absolute spacing depends on what value is represented by the least\nsignificant digit (the *ulp*, or *unit in the last place*) in $d$, i.e.,\nthe $p=52$nd one, or in terms of base 10, the 16th digit. Let's explore\nthis:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# large vs. small numbers\ndg(.1234123412341234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12341234123412339607\n```\n:::\n\n```{.python .cell-code}\ndg(1234.1234123412341234) # not accurate to 16 decimal places \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12341234123414324131\n```\n:::\n\n```{.python .cell-code}\ndg(123412341234.123412341234) # only accurate to 4 places \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n123412341234.12341308593750000000\n```\n:::\n\n```{.python .cell-code}\ndg(1234123412341234.123412341234) # no places! \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234123412341234.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12341234123412341234) # fewer than no places! \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n12341234123412340736.00000000000000000000\n```\n:::\n:::\n\n\n\nWe can see the implications of this in the context of calculations:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1234567812345678.0 - 1234567812345677.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12345678123456788888.0 - 12345678123456788887.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12345678123456780000.0 - 12345678123456770000.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10240.00000000000000000000\n```\n:::\n:::\n\n\n\nThe spacing of possible computer numbers that have a magnitude of about\n1 leads us to another definition of *machine epsilon* (an alternative,\nbut essentially equivalent definition to that given [previously](#machine-epsilon). \nMachine epsilon tells us also about the relative spacing of\nnumbers.\n\nFirst let's consider numbers of magnitude one. The next biggest number we can represent after  $1=1.00...00\\times2^{0}$ is $1.000...01\\times2^{0}$. The difference between those two numbers (i.e., the spacing) is\n$$\n\\begin{aligned}\n\\epsilon & = &0.00...01 \\times 2^{0} \\\\\n & =& 0 \\times 2^{0} + 0 \\times 2^{-1} + \\cdots + 0\\times 2^{-51} + 1\\times2^{-52}\\\\\n & =& 1\\times2^{-52}\\\\\n & \\approx & 2.2\\times10^{-16}.\n \\end{aligned}\n $$\n\n\nMachine epsilon gives\nthe *absolute spacing* for numbers near 1 and the *relative spacing* for\nnumbers with a different order of magnitude and therefore a different\nabsolute magnitude of the error in representing a real. The relative\nspacing at $x$ is $$\\frac{(1+\\epsilon)x-x}{x}=\\epsilon$$ since the next\nlargest number from $x$ is given by $(1+\\epsilon)x$.\n\nSuppose $x=1\\times10^{6}$. Then the absolute error in representing a\nnumber of this magnitude is $x\\epsilon\\approx2\\times10^{-10}$. (Actually\nthe error would be one-half of the spacing, but that's a minor\ndistinction.) We can see by looking at the numbers in decimal form,\nwhere we are accurate to the order $10^{-10}$ but not $10^{-11}$. This\nis equivalent to our discussion that we have only 16 digits of accuracy.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1000000.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1000000.09999999997671693563\n```\n:::\n:::\n\n\n\nLet's see what arithmetic we can do exactly with integer-valued numbers stored as\ndoubles and how that relates to the absolute spacing of numbers we've\njust seen:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n2.0**52\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4503599627370496.0\n```\n:::\n\n```{.python .cell-code}\n2.0**52+1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4503599627370497.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740992.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53+1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740992.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53+2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740994.0\n```\n:::\n\n```{.python .cell-code}\ndg(2.0**54)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18014398509481984.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(2.0**54+2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18014398509481984.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(2.0**54+4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18014398509481988.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\nbits(2**53)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**53+1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**53+2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000001'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54+2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54+4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000001'\n```\n:::\n:::\n\n\n\nThe absolute spacing is $x\\epsilon$, so we have spacings of\n$2^{52}\\times2^{-52}=1$, $2^{53}\\times2^{-52}=2$,\n$2^{54}\\times2^{-52}=4$ for numbers of magnitude $2^{52}$, $2^{53}$, and\n$2^{54}$, respectively.\n\nWith a bit more work (e.g., using Mathematica), one can demonstrate that\ndoubles in Python in general are represented as the nearest number that can\nstored with the 64-bit structure we have discussed and that the spacing\nis as we have discussed. The results below show the spacing that\nresults, in base 10, for numbers around 0.1. The numbers Python reports are\nspaced in increments of individual bits in the base 2 representation.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(0.1234567812345678)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456779729\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456781)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456781117\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456782)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456782505\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456783)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456782505\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456784)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456783892\n```\n:::\n\n```{.python .cell-code}\nbits(0.1234567812345678)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010000110'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456781)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010000111'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456782)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001000'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456783)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001000'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456784)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001001'\n```\n:::\n:::\n\n\n\n## Working with higher precision numbers\n\nAs we've seen, Python will automatically work with integers in arbitrary precision.\n(Note that R does not do this -- R uses 4-byte integers, and for many calculations\nit's best to use R's `numeric` type because integers that aren't really large\ncan be expressed exactly.)\n\nFor higher precision floating point numbers you can make use of the `gmpy2`\npackage.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport gmpy2\ngmpy2.get_context().precision=200\ngmpy2.const_pi()\n\n## not sure why this shows ...00004\ngmpy2.mpfr(\".1234567812345678\") \n```\n:::\n\n\n\n\n\n# 3. Implications for calculations and comparisons\n\n## Computer arithmetic is not mathematical arithmetic!\n\nAs mentioned for integers, computer number arithmetic is not closed,\nunlike real arithmetic. For example, if we multiply two computer\nfloating points, we can overflow and not get back another computer\nfloating point. \n\nAnother mathematical concept we should consider here is that computer\narithmetic does not obey the associative and distributive laws, i.e.,\n$(a+b)+c$ may not equal $a+(b+c)$ on a computer and $a(b+c)$ may not be\nthe same as $ab+ac$. Here's an example with multiplication:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nval1 = 1/10; val2 = 0.31; val3 = 0.57\nres1 = val1*val2*val3\nres2 = val3*val2*val1\nres1 == res2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n```{.python .cell-code}\ndg(res1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.01766999999999999821\n```\n:::\n\n```{.python .cell-code}\ndg(res2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.01767000000000000168\n```\n:::\n:::\n\n\n\n## Calculating with integers vs. floating points\n\nIt's important to note that operations with integers are fast and exact\n(but can easily overflow -- albeit not with Python's base `int`) while operations with floating points are\nslower and approximate. Because of this slowness, floating point\noperations (*flops*) dominate calculation intensity and are used as the\nmetric for the amount of work being done - a multiplication (or\ndivision) combined with an addition (or subtraction) is one flop. We'll\ntalk a lot about flops in the unit on linear algebra.\n\n## Comparisons\n\nAs we saw, we should never test `x == y` unless:\n\n  1. `x` and `y` are represented as integers, \n  2. they are integer-valued but stored as doubles that are small enough that they can be stored exactly), or\n  3. they are decimal numbers that have been created in the same way (e.g., `0.4-0.3 == 0.4-0.3` returns `TRUE` but `0.1 == 0.4-0.3` does not). \n\nSimilarly we should be careful\nabout testing `x == 0`. And be careful of greater than/less than\ncomparisons. For example, be careful of `x[ x < 0 ] = np.nan` if what you\nare looking for is values that might be *mathematically* less than zero,\nrather than whatever is *numerically* less than zero.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n4 - 3 == 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n\n```{.python .cell-code}\n4.0 - 3.0 == 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n\n```{.python .cell-code}\n4.1 - 3.1 == 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n```{.python .cell-code}\n0.4-0.3 == 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n```{.python .cell-code}\n0.4-0.3 == 0.4-0.3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n:::\n\n\n\nOne nice approach to checking for approximate equality is to make use of\n*machine epsilon*. If the relative spacing of two numbers is less than\n*machine epsilon*, then for our computer approximation, we say they are\nthe same. Here's an implementation that relies on the absolute spacing\nbeing $x\\epsilon$ (see above).\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 12345678123456781000\ny = 12345678123456782000\n\ndef approx_equal(a,b):\n  if abs(a - b) < np.finfo(np.float64).eps * abs(a + b):\n    print(\"approximately equal\")\n  else:\n    print (\"not equal\")\n\n\napprox_equal(a,b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnot equal\n```\n:::\n\n```{.python .cell-code}\nx = 1234567812345678\ny = 1234567812345677\n\napprox_equal(a,b)   \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnot equal\n```\n:::\n:::\n\n\n\nActually, we probably want to use a number slightly larger than\nmachine epsilon to be safe. \n\nFinally, sometimes we encounter the use of an unusual integer\nas a symbol for missing values. E.g., a datafile might store missing\nvalues as -9999. Testing for this using `==` with floats should generally be\nok:` x [ x == -9999 ] = np.nan`, because integers of this magnitude\nare stored exactly as floating point values. But to be really careful, you can read in as\nan integer or character type and do the assessment before converting to a float.\n\n## Calculations\n\nGiven the limited *precision* of computer numbers, we need to be careful\nwhen in the following two situations.\n\n1.  Subtracting large numbers that are nearly equal (or adding negative\n    and positive numbers of the same magnitude). You won't have the\n    precision in the answer that you would like. How many decimal places\n    of accuracy do we have here?\n\n    \n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    # catastrophic cancellation w/ large numbers\n    dg(123456781234.56 - 123456781234.00)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.55999755859375000000\n    ```\n    :::\n    :::\n\n\n\n    The absolute error in the original numbers here is of the order\n    $\\epsilon x=2.2\\times10^{-16}\\cdot1\\times10^{11}\\approx1\\times10^{-5}=.00001$.\n    While we might think that the result is close to the value 1 and\n    should have error of about machine epsilon, the relevant absolute\n    error is in the original numbers, so we actually only have about\n    five significant digits in our result because we cancel out the\n    other digits.\n\n    This is called *catastrophic cancellation*, because most of the\n    digits that are left represent rounding error -- many of the significant\n    digits have cancelled with each other.\\\n    Here's catastrophic cancellation with small numbers. The right\n    answer here is exactly 0.000000000000000000001234.\n\n    \n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    # catastrophic cancellation w/ small numbers\n    x = .000000000000123412341234\n    y = .000000000000123412340000\n    \n    # So we know the right answer is .000000000000000000001234 exactly.  \n    \n    dg(x-y, '.35f')\n    ## [1] \"0.00000000000000000000123399999315140\"\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.00000000000000000000123399999315140\n    ```\n    :::\n    :::\n\n\n\n    But the result is accurate only to 8 places + 20 = 28 decimal\n    places, as expected from a machine precision-based calculation,\n    since the \"1\" is in the 13th position, after 12 zeroes (12+16=28).\n    Ideally, we would have accuracy to 36 places (16 digits + the 20\n    zeroes), but we've lost 8 digits to catastrophic cancellation.\n\n    It's best to do any subtraction on numbers that are not too large.\n    For example, if we compute the sum of squares in a naive way, we can\n    lose all of the information in the calculation because the\n    information is in digits that are not computed or stored accurately:\n    $$s^{2}=\\sum x_{i}^{2}-n\\bar{x}^{2}$$\n\n    \n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    ## No problem here:\n    x = np.array([-1.0, 0.0, 1.0])\n    n = len(x)\n    np.sum(x**2)-n*np.mean(x)**2 \n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    np.sum((x - np.mean(x))**2)\n    \n    ## Adding/subtracting a constant shouldn't change the result:\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    x = x + 1e8\n    np.sum(x**2)-n*np.mean(x)**2  ## YIKES!\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    np.sum((x - np.mean(x))**2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    :::\n\n\n\n    A good principle to take away is to subtract off a number similar in\n    magnitude to the values (in this case $\\bar{x}$ is obviously ideal)\n    and adjust your calculation accordingly. In general, you can\n    sometimes rearrange your calculation to avoid catastrophic\n    cancellation. Another example involves the quadratic formula for\n    finding a root (p. 101 of Gentle).\n\n2.  Adding or subtracting numbers that are very different in magnitude.\n    The precision will be that of the large magnitude number, since we\n    can only represent that number to a certain absolute accuracy, which\n    is much less than the absolute accuracy of the smaller number:\n\n    \n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    dg(123456781234.2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19999694824218750000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.1)        # truth: 123456781234.1\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.09999084472656250000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.01)       # truth: 123456781234.19\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19000244140625000000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.001)      # truth: 123456781234.199\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19898986816406250000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.0001)     # truth: 123456781234.1999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19989013671875000000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.00001)    # truth: 123456781234.19999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19998168945312500000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.000001)   # truth: 123456781234.199999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19999694824218750000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    123456781234.2 - 0.000001 == 123456781234.2\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    True\n    ```\n    :::\n    :::\n\n\n\n    The larger number in the calculations above is of magnitude\n    $10^{11}$, so the absolute error in representing the larger number\n    is around $1\\times10^{^{-5}}$. Thus in the calculations above we can\n    only expect the answers to be accurate to about $1\\times10^{-5}$. In\n    the last calculation above, the smaller number is smaller than\n    $1\\times10^{-5}$ and so doing the subtraction has had no effect.\n    This is analogous to trying to do $1+1\\times10^{-16}$ and seeing\n    that the result is still 1.\n\n    A work-around when we are adding numbers of very different\n    magnitudes is to add a set of numbers in increasing order. However,\n    if the numbers are all of similar magnitude, then by the time you\n    add ones later in the summation, the partial sum will be much larger\n    than the new term. A (second) work-around to that problem is to add\n    the numbers in a tree-like fashion, so that each addition involves a\n    summation of numbers of similar size.\n\nGiven the limited *range* of computer numbers, be careful when you are:\n\n-   Multiplying or dividing many numbers, particularly large or small\n    ones. **Never take the product of many large or small numbers** as this\n    can cause over- or under-flow. Rather compute on the log scale and\n    only at the end of your computations should you exponentiate. E.g.,\n    $$\\prod_{i}x_{i}/\\prod_{j}y_{j}=\\exp(\\sum_{i}\\log x_{i}-\\sum_{j}\\log y_{j})$$\n\nLet's consider some challenges that illustrate that last concern.\n\n-   Challenge: consider multiclass logistic regression, where you have\n    quantities like this:\n    $$p_{j}=\\text{Prob}(y=j)=\\frac{\\exp(x\\beta_{j})}{\\sum_{k=1}^{K}\\exp(x\\beta_{k})}=\\frac{\\exp(z_{j})}{\\sum_{k=1}^{K}\\exp(z_{k})}$$\n    for $z_{k}=x\\beta_{k}$. What will happen if the $z$ values are very\n    large in magnitude (either positive or negative)? How can we\n    reexpress the equation so as to be able to do the calculation? Hint:\n    think about multiplying by $\\frac{c}{c}$ for a carefully chosen $c$.\n\n-   Second challenge: The same issue arises in the following\n    calculation. Suppose I want to calculate a predictive density (e.g.,\n    in a model comparison in a Bayesian context): $$\\begin{aligned}\n    f(y^{*}|y,x) & = & \\int f(y^{*}|y,x,\\theta)\\pi(\\theta|y,x)d\\theta\\\\\n     & \\approx & \\frac{1}{m}\\sum_{j=1}^{m}\\prod_{i=1}^{n}f(y_{i}^{*}|x,\\theta_{j})\\\\\n     & = & \\frac{1}{m}\\sum_{j=1}^{m}\\exp\\sum_{i=1}^{n}\\log f(y_{i}^{*}|x,\\theta_{j})\\\\\n     & \\equiv & \\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})\\end{aligned}$$\n    First, why do I use the log conditional predictive density? Second,\n    let's work with an estimate of the unconditional predictive density\n    on the log scale,\n    $\\log f(y^{*}|y,x)\\approx\\log\\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})$.\n    Now note that $e^{v_{j}}$ may be quite small as $v_{j}$ is the sum\n    of log likelihoods. So what happens if we have terms something like\n    $e^{-1000}$? So we can't exponentiate each individual $v_{j}$. This\n    is what is known as the \"log sum of exponentials\" problem (and the\n    solution as the \"log-sum-exp trick\"). Thoughts?\n\nNumerical issues come up frequently in linear algebra. For example, they\ncome up in working with positive definite and semi-positive-definite\nmatrices, such as covariance matrices. You can easily get negative\nnumerical eigenvalues even if all the eigenvalues are positive or\nnon-negative. Here's an example where we use an squared exponential\ncorrelation as a function of time (or distance in 1-d), which is\n*mathematically* positive definite (i.e., all the eigenvalues are\npositive) but not numerically positive definite:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nxs = np.arange(100)\ndists = np.abs(xs[:, np.newaxis] - xs)\ncorr_matrix = np.exp(-(dists/10)**2)     # This is a p.d. matrix (mathematically).\nscipy.linalg.eigvals(corr_matrix)[80:99]  # But not numerically!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-2.10937946e-16+9.49526594e-17j, -2.10937946e-16-9.49526594e-17j,\n       -1.77590164e-16+1.30160558e-16j, -1.77590164e-16-1.30160558e-16j,\n       -2.09305049e-16+0.00000000e+00j,  2.23869166e-16+3.21640840e-17j,\n        2.23869166e-16-3.21640840e-17j,  1.98271873e-16+9.08175827e-17j,\n        1.98271873e-16-9.08175827e-17j, -1.49116518e-16+0.00000000e+00j,\n       -1.23773149e-16+6.06467275e-17j, -1.23773149e-16-6.06467275e-17j,\n       -2.48071368e-18+1.51188749e-16j, -2.48071368e-18-1.51188749e-16j,\n       -4.08131705e-17+6.79669911e-17j, -4.08131705e-17-6.79669911e-17j,\n        1.27901871e-16+2.34695655e-17j,  1.27901871e-16-2.34695655e-17j,\n        5.23476667e-17+4.08642121e-17j])\n```\n:::\n:::\n\n\n\n## Final note\n\nHow the computer actually does arithmetic with the floating point\nrepresentation in base 2 gets pretty complicated, and we won't go into\nthe details. These rules of thumb should be enough for our practical\npurposes. Monahan and the URL reference have many of the gory details.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}