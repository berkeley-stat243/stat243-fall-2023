{
  "hash": "1b891fdf891e6c48d0bab7967e5186ba",
  "result": {
    "markdown": "---\ntitle: \"Numbers on a computer\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-07-26\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n\n[PDF](./unit8-numbers.pdf){.btn .btn-primary}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nReferences:\n\n-   Gentle, Computational Statistics, Chapter 2.\n-   [http://www.lahey.com/float.htm](http://www.lahey.com/float.htm)\n-   And for more gory detail, see Monahan, Chapter 2.\n\nA quick note that, as we've already seen, Python's version of scientific\nnotation is `XeY`, which means $X\\cdot10^{Y}$.\n\nA second note is that the concepts developed here apply outside of R,\nbut we'll illustrate the principles of computer numbers using R. R makes\nuse of the *double* and *int* types in C for the underlying\nrepresentation of R's numbers in C variables, so what we'll really be\nseeing is how such types behave in C on most modern machines. The behavior\nof real-valued numbers in Python is essentially the same, but\nPython handles the integer type differently.\n\nVideos (optional): \n\nThere are various videos from 2020 in the bCourses Media Gallery that you\ncan use for reference if you want to. \n\n  - Video 1. Bits, bytes, and integers\n  - Video 2. Double precision numbers: intro\n  - Video 3. Double precision numbers: details\n  - Video 4. Overflow and integers vs. doubles\n\n# 1. Basic representations\n\nEverything in computer memory or on disk is stored in terms of bits. A\n*bit* is essentially a switch than can be either on or off. Thus\neverything is encoded as numbers in base 2, i.e., 0s and 1s. 8 bits make\nup a *byte*. For information stored as plain text (ASCII), each byte is\nused to encode a single character (as previously discussed, actually only 7 of the 8 bits are\nactually used, hence there are $2^{7}=128$ ASCII characters). One way to\nrepresent a byte is to write it in hexadecimal, rather than as 8 0/1\nbits. Since there are $2^{8}=256$ possible values in a byte, we can\nrepresent it more compactly as 2 base-16 numbers, such as \"3e\" or \"a0\"\nor \"ba\". A file format is nothing more than a way of interpreting the\nbytes in a file.\n\n\nWe'll create some helper functions to all us to look\nat the underlying binary representation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## `Bits` is in the `bitstring` package\ndef bits(x):\n    obj = Bits(float = x, length = 64)\n    return(obj.bin)\n\ndef bitsi(x, len = 64, hex = False):\n    obj = Bits(int = x, length = len)\n    if hex:\n       return(obj)\n    else:\n       return(obj.bin)\n\ndef dg(x, form = '.20f'):\n    print(format(x, form))\n```\n:::\n\n\nNote that 'b' is encoded as 1\nmore than 'a', and similarly for '0', '1', and '2'.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nBits(bytes=b'a').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01100001'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'b').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01100010'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'0').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110000'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'1').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110001'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'2').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'00110010'\n```\n:::\n\n```{.python .cell-code}\nBits(bytes=b'@').bin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'01000000'\n```\n:::\n:::\n\n\n\nWe can think about how we'd store an integer in terms of bytes. With two\nbytes (16 bits), we could encode any value from $0,\\ldots,2^{16}-1=65535$. This is\nan *unsigned* integer representation. To store negative numbers as well,\nwe can use one bit for the sign, giving us the ability to encode\n-32767 - 32767 ($\\pm2^{15}-1$).\n\nNote that in general, rather than be stored\nsimply as the sign and then a number in base 2, integers (at least the\nnegative ones) are actually stored in different binary encoding to\nfacilitate arithmetic. \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbitsi(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbitsi(0, hex = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBits('0x0000000000000000')\n```\n:::\n\n```{.python .cell-code}\nbitsi(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000001'\n```\n:::\n\n```{.python .cell-code}\nbitsi(1, hex = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBits('0x0000000000000001')\n```\n:::\n\n```{.python .cell-code}\nbitsi(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000000000000000000000000000000000010'\n```\n:::\n\n```{.python .cell-code}\nbitsi(-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'1111111111111111111111111111111111111111111111111111111111111111'\n```\n:::\n:::\n\n\nWhat do I mean about facilitating arithmetic? As an example, consider adding\nthe binary representations of -1 and 1. Nice, right?\n\n\nFinally note that the set of computer integers is not closed under\narithmetic. We get an overflow (i.e., a result that is too\nlarge to be stored as an integer of the particular length):\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = np.int32(3423333)\na * a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-1756921895\n\n<string>:1: RuntimeWarning: overflow encountered in int_scalars\n```\n:::\n\n```{.python .cell-code}\na = np.int64(3423333)\na * a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11719208828889\n```\n:::\n\n```{.python .cell-code}\na = np.int64(34233332342343)\na * a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1001093889201452977\n\n<string>:1: RuntimeWarning: overflow encountered in long_scalars\n```\n:::\n:::\n\n\n\nReal numbers (or *floating points*) use a minimum of 4 bytes, for single\nprecision floating points. (GPU calculations often use single precision.)\nIn general (including in Python and R) 8 bytes are used to represent real\nnumbers on a computer and these are called *double precision floating\npoints* or *doubles*. Let's see some examples in Python of how much space\ndifferent types of variables take up.\n\nLet's see how this plays out in terms of memory use in R.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndoubleVec = np.random.normal(size = 100000)\nsys.getsizeof(doubleVec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n800112\n```\n:::\n:::\n\n\n\nWe can easily calculate the number of megabytes (MB) a vector of\nfloating points (in double precision) will use as the number of elements\ntimes 8 (bytes/double) divided by $10^{6}$ to convert from bytes to\nmegabytes. (In some cases when considering computer memory, a megabyte\nis $1,048,576=2^{20}=1024^{2}$ bytes (this is formally called a\n*mebibyte*) so slightly different than $10^{6}$ -- see [here for more\ndetails](https://en.wikipedia.org/wiki/Megabyte)).\n\nFinally, `numpy` has some helper functions that can tell us\nabout the characteristics of computer\nnumbers on the machine that Python is running.\n\nSo the max for a 32-bit (4-byte) integer is $2147483647=2^{31}-1$, which\nis consistent with 4 bytes.  Since we have both negative and\npositive numbers, we have $2\\cdot2^{31}=2^{32}=(2^{8})^{4}$, i.e., 4\nbytes, with each byte having 8 bits.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.iinfo(np.int32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niinfo(min=-2147483648, max=2147483647, dtype=int32)\n```\n:::\n\n```{.python .cell-code}\nnp.iinfo(np.int64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n```\n:::\n\n```{.python .cell-code}\nbitsi(2147483647)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0000000000000000000000000000000001111111111111111111111111111111'\n```\n:::\n\n```{.python .cell-code}\nbitsi(-2147483648)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'1111111111111111111111111111111110000000000000000000000000000000'\n```\n:::\n:::\n\n\n\n# 2. Floating point basics\n\n## Representing real numbers\n\nReals (also called floating points) are stored on the computer as an\napproximation, albeit a very precise approximation. As an example, if we\nrepresent the distance from the earth to the sun using a double, the\nerror is around a millimeter. However, we need to be very careful if\nwe're trying to do a calculation that produces a very small (or very\nlarge number) and particularly when we want to see if numbers are equal\nto each other.\n\nIf you run the code here, the results may surprise you.\n\n::: {.cell}\n\n```{.python .cell-code}\n0.3 - 0.2 == 0.1\n0.3\n0.2\n0.1 # Hmmm...\n\nnp.float64(0.3) - np.float64(0.2) == np.float64(0.1)\n\n0.75 - 0.5 == 0.25\n0.6 - 0.4 == 0.2\n## any ideas what is different about those two comparisons?\n```\n:::\n\n\nNext, let's consider the number of digits of accuracy\nwe have for a variety of numbers. We'll use `format` within\na handy wrapper function, `dg` to view as many digit as we want:\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = 0.3\nb = 0.2\ndg(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.29999999999999998890\n```\n:::\n\n```{.python .cell-code}\ndg(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.20000000000000001110\n```\n:::\n\n```{.python .cell-code}\ndg(a-b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.09999999999999997780\n```\n:::\n\n```{.python .cell-code}\ndg(0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.10000000000000000555\n```\n:::\n\n```{.python .cell-code}\ndg(1/3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.33333333333333331483\n```\n:::\n:::\n\n\nSo empirically, it looks like we're accurate up to the 16th decimal place\n\nBut actually, the key is the number of digits, not decimal places.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1234.1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12339999999994688551\n```\n:::\n\n```{.python .cell-code}\ndg(1234.123412341234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12341234123391586763\n```\n:::\n:::\n\n\nLet's return to our comparison, `0.75-0.5 == 0.25`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.75000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(0.50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.50000000000000000000\n```\n:::\n:::\n\n\n\nNotice that we can represent the result accurately only up to 16\nsignificant digits. This suggests no need to show more than 16\nsignificant digits and no need to print out any more when writing to a\nfile (except that if the number is bigger than $10^{16}$ then we need\nextra digits to correctly show the magnitude of the number if not using\nscientific notation). And of course, often we don't need anywhere near\nthat many.\n\n*Machine epsilon* is the term used for indicating the\n(relative) accuracy of real numbers and it is defined as the smallest\nfloat, $x$, such that $1+x\\ne1$:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n1e-16 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n\n```{.python .cell-code}\nnp.array(1e-16) + np.array(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n\n```{.python .cell-code}\n1e-15 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.000000000000001\n```\n:::\n\n```{.python .cell-code}\nnp.array(1e-15) + np.array(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.000000000000001\n```\n:::\n\n```{.python .cell-code}\n2e-16 + 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0000000000000002\n```\n:::\n\n```{.python .cell-code}\nnp.finfo(np.float64).eps\n## What about in single precision, e.g. on a GPU?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.220446049250313e-16\n```\n:::\n\n```{.python .cell-code}\nnp.finfo(np.float32).eps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.1920929e-07\n```\n:::\n:::\n\n\nI'm not sure why `numpy` reports the the 'resolution' as `1e-15` when machine epsilon is\nactually about `2.2e-16`.\n\n#### Floating point representation\n\n*Floating point* refers to the decimal point (or *radix* point since we'll\nbe working with base 2 and *decimal* relates to 10). Consider Avogadro's\nnumber in terms of scientific notation: $+6.023\\times10^{23}$. As a\nbaseline for what is about to follow note that we can express a decimal\nnumber in the following expansion\n$$6.03=6\\times10^{0}+0\\times10^{-1}+3\\times10^{-2}$$ A real number on a\ncomputer is stored in what is basically scientific notation:\n$$\\pm d_{0}.d_{1}d_{2}\\ldots d_{p}\\times b^{e}\\label{eq:floatRep}$$\nwhere $b$ is the base, $e$ is an integer and $d_{i}\\in\\{0,\\ldots,b-1\\}$.\n$e$ is called the *exponent* and $d=d_{1}d_{2}\\ldots d_{p}$ is called the *mantissa*.\n\nLet's consider the choices that the computer pioneers needed to make\nin using this system to represent numbers on a computer using base 2.\nFirst, we need to choose the number of bits to represent $e$ so that we\ncan represent sufficiently large and small numbers. Second we need to\nchoose the number of bits, $p$, to allocate to \n$d=d_{1}d_{2}\\ldots d_{p}$, which determines the accuracy of any\ncomputer representation of a real.\n\nThe great thing about floating points\nis that we can represent numbers that range from incredibly small to\nvery large while maintaining good precision. The floating point *floats*\nto adjust to the size of the number. Suppose we had only three digits to\nuse and were in base 10. In floating point notation we can express\n$0.12\\times0.12=0.0144$ as\n$(1.20\\times10^{-1})\\times(1.20\\times10^{-1})=1.44\\times10^{-2}$, but if\nwe had fixed the decimal point, we'd have $0.120\\times0.120=0.014$ and\nwe'd have lost a digit of accuracy. (Furthermore, we wouldn't be able\nto represent numbers bigger than $0.99$.\n\nMore specifically, the actual storage of a number on a computer these\ndays is generally as a double in the form:\n$$(-1)^{S}\\times1.d\\times2^{e-1023}=(-1)^{S}\\times1.d_{1}d_{2}\\ldots d_{52}\\times2^{e-1023}$$\nwhere the computer uses base 2, $b=2$, (so $d_{i}\\in\\{0,1\\}$) because\nbase-2 arithmetic is faster than base-10 arithmetic. The leading 1\nnormalizes the number; i.e., ensures there is a unique representation\nfor a given computer number. This avoids representing any number in\nmultiple ways, e.g., either\n$1=1.0\\times2^{0}=0.1\\times2^{1}=0.01\\times2^{2}$. For a double, we have\n8 bytes=64 bits. Consider our representation as ($S,d,e$) where $S$ is\nthe sign. The leading 1 is the *hidden bit* and doesn't need to be\nstored because it is always present. In general $e$ is\nrepresented using 11 bits ($2^{11}=2048$), and the subtraction takes the\nplace of having a sign bit for the exponent. (Note that in our\ndiscussion we'll just think of $e$ in terms of its base 10\nrepresentation, although it is of course represented in base 2.) This\nleaves $p=52 = 64-1-11$ bits for $d$.\n\nIn this code I'll force storage as a double using by tacking on a decimal place, `.0`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbits(2.0**(-1)) # 1/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111111100000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**0)  # 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111111110000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**1)  # 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**1 + 2.0**0)  # 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000001000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2.0**2)  # 4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100000000010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(-2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'1100000000000000000000000000000000000000000000000000000000000000'\n```\n:::\n:::\n\n\n**Question**: Given a fixed number of bits for a number, what is the\ntradeoff between using bits for the $d$ part vs. bits for the $e$ part?\n\nLet's consider what can be represented exactly:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.10000000000000000555\n```\n:::\n\n```{.python .cell-code}\ndg(.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.50000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.25000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(.26)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.26000000000000000888\n```\n:::\n\n```{.python .cell-code}\ndg(1/32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03125000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(1/33)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03030303030303030387\n```\n:::\n:::\n\n\nSo why is 0.5 stored exactly and 0.1 not stored exactly? By analogy,\nconsider the difficulty with representing 1/3 in base 10.\n\n## Overflow and underflow\n\nThe largest and smallest numbers we can represent are $2^{e_{\\max}}$ and\n$2^{e_{\\min}}$ where $e_{\\max}$ and $e_{\\min}$ are the smallest and\nlargest possible values of the exponent. Let's consider the exponent and\nwhat we can infer about the range of possible numbers. With 11 bits for\n$e$, we can represent $\\pm2^{10}=\\pm1024$ different exponent values (see\n`np.finfo(np.float64).maxexp`) (why is `np.finfo(np.float64).minexp` only\n-1022?). So the largest number we could represent is $2^{1024}$. What\nis this in base 10?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n1e308\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1e+308\n```\n:::\n\n```{.python .cell-code}\n1e309\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninf\n```\n:::\n\n```{.python .cell-code}\ntry:\n    np.log10(2.0**1024)\nexcept:\n    print(\"Whoops -- that just barely overflowed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWhoops -- that just barely overflowed.\n```\n:::\n\n```{.python .cell-code}\nnp.log10(2.0**1023)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n307.95368556425274\n```\n:::\n\n```{.python .cell-code}\nnp.finfo(np.float64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n```\n:::\n:::\n\n\nWe could have been smarter about that calculation:\n$\\log_{10}2^{1024}=\\log_{2}2^{1024}/\\log_{2}10=1024/3.32\\approx308$. The\nresult is analogous for the smallest number, so we have that floating\npoints can range between $1\\times10^{-308}$ and $1\\times10^{308}$. Take\na look at *.Machine\\$double.xmax* and *.Machine.double.xmin*. Producing\nsomething larger or smaller in magnitude than these values is called\noverflow and underflow respectively. When we overflow, R gives back an\nInf or -Inf (and in other cases we might get an error message). When we\nunderflow, we get back 0, which in particular can be a problem if we try\nto divide by the value.\n\n## Integers or floats?\n\nValues stored as integers should overflow if they exceed the maximum integer.\n\nShould $2^{65}$ overflow?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.log2(np.iinfo(np.int64).max)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n63.0\n```\n:::\n\n```{.python .cell-code}\nx = np.int64(2)\n# Yikes!\nx**64\n\n# Interesting:\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n```\n:::\n\n```{.python .cell-code}\n2**64\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18446744073709551616\n```\n:::\n\n```{.python .cell-code}\n2**100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1267650600228229401496703205376\n```\n:::\n\n```{.python .cell-code}\ndg(2.0**64, '.2f')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18446744073709551616.00\n```\n:::\n\n```{.python .cell-code}\ndg(2.0**100, '.2f')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1267650600228229401496703205376.00\n```\n:::\n:::\n\n\nDoubles won't overflow until much larger values than 4- or 8-byte integers. However we need to think about\nwhat integer-valued numbers can and can't be stored exactly in our base 2 representation of floating point numbers.\nIt turns out that integer-valued numbers can be stored exactly as doubles when their absolute\nvalue is less than $2^{53}$.\n\n> *Challenge*: Why $2^{53}$? Write out what integers can be stored exactly in our base 2 representation of floating point numbers.\n\nYou can force storage as integers or doubles in a few ways.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3; type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'int'>\n```\n:::\n\n```{.python .cell-code}\nx = np.float64(x); type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.float64'>\n```\n:::\n\n```{.python .cell-code}\nx = 3.0; type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'float'>\n```\n:::\n\n```{.python .cell-code}\nx = np.float64(3); type(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.float64'>\n```\n:::\n:::\n\n\n## Precision\n\nConsider our representation as (*S, d, e*) where we have $p=52$ bits for\n$d$. Since we have $2^{52}\\approx0.5\\times10^{16}$, we can represent\nabout that many discrete values, which means we can accurately represent\nabout 16 digits (in base 10). The result is that floats on a computer\nare actually discrete (we have a finite number of bits), and if we get a\nnumber that is in one of the gaps (there are uncountably many reals),\nit's approximated by the nearest discrete value. The accuracy of our\nrepresentation is to within 1/2 of the gap between the two discrete\nvalues bracketing the true number. Let's consider the implications for\naccuracy in working with large and small numbers. By changing $e$ we can\nchange the magnitude of a number. So regardless of whether we have a\nvery large or small number, we have about 16 digits of accuracy, since\nthe absolute spacing depends on what value is represented by the least\nsignificant digit (the *ulp*, or *unit in the last place*) in $d$, i.e.,\nthe $p=52$nd one, or in terms of base 10, the 16th digit. Let's explore\nthis:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# large vs. small numbers\ndg(.1234123412341234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12341234123412339607\n```\n:::\n\n```{.python .cell-code}\ndg(1234.1234123412341234) # not accurate to 16 decimal places \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234.12341234123414324131\n```\n:::\n\n```{.python .cell-code}\ndg(123412341234.123412341234) # only accurate to 4 places \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n123412341234.12341308593750000000\n```\n:::\n\n```{.python .cell-code}\ndg(1234123412341234.123412341234) # no places! \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1234123412341234.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12341234123412341234) # fewer than no places! \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n12341234123412340736.00000000000000000000\n```\n:::\n:::\n\n\nWe can see the implications of this in the context of calculations:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1234567812345678.0 - 1234567812345677.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12345678123456788888.0 - 12345678123456788887.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.00000000000000000000\n```\n:::\n\n```{.python .cell-code}\ndg(12345678123456780000.0 - 12345678123456770000.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10240.00000000000000000000\n```\n:::\n:::\n\n\nThe spacing of possible computer numbers that have a magnitude of about\n1 leads us to another definition of *machine epsilon* (an alternative,\nbut essentially equivalent definition to that given previously in this\nUnit). Machine epsilon tells us also about the relative spacing of\nnumbers. First let's consider numbers of magnitude one. The difference\nbetween $1=1.00...00\\times2^{0}$ and $1.000...01\\times2^{0}$ is\n$\\epsilon=1\\times2^{-52}\\approx2.2\\times10^{-16}$. Machine epsilon gives\nthe *absolute spacing* for numbers near 1 and the *relative spacing* for\nnumbers with a different order of magnitude and therefore a different\nabsolute magnitude of the error in representing a real. The relative\nspacing at $x$ is $$\\frac{(1+\\epsilon)x-x}{x}=\\epsilon$$ since the next\nlargest number from $x$ is given by $(1+\\epsilon)x$.\n\nSuppose $x=1\\times10^{6}$. Then the absolute error in representing a\nnumber of this magnitude is $x\\epsilon\\approx2\\times10^{-10}$. (Actually\nthe error would be one-half of the spacing, but that's a minor\ndistinction.) We can see by looking at the numbers in decimal form,\nwhere we are accurate to the order $10^{-10}$ but not $10^{-11}$. This\nis equivalent to our discussion that we have only 16 digits of accuracy.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(1000000.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1000000.09999999997671693563\n```\n:::\n:::\n\n\nLet's see what arithmetic we can do exactly with integer-valued numbers stored as\ndoubles and how that relates to the absolute spacing of numbers we've\njust seen:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n2.0**52\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4503599627370496.0\n```\n:::\n\n```{.python .cell-code}\n2.0**52+1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4503599627370497.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740992.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53+1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740992.0\n```\n:::\n\n```{.python .cell-code}\n2.0**53+2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9007199254740994.0\n```\n:::\n\n```{.python .cell-code}\n2.0**54\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.8014398509481984e+16\n```\n:::\n\n```{.python .cell-code}\n2.0**54+2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.8014398509481984e+16\n```\n:::\n\n```{.python .cell-code}\n2.0**54+4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.8014398509481988e+16\n```\n:::\n\n```{.python .cell-code}\nbits(2**53)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**53+1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**53+2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101000000000000000000000000000000000000000000000000000001'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54+2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000000'\n```\n:::\n\n```{.python .cell-code}\nbits(2**54+4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0100001101010000000000000000000000000000000000000000000000000001'\n```\n:::\n:::\n\n\nThe absolute spacing is $x\\epsilon$, so we have spacings of\n$2^{52}\\times2^{-52}=1$, $2^{53}\\times2^{-52}=2$,\n$2^{54}\\times2^{-52}=4$ for numbers of magnitude $2^{52}$, $2^{53}$, and\n$2^{54}$, respectively.\n\nWith a bit more work (e.g., using Mathematica), one can demonstrate that\ndoubles in Python in general are represented as the nearest number that can\nstored with the 64-bit structure we have discussed and that the spacing\nis as we have discussed. The results below show the spacing that\nresults, in base 10, for numbers around 0.1. The numbers R reports are\nspaced in increments of individual bits in the base 2 representation.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndg(0.1234567812345678)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456779729\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456781)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456781117\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456782)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456782505\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456783)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456782505\n```\n:::\n\n```{.python .cell-code}\ndg(0.12345678123456784)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12345678123456783892\n```\n:::\n\n```{.python .cell-code}\nbits(0.1234567812345678)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010000110'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456781)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010000111'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456782)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001000'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456783)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001000'\n```\n:::\n\n```{.python .cell-code}\nbits(0.12345678123456784)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'0011111110111111100110101101110100010101110111110011010010001001'\n```\n:::\n:::\n\n\n## Working with higher precision numbers\n\nAs we've seen, Python will automatically work with integers in arbitrary precision.\n(Note that R does not do this -- R uses 4-byte integers, and for many calculations\nit's best to use R's `numeric` type because integers that aren't really large\ncan be expressed exactly.)\n\nFor higher precision floating point numbers you can make use of the `gmpy2`\npackage.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport gmpy2\ngmpy2.get_context().precision=200\ngmpy2.const_pi()\n\n## not sure why this shows ...00004\ngmpy2.mpfr(\".1234567812345678\") \n```\n:::\n\n\n\n\n# 3. Implications for calculations and comparisons\n\n## Computer arithmetic is not mathematical arithmetic!\n\nAs mentioned for integers, computer number arithmetic is not closed,\nunlike real arithmetic. For example, if we multiply two computer\nfloating points, we can overflow and not get back another computer\nfloating point. One term that is used, which might pop up in an error\nmessage (though probably not in R) is that an \"exception\" is \"thrown\".\n\nAnother mathematical concept we should consider here is that computer\narithmetic does not obey the associative and distributive laws, i.e.,\n$(a+b)+c$ may not equal $a+(b+c)$ on a computer and $a(b+c)$ may not be\nthe same as $ab+ac$. Here's an example with multiplication:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nval1 = 1/10; val2 = 0.31; val3 = 0.57\nres1 = val1*val2*val3\nres2 = val3*val2*val1\nres1 == res2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n```{.python .cell-code}\ndg(res1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.01766999999999999821\n```\n:::\n\n```{.python .cell-code}\ndg(res2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.01767000000000000168\n```\n:::\n:::\n\n\n## Calculating with integers vs. floating points\n\nIt's important to note that operations with integers are fast and exact\n(but can easily overflow) while operations with floating points are\nslower and approximate. Because of this slowness, floating point\noperations (*flops*) dominate calculation intensity and are used as the\nmetric for the amount of work being done - a multiplication (or\ndivision) combined with an addition (or subtraction) is one flop. We'll\ntalk a lot about flops in the unit on linear algebra.\n\n## Comparisons\n\nAs we saw, we should never test `a==b` unless (1) *a* and *b* are\nrepresented as integers in R, (2) they are integer-valued but stored as\ndoubles that are small enough that they can be stored exactly) or (3)\nthey are decimal numbers that have been created in the same way (e.g.,\n`0.4-0.3==0.4-0.3` returns `TRUE` but `0.1==0.4-0.3` does not). Similarly we should be careful\nabout testing `a==0`. And be careful of greater than/less than\ncomparisons. For example, be careful of `x[ x < 0 ] = NA` if what you\nare looking for is values that might be *mathematically* less than zero,\nrather than whatever is *numerically* less than zero.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n4 - 3 == 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n\n```{.python .cell-code}\n4.0 - 3.0 == 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n\n```{.python .cell-code}\n4.1 - 3.1 == 1.0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n:::\n\n\nOne nice approach to checking for approximate equality is to make use of\n*machine epsilon*. If the relative spacing of two numbers is less than\n*machine epsilon*, then for our computer approximation, we say they are\nthe same. Here's an implementation that relies on the absolute spacing\nbeing $x\\epsilon$ (see above).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = 12345678123456781000\nb = 12345678123456782000\n\ndef approxEqual(a,b):\n  if abs(a - b) < np.finfo(np.float64).eps * abs(a + b):\n    print(\"approximately equal\")\n  else:\n    print (\"not equal\")\n\n\napproxEqual(a,b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\napproximately equal\n```\n:::\n\n```{.python .cell-code}\na = 1234567812345678\nb = 1234567812345677\n\napproxEqual(a,b)   \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnot equal\n```\n:::\n:::\n\n\nActually, we probably want to use a number slightly larger than\nmachine epsilon to be safe. \n\nFinally, in computing, we often encounter the use of an unusual integer\nas a symbol for missing values. E.g., a datafile might store missing\nvalues as -9999. Testing for this using == with floats should generally be\nok:` x [ x == -9999 ] = np.nan`, both because integers of this magnitude\nare stored exactly and because the -9999 values would presumably have\nbeen created in the same way. To be really careful, you can read in as\ncharacter type and do the assessment before converting to numeric.\n\n## Calculations\n\nGiven the limited *precision* of computer numbers, we need to be careful\nwhen in the following two situations.\n\n1.  Subtracting large numbers that are nearly equal (or adding negative\n    and positive numbers of the same magnitude). You won't have the\n    precision in the answer that you would like. How many decimal places\n    of accuracy do we have here?\n\n    \n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    # catastrophic cancellation w/ large numbers\n    dg(123456781234.56 - 123456781234.00)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.55999755859375000000\n    ```\n    :::\n    :::\n\n\n    The absolute error in the original numbers here is of the order\n    $\\epsilon x=2.2\\times10^{-16}\\cdot1\\times10^{11}\\approx1\\times10^{-5}=.00001$.\n    While we might think that the result is close to the value 1 and\n    should have error of about machine epsilon, the relevant absolute\n    error is in the original numbers, so we actually only have about\n    five significant digits in our result because we cancel out the\n    other digits.\n\n    This is called *catastrophic cancellation*, because most of the\n    digits that are left represent rounding error -- many of the significant\n    digits have cancelled with each other.\\\n    Here's catastrophic cancellation with small numbers. The right\n    answer here is exactly 0.000000000000000000001234.\n\n    \n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    # catastrophic cancellation w/ small numbers\n    a = .000000000000123412341234\n    b = .000000000000123412340000\n    \n    # so we know the right answer is .000000000000000000001234 EXACTLY  \n    \n    dg(a-b, '.35f')\n    ## [1] \"0.00000000000000000000123399999315140\"\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.00000000000000000000123399999315140\n    ```\n    :::\n    :::\n\n\n    But the result is accurate only to 8 places + 20 = 28 decimal\n    places, as expected from a machine precision-based calculation,\n    since the \"1\" is in the 13th position, after 12 zeroes (12+16=28).\n    Ideally, we would have accuracy to 36 places (16 digits + the 20\n    zeroes), but we've lost 8 digits to catastrophic cancellation.\n\n    It's best to do any subtraction on numbers that are not too large.\n    For example, if we compute the sum of squares in a naive way, we can\n    lose all of the information in the calculation because the\n    information is in digits that are not computed or stored accurately:\n    $$s^{2}=\\sum x_{i}^{2}-n\\bar{x}^{2}$$\n\n    \n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    ## No problem here:\n    x = np.array([-1.0, 0.0, 1.0])\n    n = len(x)\n    np.sum(x**2)-n*np.mean(x)**2 \n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    np.sum((x - np.mean(x))**2)\n    \n    ## Adding/subtracting a constant shouldn't change the result:\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    x = x + 1e8\n    np.sum(x**2)-n*np.mean(x)**2  # the result of this is not good!\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.0\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    np.sum((x - np.mean(x))**2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    2.0\n    ```\n    :::\n    :::\n\n\n    A good principle to take away is to subtract off a number similar in\n    magnitude to the values (in this case $\\bar{x}$ is obviously ideal)\n    and adjust your calculation accordingly. In general, you can\n    sometimes rearrange your calculation to avoid catastrophic\n    cancellation. Another example involves the quadratic formula for\n    finding a root (p. 101 of Gentle).\n\n2.  Adding or subtracting numbers that are very different in magnitude.\n    The precision will be that of the large magnitude number, since we\n    can only represent that number to a certain absolute accuracy, which\n    is much less than the absolute accuracy of the smaller number:\n\n    \n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    dg(123456781234.2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19999694824218750000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.1)        # truth: 123456781234.1\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.09999084472656250000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.01)       # truth: 123456781234.19\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19000244140625000000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.001)      # truth: 123456781234.199\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19898986816406250000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.0001)     # truth: 123456781234.1999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19989013671875000000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.00001)    # truth: 123456781234.19999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19998168945312500000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    dg(123456781234.2 - 0.000001)   # truth: 123456781234.199999\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    123456781234.19999694824218750000\n    ```\n    :::\n    \n    ```{.python .cell-code}\n    123456781234.2 - 0.000001 == 123456781234.2\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    True\n    ```\n    :::\n    :::\n\n\n    The larger number in the calculations above is of magnitude\n    $10^{11}$, so the absolute error in representing the larger number\n    is around $1\\times10^{^{-5}}$. Thus in the calculations above we can\n    only expect the answers to be accurate to about $1\\times10^{-5}$. In\n    the last calculation above, the smaller number is smaller than\n    $1\\times10^{-5}$ and so doing the subtraction has had no effect.\n    This is analogous to trying to do $1+1\\times10^{-16}$ and seeing\n    that the result is still 1.\\\n    A work-around when we are adding numbers of very different\n    magnitudes is to add a set of numbers in increasing order. However,\n    if the numbers are all of similar magnitude, then by the time you\n    add ones later in the summation, the partial sum will be much larger\n    than the new term. A (second) work-around to that problem is to add\n    the numbers in a tree-like fashion, so that each addition involves a\n    summation of numbers of similar size.\n\nGiven the limited *range* of computer numbers, be careful when you are:\n\n-   Multiplying or dividing many numbers, particularly large or small\n    ones. Never take the product of many large or small numbers as this\n    can cause over- or under-flow. Rather compute on the log scale and\n    only at the end of your computations should you exponentiate. E.g.,\n    $$\\prod_{i}x_{i}/\\prod_{j}y_{j}=\\exp(\\sum_{i}\\log x_{i}-\\sum_{j}\\log y_{j})$$\n\nLet's consider some challenges that illustrate that last concern.\n\n-   Challenge: consider multiclass logistic regression, where you have\n    quantities like this:\n    $$p_{j}=\\text{Prob}(y=j)=\\frac{\\exp(x\\beta_{j})}{\\sum_{k=1}^{K}\\exp(x\\beta_{k})}=\\frac{\\exp(z_{j})}{\\sum_{k=1}^{K}\\exp(z_{k})}$$\n    for $z_{k}=x\\beta_{k}$. What will happen if the $z$ values are very\n    large in magnitude (either positive or negative)? How can we\n    reexpress the equation so as to be able to do the calculation? Hint:\n    think about multiplying by $\\frac{c}{c}$ for a carefully chosen $c$.\n\n-   Second challenge: The same issue arises in the following\n    calculation. Suppose I want to calculate a predictive density (e.g.,\n    in a model comparison in a Bayesian context): $$\\begin{aligned}\n    f(y^{*}|y,x) & = & \\int f(y^{*}|y,x,\\theta)\\pi(\\theta|y,x)d\\theta\\\\\n     & \\approx & \\frac{1}{m}\\sum_{j=1}^{m}\\prod_{i=1}^{n}f(y_{i}^{*}|x,\\theta_{j})\\\\\n     & = & \\frac{1}{m}\\sum_{j=1}^{m}\\exp\\sum_{i=1}^{n}\\log f(y_{i}^{*}|x,\\theta_{j})\\\\\n     & \\equiv & \\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})\\end{aligned}$$\n    First, why do I use the log conditional predictive density? Second,\n    let's work with an estimate of the unconditional predictive density\n    on the log scale,\n    $\\log f(y^{*}|y,x)\\approx\\log\\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})$.\n    Now note that $e^{v_{j}}$ may be quite small as $v_{j}$ is the sum\n    of log likelihoods. So what happens if we have terms something like\n    $e^{-1000}$? So we can't exponentiate each individual $v_{j}$. This\n    is what is known as the \"log sum of exponentials\" problem (and the\n    solution as the \"log-sum-exp trick\"). Thoughts?\n\nNumerical issues come up frequently in linear algebra. For example, they\ncome up in working with positive definite and semi-positive-definite\nmatrices, such as covariance matrices. You can easily get negative\nnumerical eigenvalues even if all the eigenvalues are positive or\nnon-negative. Here's an example where we use an squared exponential\ncorrelation as a function of time (or distance in 1-d), which is\n*mathematically* positive definite (i.e., all the eigenvalues are\npositive) but not numerically positive definite:\n\n\n\n::: {.cell hash='unit8-numbers_cache/pdf/unnamed-chunk-30_c992f13f354ca35126aeb5f5760ae6ca'}\n\n```{.python .cell-code}\n\ndef custom_operation(a, b):\n    return abs(a - b)  # Custom pairwise operation\n\nxs = np.arange(100)\ndists = custom_operation(xs[:, np.newaxis], xs)\ncorMat = np.exp(- (dists/10)**2) # this is a p.d. matrix (mathematically)\nscipy.linalg.eigvals(corMat)[80:99]  # but not numerically\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 1.87147120e-16-1.38569039e-16j, -2.63753999e-16+0.00000000e+00j,\n        2.02746697e-16+9.27811726e-17j,  2.02746697e-16-9.27811726e-17j,\n       -2.18770059e-16+0.00000000e+00j, -1.81043922e-16+6.92659199e-17j,\n       -1.81043922e-16-6.92659199e-17j,  2.16131809e-16+2.53937305e-17j,\n        2.16131809e-16-2.53937305e-17j, -1.48286621e-16+5.22080636e-17j,\n       -1.48286621e-16-5.22080636e-17j, -1.45202652e-16+0.00000000e+00j,\n       -5.96180427e-18+1.51715979e-16j, -5.96180427e-18-1.51715979e-16j,\n       -4.10075792e-17+5.46543960e-17j, -4.10075792e-17-5.46543960e-17j,\n        1.51314597e-16+0.00000000e+00j,  5.42455332e-17+4.22706275e-17j,\n        5.42455332e-17-4.22706275e-17j])\n```\n:::\n:::\n\n\n## Final note\n\nHow the computer actually does arithmetic with the floating point\nrepresentation in base 2 gets pretty complicated, and we won't go into\nthe details. These rules of thumb should be enough for our practical\npurposes. Monahan and the URL reference have many of the gory details.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}