{
  "hash": "f3bd68e507fcb41f44325e491f1b5d77",
  "result": {
    "markdown": "---\ntitle: \"The bash shell and UNIX commands\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-08-26\"\n\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\nengine: knitr\n---\n\n\n[PDF](./unit3-bash.pdf){.btn .btn-primary}\n\n\nReference:\n\n- Newham and Rosenblatt, Learning the bash Shell, 2nd ed.\n\nNote that it can be difficult to distinguish what is shell-specific and\nwhat is just part of the operating system (i.e., a UNIX-style operating\nsystem). Some of the material here is not bash-specific but general to\nUNIX (e.g., the commands themselves are not specific to bash).\nI'll use 'UNIX' to refer to the family of operating systems that\ndescend from the path-breaking UNIX operating system developed at AT&T's\nBell Labs in the 1970s. These include MacOS and various flavors of Linux\n(e.g., Ubuntu, Debian, CentOS, Fedora).\n\nFor your work on this unit, either bash on a Linux machine, the older\nversion of bash on MacOS, or zsh on MacOS (or Linux) are fine. I'll\nprobably demo everything using bash on a Linux machine, and there are\nsome annoying differences from the older bash on MacOS that may be\noccasionally confusing (in particular the options to various commands\ncan differ on MacOS).\n\nThe Windows PowerShell and old cmd.exe/DOS command interpreter both provide\na command-line interface on Windows, but not a UNIX-based one and\nnot interfaces that will be considered here. \n\n\n# 1. Shell basics\n\nThe shell is the interface between you and the UNIX operating system.\nWhen you are working in a terminal window (i.e., a window providing the\ncommand line interface), you're interacting with a shell.\n\nThere are multiple shells (`sh`, `bash`, `zsh`, `csh`, `tcsh`, `ksh`).\nWe'll assume usage of `bash`, as this is a very commonly-used shell in\nLinux, plus was the default for Mac OS X (until Catalina, for which\n`zsh` is the default), the SCF machines, and the UC Berkeley campus\ncluster (Savio). It's fine for you to use `zsh`.\n\nUNIX shell commands are designed to each do a specific task really well\nand really fast. They are modular and composable, so you can build up\ncomplicated operations by combining the commands. These tools were\ndesigned decades ago, so using the shell might seem old-fashioned, but\nthe shell still lies at the heart of modern scientific computing. By\nusing the shell you can automate your work and make it reproducible.\nAnd once you know how to use it, you'll find that enter commands\nquickly and without a lot of typing.\n\n# 2. Using the bash shell\n\nFor this Unit, we'll rely on [the bash shell tutorial](https://berkeley-scf.github.io/tutorial-using-bash)\nfor the details of how to use the shell. We won't cover the page on Managing Processes.\nFor the moment, we won't cover the page on Regular Expressions, but when we talk about string processing and regular expressions in Unit 5, we'll come back to that material.\n\n# 3. bash shell examples\n\nHere we'll work through a few examples to start to give you a feel for\nusing the bash shell to manage your workflows and process data.\n\nFirst let's get the files from GitHub to have a set of\nfiles we can do interesting things with.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngit clone https://github.com/berkeley-stat243/stat243-fall-2023\n```\n:::\n\n\nOne important note is that most of the shell commands that work with\ndata inside files (in contrast to commands like `ls` and `cd`)\nwork only with text files and not binary files. Also the\ncommands operate on a line-by-line basis.\n\n**Our first mission** is some basic manipulation of a data file. Suppose\nwe want to get a sense for the number of weather stations in different\nstates using the *coop.txt* file.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncd stat243-fall-2021/data\ngzip -cd coop.txt.gz | less\ngunzip coop.txt.gz\ncut -b50-70 coop.txt | less \ncut -b60-61 coop.txt | uniq\ncut -b60-61 coop.txt | sort | uniq\ncut -b60-61 coop.txt | sort | uniq -c\n## all in one line with no change to the original file:\ngzip -cd coop.txt.gz | cut -b60-61 coop.txt | sort | uniq -c\n```\n:::\n\n\nI could have done that in R or Python, but it would have required\nstarting the program up and reading all the data into memory.\n\nIf you feel that manually figuring out the position of the state field\nis inconsistent with our emphasis on programmatic workflows, see\nthe [fifth challenge](#fifth-challenge) below.\n\n**Our second mission**: how can I count the number of fields in a CSV\nfile programmatically?\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ntail -n 1 cpds.csv | grep -o ',' | wc -l\nnfields=$(tail -n 1 cpds.csv | grep -o ',' | wc -l)\n\nnfields=$((${nfields}+1))\necho $nfields\n\n## alternatively, we can use `bc`\nnfields=$(echo \"${nfields}+1\" | bc)\n```\n:::\n\n\n\nTrouble-shooting: How could the syntax above get the wrong answer?\n\nExtension: We could write a function that can count the number of fields\nin any file.\n\nExtension: How could I see if all of the lines have the same number of\nfields?\n\n**Our third mission**: was the `sqlite3` package in the five most\nrecently modified Quarto Markdown files in the units directory?\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncd ../units\ngrep -l 'import sqlite3' unit7-bigData.qmd\nls -tr *.qmd\n## if unit7-bigData.qmd is not amongst the 5 most recently used,\n## let's artificially change the timestamp so it is recently used.\ntouch unit7-bigData.qmd\n\nls -tr *.qmd | tail -n 5\nls -tr *.qmd | tail -n 5 | grep sqlite3\nls -tr *.qmd | tail -n 5 | grep \"unit7-bigData\"\nls -tr *.qmd | tail -n 5 | xargs grep 'import sqlite3'\nls -tr *.qmd | tail -n 5 | xargs grep -l 'import sqlite3'\n## here's how we could do it by explicitly passing the file names\n## rather than using xargs\ngrep -l 'import sqlite3' $(ls -tr *.R | tail -n 5)\n```\n:::\n\n\nNotice that `man tail` indicates it can take input from a FILE or from\n`stdin`. Here it uses `stdin`, so it is gives the last five lines of the\noutput of `ls`, not the last five lines of the files indicated in that\noutput.\n\n`man grep` also indicates it can take input from a FILE or from `stdin`.\nHowever, we want grep to operate on the content of the files indicated\nin stdin. So we use `xargs` to convert `stdin` to be recognized as\narguments, which then are the FILE inputs to `grep`.\n\nHere are some of the ways we can pass information from a command to somewhere else:\n\n- Piping allows us to pass information from one command to another command via stdout to stdin.\n- `$()` allows us to store the result of a command in a variable.\n    - also used to create a temporary variable to pass the output from one command as an option or argument (e.g., the FILE argument) of another command\n- File redirection operators such as `>` and `>>` allow us to pass information from a command into a file.\n\n**Our fourth mission**: write a function that will move the most recent\n*n* files in your Downloads directory to another directory.\n\nIn general, we want to start with a specific case, and then generalize\nto create the function.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nls -rt ~/Downloads | tail -n 5\n# create dummy test files without any spaces\ntouch ~/Downloads/test{1..4}\nls -rt ~/Downloads | tail -n 5\n\n## sometimes the ~ behaves weirdly in scripting, so let's use full path \nmv \"/accounts/vis/paciorek/Downloads/$(ls -rt \\\n   /accounts/vis/paciorek/Downloads | tail -n 1)\" ~/Desktop\n\nfunction mvlast() {\n    mv \"/accounts/vis/paciorek/Downloads/$(ls -rt \\\n       /accounts/vis/paciorek/Downloads | tail -n 1)\" $1\n}\n```\n:::\n\n\nNote that the quotes deal with cases where a file has a space in its name.\n\nIf we wanted to handle multiple files, we could do it with a loop:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfunction mvlast() {\n    for ((i=1; i<=${1}; i++)); do\n        mv \"/accounts/vis/paciorek/Downloads/$(ls -rt \\\n           /accounts/vis/paciorek/Downloads | tail -n 1)\" ${2}\n    done\n}\n```\n:::\n\n\nSide note: if we were just moving files from the current working directory and with files without spaces in their names, it should be possible to use `tail -n ${1}` without the loop.\n\n**Our fifth mission**: automate the process of determining what Python\npackages are used in all of the qmd code chunks here and install those packages\non a new machine.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngrep import *.qmd\ngrep --no-filename import *.qmd\ngrep --no-filename \"^import\" *.qmd\ngrep --no-filename \"^import \" *.qmd\ngrep --no-filename \"^import \" *.qmd | sort | uniq\ngrep --no-filename \"^import \" *.qmd | cut -d'#' -f1\ngrep --no-filename \"^import \" *.qmd | cut -d'#' -f1 | sed  \"s/as .*//\"\ngrep --no-filename \"^import \" *.qmd | cut -d'#' -f1 | \\\n                   sed  \"s/as .*//\" | sed \"s/import //\" > tmp.txt\nsed \"s/,/\\n/g\" tmp.txt | sed \"s/ //g\" | sort | uniq | tee requirements.txt\n\n## note: on a Mac, use 's/,/\\\\\\n/g'\n## See https://superuser.com/questions/307165/newlines-in-sed-on-mac-os-x\n\necho \"There are $(wc -l requirements.txt | cut -d' ' -f1) \\\nunique packages we will install.\"\n## note: on Linux, wc -l puts the number as the first characters of the output\n## on a Mac, there may be a bunch of spaces preceding the number, so try this:\n## echo \"There are $(wc -l libs.txt | tr -s ' ' | cut -d' ' -f2) \\\n## unique packages we will install.\"\n\npip install -r requirements.txt\n# or use Mamba/Conda\n```\n:::\n\n\nYou probably wouldn't want to use this code to accomplish this task in reality - you would want to see if there are  packages that can accomplish this. (In the R ecosystem, the `renv` and `packrat` packages do this for R projects.) The main point was to illustrate how one can quickly hack together some code to do fairly complicated tasks.\n\n**Our sixth mission**: suppose I've accidentally started a bunch of jobs\n(perhaps with a for loop in bash!) and need to kill them. (This example uses\nsyntax from the Managing Processes page of the bash tutorial, so it goes\nbeyond what you were asked to read for this Unit.)\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# use a 'here document':\ncat > job.py << EOF\nimport time\ntime.sleep(1e5)\nEOF\n\n# alternatively:\necho -e \"import time\\ntime.sleep(1e5)\" > job.py\n\nnJobs=30 \nfor (( i=1; i<=${nJobs}; i++ )); do     \n   python job.py > job-${i}.out & \ndone \n\n# on Linux:\nps -o pid,pcpu,pmem,user,cmd -C python \nps -o pid,pcpu,pmem,user,cmd,start_time --sort=start_time -C python | tail -n 30\nps -o pid --sort=start_time -C python | tail -n ${nJobs} | xargs kill\n\n# on a Mac:\nps -o pid,pcpu,pmem,user,command | grep python\n# not clear how to sort by start time\nps -o pid,command | grep python | cut -d' ' -f1 |  tail -n ${nJobs} | xargs kill\n```\n:::\n\n\n# 4. bash shell challenges\n\n## 4.1 First challenge\n\nConsider the file `cpds.csv`. How would you write a shell command\nthat returns \"There are 8 occurrences of the word 'Belgium' in this file.\",\nwhere '8' should instead be the correct number of times the word occurs.\n\nExtra: make your code into a function that can operate on any file\nindicated by the user and any word of interest.\n\n## 4.2 Second challenge\n\nConsider the data in the `RTADataSub.csv` file. This is a subset of data\ngiving freeway travel times for segments of a freeway in an Australian\ncity. The data are from a kaggle.com competition. We want to try to\nunderstand the kinds of data in each field of the file. The following\nwould be particularly useful if the data were in many files or the data\nwere many gigabytes in size.\n\n1. First, take the fourth column. Figure out the unique values in that\ncolumn.\n2. Next, automate the process of determining if any of the values are\nnon-numeric so that you don't have to scan through all of the unique\nvalues looking for non-numbers. You'll need to look for the following\nregular expression pattern `[^0-9]`, which is interpreted as NOT any\nof the numbers 0 through 9.\n3. Now, do it for all the fields, except the first one. Have your code\nprint out the result in a human-readable way understandable by someone\nwho didn't write the code.\n\n\n## 4.3 Third challenge\n\n1.  For Belgium, determine the minimum unemployment value (field #6) in\n    `cpds.csv` in a programmatic way.\n2.  Have what is printed out to the screen look like \"Belgium 6.2\".\n3.  Now store the unique values of the countries in a variable, first\n    stripping out the quotation marks.\n4.  Figure out how to automate step 1 to do the calculation for all the\n    countries and print to the screen.\n5.  How would you instead store the results in a new file?\n\n\n## 4.4 Fourth challenge\n\nLet's return to the `RTADataSub.csv` file and the issue of missing values.\n\n1. Create a new file without any rows that have an 'x' (which indicate a missing value).\n2. Turn the code into a function that also prints out the number of rows that are being removed and that sends its output to stdout so that it can be used with piping.\n3. Now modify your function so that the user could provide the missing value string, the input\nfilename and the output filename as arguments.\n\n## 4.5 Fifth challenge\n\nConsider the `coop.txt` weather station file.\n\nFigure out how to use `grep` to tell you the starting position of the state field.\nHints: search for a known state-country combination and figure out\nwhat flags you can use with `grep` to print out the \"byte offset\" for the matched\nstate.\n\nUse that information to automate the [first mission](#bash-shell-examples) where we extracted\nthe state field using `cut`. You'll need to do a bit of arithmetic using shell commands.\n\n## 4.6 Sixth challenge\n\nHere's an advanced one - you'll probably need to use `sed`, but the\nbrief examples of text substitution in the using bash tutorial (or in the demos above) should be\nsufficient to solve the problem.\n\nConsider a CSV file that has rows that look like this:\n\n```\n1,\"America, United States of\",45,96.1,\"continental, coastal\" \n2,\"France\",33,807.1,\"continental, coastal\"\n```\n\nWhile Pandas would be able to handle this using `read_csv()`, using `cut`\nin UNIX won't work because of the commas embedded within the fields. The\nchallenge is to convert this file to one that we can use `cut` on, as\nfollows.\n\nFigure out a way to make this into a new delimited file in which the\ndelimiter is not a comma. At least one solution that will work for this\nparticular two-line dataset does not require you to use regular\nexpressions, just simple replacement of fixed patterns.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}