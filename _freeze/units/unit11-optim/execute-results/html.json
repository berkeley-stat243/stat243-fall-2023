{
  "hash": "8dd1e2d92dd490e7624bb228ca2ea44e",
  "result": {
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-11-03\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\nfrom: markdown+tex_math_single_backslash\n---\n\n[PDF](./unit11-optim.pdf){.btn .btn-primary}\n\nReferences:\n\n-   Gentle: *Computational Statistics*\n-   Lange: *Optimization*\n-   Monahan: *Numerical Methods of Statistics*\n-   Givens and Hoeting: *Computational Statistics*\n-   Materials online from Stanford's [EE364a\n    course](http://www.stanford.edu/class/ee364a/lectures.html) on\n    convex optimization, including [Boyd and Vandenberghe's (online)\n    book Convex\n    Optimization](https://web.stanford.edu/~boyd/cvxbook/).\n\nVideos (optional): \n\nThere are various videos from 2020 in the bCourses Media Gallery that you\ncan use for reference if you want to. \n\n  - Video 1. Convergence in optimization\n  - Video 2. Profiling\n  - Video 3. Multivariate Newton-Raphson\n  - Video 4. Descent methods and Newton-like methods\n  - Video 5. Stochastic gradient descent\n  - Video 6. Coordinate descent\n  - Video 7. Nelder-Mead\n  - Video 8. Optimization in practice\n  - Video 9. Introduction to optimization under constraints\n  - Video 10. Optimization under equality constraints\n  - Video 11. Barrier method for constrained optimization\n\n# 1. Notation\n\nWe'll make use of the first derivative (the gradient) and second\nderivative (the Hessian) of functions. We'll generally denote univariate\nand multivariate functions (without distinguishing between them) as\n$f(x)$ with $x=(x_{1},\\ldots,x_{p})$. The (column) vector of first\npartial derivatives (the gradient) is\n$f^{\\prime}(x)=\\nabla f(x)=(\\frac{\\partial f}{\\partial x_{1}},\\ldots,\\frac{\\partial f}{\\partial x_{p}})^{\\top}$\nand the matrix of second partial derivatives (the Hessian) is\n$$f^{\\prime\\prime}(x)=\\nabla^{2}f(x)=H_{f}(x)=\\left(\\begin{array}{cccc}\n\\frac{\\partial^{2}f}{\\partial x_{1}^{2}} & \\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{2}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{p}}\\\\\n\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{2}} & \\frac{\\partial^{2}f}{\\partial x_{2}^{2}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{p}}\\\\\n\\vdots & \\vdots & \\ddots\\\\\n\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{p}} & \\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{p}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{p}^{2}}\n\\end{array}\\right).$$ In considering iterative algorithms, I'll use\n$x_{0},\\,x_{1},\\ldots,x_{t},\\,x_{t+1}$ to indicate the sequence of\nvalues as we search for the optimum, denoted $x^{*}$. $x_{0}$ is the\nstarting point, which we must choose (often carefully). If it's unclear\nat any point whether I mean a value of $x$ in the sequence or a\nsub-element of the $x$ vector, let me know, but hopefully it will be\nclear from context most of the time.\n\nI'll try to use $x$ (or if we're talking explicitly about a likelihood,\n$\\theta$) to indicate the argument with respect to which we're\noptimizing and $Y$ to indicate data involved in a likelihood. I'll try\nto use $z$ to indicate covariates/regressors so there's no confusion\nwith $x$.\n\n# 2. Overview\n\nThe basic goal here is to optimize a function numerically when we cannot\nfind the maximum (or minimum) analytically. Some examples:\n\n1.  Finding the MLE for a GLM\n\n2.  Finding least squares estimates for a nonlinear regression model,\n    $$Y_{i}\\sim\\mathcal{N}(g(z_{i};\\beta),\\sigma^{2})$$ where $g(\\cdot)$\n    is nonlinear and we seek to find the value of\n    $\\theta=(\\beta,\\sigma^{2})$ that best fits the data.\n\n3.  Maximizing a likelihood under constraints\n\n4.  Fitting a machine learning prediction method\n\nMaximum likelihood estimation and variants thereof is a standard\nsituation in which optimization comes up.\n\nWe'll focus on **minimization**, since any maximization of $f$ can be\ntreated as minimization of $-f$. The basic setup is to find the\n*argument*, $x$, that minimizes $f(x)$: $$x^{*}=\\arg\\min_{x\\in D}f(x)$$\nwhere $D$ is the domain. Sometimes $D=\\Re^{p}$ but other times it\nimposes constraints on $x$. When there are no constraints, this is\nunconstrained optimization, where any $x$ for which $f(x)$ is defined is\na possible solution. We'll assume that $f$ is continuous as there's\nlittle that can be done systematically if we're dealing with a\ndiscontinuous function.\n\nIn one dimension, minimization is the same as root-finding with the\nderivative function, since the minimum of a differentiable function can\nonly occur at a point at which the derivative is zero. So with\ndifferentiable functions we'll seek to find $x^{*}$ s.t.\n$f^{\\prime}(x^{*})=\\nabla f(x^{*})=0$. To ensure a minimum, we want that\nfor all $y$ in a neighborhood of $x^{*}$, $f(y)\\geq f(x^{*})$, or (for\ntwice differentiable functions) $f^{\\prime\\prime}(x^{*})\\geq0$.\n\nIn more than one dimension, we want that the Hessian evaluated at\n$x^{*}$ is positive semi-definite, which tells us that moving in any\ndirection away from $x^{*}$ would not go downhill.\n\nDifferent strategies are used depending on whether $D$ is discrete and\ncountable, or continuous, dense and uncountable. We'll concentrate on\nthe continuous case but the discrete case can arise in statistics, such\nas in doing variable selection.\n\nIn general we rely on the fact that we can evaluate $f$. Often we make\nuse of analytic or numerical derivatives of $f$ as well.\n\nTo some degree, optimization is a solved problem, with good software\nimplementations, so it raises the question of how much to discuss in\nthis class. The basic motivation for going into some of the basic\nclasses of optimization strategies is that the function being optimized\nchanges with each problem and can be tricky to optimize, and I want you\nto know something about how to choose a good approach when you find\nyourself with a problem requiring optimization. Finding global, as\nopposed to local, minima can also be an issue.\n\nNote that I'm not going to cover MCMC (Markov chain Monte Carlo)\nmethods, which are used for approximating integrals and sampling from\nposterior distributions in a Bayesian context and in a variety of ways\nfor optimization. If you take a Bayesian course you'll cover this in\ndetail, and if you don't do Bayesian work, you probably won't have much\nneed for MCMC, though it comes up in MCEM (Monte Carlo EM) and simulated\nannealing, among other places.\n\n#### Goals for the unit\n\nOptimization is a big topic. Here's what I would like you to get out of\nthis:\n\n1.  an understanding of line searches (one-dimensional optimization),\n2.  an understanding of multivariate derivative-based optimization and\n    how line searches are useful within this,\n3.  an understanding of derivative-free methods,\n4.  an understanding of the methods used in R's optimization routines,\n    their strengths and weaknesses, and various tricks for doing better\n    optimization in R, and\n5.  a basic idea of what convex optimization is and when you might want\n    to go learn more about it.\n\n# 3. Univariate function optimization\n\nWe'll start with some strategies for univariate functions. These can be\nuseful later on in dealing with multivariate functions.\n\n## Golden section search\n\nThis strategy requires only that the function be unimodal.\n\nAssume we have a single minimum, in $[a,b]$. We choose two points in the\ninterval and evaluate them, $f(x_{1})$ and $f(x_{2})$. If\n$f(x_{1})<f(x_{2})$ then the minimum must be in $[a,x_{2}]$, and if the\nconverse in $[x_{1},b]$. We proceed by choosing a new point in the new,\nsmaller interval and iterate. At each step we reduce the length of the\ninterval in which the minimum must lie. The primary question involves\nwhat is an efficient rule to use to choose the new point at each\niteration.\n\nSuppose we start with $x_{1}$ and $x_{2}$ s.t. they divide $[a,b]$ into\nthree equal segments. Then we use $f(x_{1})$ and $f(x_{2})$ to rule out\neither the leftmost or rightmost segment based on whether\n$f(x_{1})<f(x_{2})$. If we have divided equally, we cannot place the\nnext point very efficiently because either $x_{1}$ or $x_{2}$ equally\ndivides the remaining space, so we are forced to divide the remaining\nspace into relative lengths of 0.25, 0.25, and 0.5. The next time\naround, we may only rule out the shorter segment, which leads to\ninefficiency.\n\nThe efficient strategy is to maintain the *golden ratio* between the\ndistances between the points using $\\phi=(\\sqrt{5}-1)/2\\approx.618$ (the\ngolden ratio), which is determined by solving for $\\phi$ in this\nequation: $\\phi-\\phi^{2}=2\\phi-1$. We start with $x_{1}=a+(1-\\phi)(b-a)$\nand $x_{2}=a+\\phi(b-a)$. Then suppose $f(x_{1})<f(x_{2})$ so the minimum\nmust be in $[a,x_{2}]$. Since $x_{1}-a>x_{2}-x_{1}$, we now choose\n$x_{3}$ in the interval $[a,x_{1}]$ to produce three subintervals,\n$[a,x_{3}],\\,[x_{3},x_{1}],\\,[x_{1},x_{2}]$. We choose to place $x_{3}$\ns.t. it uses the golden ratio in the interval $[a,x_{1}]$, namely\n$x_{3}=a+(1-\\phi)(x_{2}-a)$. This means that the length of the first\nsubinterval is $(\\phi-\\phi^{2})(b-a)$ and the length of the third\nsubinterval is $(2\\phi-1)(b-a)$, but those lengths are equal because we\nfound $\\phi$ to satisfy $\\phi-\\phi^{2}=2\\phi-1$.\n\nThe careful choice of $\\phi$ allows us to narrow the search interval by\nan equal proportion,$1-\\phi$, in each iteration. Eventually we have\nnarrowed the minimum to between $x_{t-1}$ and $x_{t}$, where the\ndifference $|x_{t}-x_{t-1}|$ is sufficiently small (within some\ntolerance - see Section 4 for details), and we report\n$(x_{t}+x_{t-1})/2$.\n\n## Bisection method \n\nThe bisection method requires the existence of the first derivative but\nhas the advantage over the golden section search of halving the interval\nat each step. We again assume unimodality.\n\nWe start with an initial interval $(a_{0},b_{0})$ and proceed to shrink\nthe interval. Let's choose $a_{0}$ and $b_{0}$, and set $x_{0}$ to be\nthe mean of these endpoints. Now we update according to the following\nalgorithm, assuming our current interval is $[a_{t},b_{t}]$.\n\n- If $f^{\\prime}(a_{t})f^{\\prime}(x_{t})<0$, then $[a_{t+1},b_{t+1}] = [a_{t},x_{t}]$\n- If $f^{\\prime}(a_{t}) f^{\\prime}(x_{t})>0$, then $[a_{t+1},b_{t+1}] = [x_{t},b_{t}]$\n\nand set $x_{t+1}$ to the mean of $a_{t+1}$ and $b_{t+1}$.\nThe basic idea is that if the derivative at both $a_{t}$ and $x_{t}$ is\nnegative, then the minimum must be between $x_{t}$ and $b_{t}$, based on\nthe intermediate value theorem. If the derivatives at $a_{t}$ and\n$x_{t}$ are of different signs, then the minimum must be between $a_{t}$\nand $x_{t}$.\n\nSince the bisection method reduces the size of the search space by\none-half at each iteration, one can work out that each decimal place of\nprecision requires 3-4 iterations. Obviously bisection is more efficient\nthan the golden section search because we reduce by $0.5>0.382=1-\\phi$,\nso we've gained information by using the derivative. It requires an\nevaluation of the derivative however, while golden section just requires\nan evaluation of the original function.\n\nBisection is an example of a *bracketing* method, in which we trap the\nminimum within a nested sequence of intervals of decreasing length.\nThese tend to be slow, but if the first derivative is continuous, they\nare robust and don't require that a second derivative exist.\n\n## Newton-Raphson (Newton's method)\n\n### Overview\n\nWe'll talk about Newton-Raphson (N-R) as an optimization method rather\nthan a root-finding method, but they're just different perspectives on\nthe same algorithm.\n\nFor N-R, we need two continuous derivatives that we can evaluate. The\nbenefit is speed, relative to bracketing methods. We again assume the\nfunction is unimodal. The minimum must occur at $x^{*}$ s.t.\n$f^{\\prime}(x^{*})=0$, provided the second derivative is non-negative at\n$x^{*}$. So we aim to find a zero (a root) of the first derivative\nfunction. Assuming that we have an initial value $x_{0}$ that is close\nto $x^{*}$, we have the Taylor series approximation\n$$f^{\\prime}(x)\\approx f^{\\prime}(x_{0})+(x-x_{0})f^{\\prime\\prime}(x_{0}).$$\nNow set $f^{\\prime}(x)=0$, since that is the condition we desire (the\ncondition that holds when we are at $x^{*}$), and solve for $x$ to get\n$$x_{1}=x_{0}-\\frac{f^{\\prime}(x_{0})}{f^{\\prime\\prime}(x_{0})},$$ and\niterate, giving us updates of the form\n$x_{t+1}=x_{t}-\\frac{f^{\\prime}(x_{t})}{f^{\\prime\\prime}(x_{t})}$. What\nare we doing intuitively? Basically we are taking the tangent to $f(x)$\nat $x_{0}$ and extrapolating along that line to where it crosses the\nx-axis to find $x_{1}$. We then reevaluate $f(x_{1})$ and continue to\ntravel along the tangents.\n\nOne can prove that if $f^{\\prime}(x)$ is twice continuously\ndifferentiable, is convex, and has a root, then N-R converges from any\nstarting point.\n\nNote that we can also interpret the N-R update as finding the analytic\nminimum of the quadratic Taylor series approximation to $f(x)$.\n\nNewton's method converges very quickly (as we'll discuss in Section 4),\nbut if you start too far from the minimum, you can run into serious\nproblems.\n\n### Secant method variation on N-R\n\nSuppose we don't want to calculate the second derivative required in the\ndivisor of N-R. We might replace the analytic derivative with a discrete\ndifference approximation based on the secant line joining\n$(x_{t},f^{\\prime}(x_{t}))$ and $(x_{t-1},f^{\\prime}(x_{t-1}))$, giving\nan approximate second derivative:\n$$f^{\\prime\\prime}(x_{t})\\approx\\frac{f^{\\prime}(x_{t})-f^{\\prime}(x_{t-1})}{x_{t}-x_{t-1}}.$$\nFor this variant on N-R, we need two starting points, $x_{0}$ and\n$x_{1}$.\n\nAn alternative to the secant-based approximation is to use a standard\ndiscrete approximation of the derivative such as\n$$f^{\\prime\\prime}(x_{t})\\approx\\frac{f^{\\prime}(x_{t}+h)-f^{\\prime}(x_{t}-h)}{2h}.$$\n\n### How can Newton's method go wrong?\n\nLet's think about what can go wrong - namely when we could have\n$f(x_{t+1})>f(x_{t})$? To be concrete (and without loss of generality),\nlet's assume that $f(x_{t})>0$, in other words that $x^{*}<x_{t}$.\n\n1.  As usual, we can develop some intuition by starting with the worst\n    case that $f^{\\prime\\prime}(x_{t})$ is 0, in which case the method\n    would fail as $x_{t+1}$ would be $-\\infty$.\n2.  Now suppose that $f^{\\prime\\prime}(x_{t})$ is a small positive\n    number. Basically, if $f^{\\prime}(x_{t})$ is relatively flat, we can\n    get that $|x_{t+1}-x^{*}|>|x_{t}-x^{*}|$ because we divide by a\n    small value for the second derivative, causing $x_{t+1}$ to be far\n    from $x_{t}$ (though it does at least go in the correct direction).\n    We'll see an example on the board and the demo code (see below).\n3.  Newton's method can also go uphill (going in the wrong direction,\n    away from $x^{*}$) when the second derivative is negative, with the\n    method searching for a maximum, since we would have $x_{t+1}>x_{t}$.\n    Another way to think of this is that Newton's method does not\n    automatically minimize the function, rather it finds local optima.\n\nIn all these cases Newton's method could diverge, failing to converge on\nthe optimum.\n\n#### Divergence\n\nFirst let's see an example of divergence. The left and middle panels\nshow two cases of convergence, while the right panel shows divergence.\nIn the right panel, the initial second derivative value is small enough\nthat $x_{2}$ is further from $x^{*}$ than $x_{1}$ and then $x_{3}$ is\nyet further away. In all cases the sequence of $x$ values is indicated\nby the red letters.\n\n::: {.cell fig-height='3' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef fp(x, theta=1):\n    ## First derivative - we want the root of this.\n    return np.exp(x * theta) / (1 + np.exp(x * theta)) - 0.5\n\ndef fpp(x, theta=1):\n    ## Second derivative - used to scale the optimization steps.\n    return np.exp(x * theta) / ((1 + np.exp(x * theta)) ** 2)\n\ndef make_plot(xs, xvals, fp, fpp, subplot, title):\n    plt.plot(xs, fp(xs), '-', label=\"f'(x)\", color = 'grey')\n    plt.plot(xs, fpp(xs), '--', label=\"f''(x)\", color = 'grey')\n    for i in range(len(xvals)):\n        plt.text(xvals[i], fp(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"f'(x)\")\n    plt.title(title)\n    plt.legend(loc='upper left')\n\n \nxs = np.linspace(-15, 15, 300)\n\nn = 10\nxvals = np.zeros(n)\n\n## Good starting point\nx0 = 1\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals, fp, fpp, 1, \"converges quickly\")\nplt.show()\n\n## Ok starting point\nx0 = 2\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals, fp, fpp, 2, \"converges\")\nplt.show()\n\n## Bad starting point\n\nx0 = 2.5\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals[np.abs(xvals) < 15], fp, fpp, 3, \"diverges\")\n## whoops!\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-2-output-1.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-2-output-2.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_483690/3161030548.py:58: RuntimeWarning:\n\ndivide by zero encountered in double_scalars\n\n/tmp/ipykernel_483690/3161030548.py:6: RuntimeWarning:\n\ninvalid value encountered in double_scalars\n\n/tmp/ipykernel_483690/3161030548.py:10: RuntimeWarning:\n\ninvalid value encountered in double_scalars\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-2-output-4.png){width=600 height=449}\n:::\n:::\n\n\n#### Multiple optima: converging to the wrong optimum\n\nIn the first row of the next figure, let's see an example of climbing\nuphill and finding a local maximum rather than minimum. The other rows\nshow convergence. In all cases the minimum is at $x^{*}\\approx3.14$\n\n::: {.cell fig-height='7' execution_count=2}\n``` {.python .cell-code}\n# Define the original function\ndef f(x):\n    return np.cos(x)\n\n# Define the gradient\ndef fp(x):\n    return -np.sin(x)\n\n# Define the second derivative\ndef fpp(x):\n    return -np.cos(x)# original fxn\n\ndef make_plot2(xs, xvals, f, fp, num, title):\n    # gradient subplot\n    plt.subplot(3, 2, num)\n    plt.plot(xs, fp(xs), '-', label=\"f'(x)\")\n    plt.scatter(np.pi, fp(np.pi))\n    for i in range(len(xvals)):\n        plt.text(xvals[i], fp(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel('x')\n    plt.ylabel(\"f'(x)\")\n    plt.title(title[0])\n    plt.legend(loc='lower right')\n    # function subplot\n    plt.subplot(3, 2, num+1)\n    plt.plot(xs, f(xs), '-', label=\"f(x)\")\n    plt.scatter(np.pi, f(np.pi))\n    for i in range(len(xvals)):\n        plt.text(xvals[i], f(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel('x')\n    plt.ylabel(\"f(x)\")\n    plt.title(title[1])\n    plt.legend(loc='lower right')\n \n\nxs = np.linspace(0, 2 * np.pi, num=300)\n\nx0 = 5.5 # starting point\nfp(x0) # positive\nfpp(x0) # negative\nx1 = x0 - fp(x0)/fpp(x0) # whoops, we've gone uphill \n## because of the negative second derivative\nxvals = np.zeros(n)\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n## print(xvals)\n\nplt.figure(figsize=(10, 7))\n\nmake_plot2(xs, xvals, f, fp, 1, title =\n    ['uphill to local maximum, gradient view', 'uphill to local maximum, function view'])\n\n## In contrast, with better starting points we can find the minimum\n## (but this nearly diverges).\n\nx0 = 4.3 # ok starting point\nfp(x0) \nfpp(x0) \nx1 = x0 - fp(x0)/fpp(x0)  # going downhill\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n## print(xvals)\n\nmake_plot2(xs, xvals, f, fp, 3, title =\n    ['nearly diverges, gradient view', 'nearly diverges, function view'])\n\n## With a better starting point, we converge quickly.\n\nx0 = 3.8 # good starting point\nfp(x0) \nfpp(x0) \nx1 = x0 - fp(x0)/fpp(x0) \n\nmake_plot2(xs, xvals, f, fp, 5, title =\n    ['better starting point, gradient view', 'better starting point, function view'])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-3-output-1.png){width=830 height=597}\n:::\n:::\n\n\n#### Improving Newton's method\n\nOne nice, general idea is to use a fast method such as Newton's method\n*safeguarded* by a robust, but slower method. Here's how one can do this\nfor N-R, safeguarding with a bracketing method such as bisection.\nBasically, we check the N-R proposed move to see if N-R is proposing a\nstep outside of where the root is known to lie based on the previous\nsteps and the gradient values for those steps. If so, we could choose\nthe next step based on bisection.\n\nAnother approach is backtracking. If a new value is proposed that yields\na larger value of the function, backtrack to find a value that reduces\nthe function. One possibility is a line search but given that we're\ntrying to reduce computation, a full line search is often unwise\ncomputationally (also in the multivariate Newton's method, we are in the\nmiddle of an iterative algorithm for which we will just be going off in\nanother direction anyway at the next iteration). A basic approach is to\nkeep backtracking in halves. A nice alternative is to fit a polynomial\nto the known information about that slice of the function, namely\n$f(x_{t+1})$, $f(x_{t})$, $f^{\\prime}(x_{t})$ and\n$f^{\\prime\\prime}(x_{t})$ and find the minimum of the polynomial\napproximation.\n\n# 4. Convergence ideas \n\n## Convergence metrics\n\nWe might choose to assess whether $f^{\\prime}(x_{t})$ is near zero,\nwhich should assure that we have reached the critical point. However, in\nparts of the domain where $f(x)$ is fairly flat, we may find the\nderivative is near zero even though we are far from the optimum.\nInstead, we generally monitor $|x_{t+1}-x_{t}|$ (for the moment, assume\n$x$ is scalar). We might consider absolute convergence:\n$|x_{t+1}-x_{t}|<\\epsilon$ or relative convergence,\n$\\frac{|x_{t+1}-x_{t}|}{|x_{t}|}<\\epsilon$. Relative convergence is\nappealing because it accounts for the scale of $x$, but it can run into\nproblems when $x_{t}$ is near zero, in which case one can use\n$\\frac{|x_{t+1}-x_{t}|}{|x_{t}|+\\epsilon}<\\epsilon$. We would want to\naccount for machine precision in thinking about setting $\\epsilon$. For\nrelative convergence a reasonable choice of $\\epsilon$ would be to use\nthe square root of machine epsilon or about $1\\times10^{-8}$. \n\nProblems with the optimization may show up in a convergence measure that\nfails to decrease or cycles (oscillates). Software generally has a\nstopping rule that stops the algorithm after a fixed number of\niterations; these can generally be changed by the user. When an\nalgorithm stops because of the stopping rule before the convergence\ncriterion is met, we say the algorithm has failed to converge. Sometimes\nwe just need to run it longer, but often it indicates a problem with the\nfunction being optimized or with your starting value.\n\nFor multivariate optimization, we use a distance metric between\n$x_{t+1}$ and $x_{t}$, such as $\\|x_{t+1}-x_{t}\\|_{p}$ , often with\n$p=1$ or $p=2$.\n\n## Starting values\n\nGood starting values are important because they can improve the speed of\noptimization, prevent divergence or cycling, and prevent finding local\noptima.\n\nUsing random or selected multiple starting values can help with multiple\noptima (aka multimodality).\n\nHere's a function (the Rastrigin function) with multiple optima that is\ncommonly used for testing methods that claim to work well for multimodal\nproblems. This is a hard function to optimize with respect to,\nparticularly in higher dimensions (one can do it in higher dimensions\nthan 2 by simply making the $x$ vector longer but having the same\nstructure). In particular Rastrigin with 30 dimensions is considered to\nbe very hard.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef rastrigin(x):\n    A = 10\n    n = len(x)\n    return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\nconst = 5.12\nnGrid = 100\ngr = np.linspace(-const, const, num=nGrid)\n\n# Create a grid of x values\nx1, x2 = np.meshgrid(gr, gr)\nxs = np.column_stack((x1.ravel(), x2.ravel()))\n\n# Calculate the Rastrigin function for each point in the grid\ny = np.apply_along_axis(rastrigin, 1, xs)\n\n# Create a plot\nplt.figure(figsize=(8, 6))\nplt.imshow(y.reshape((nGrid, nGrid)), extent=[-const, const, -const, const], origin='lower', cmap='viridis')\nplt.colorbar()\nplt.title('Rastrigin Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-4-output-1.png){width=588 height=523}\n:::\n:::\n\n\n## Convergence rates\n\nLet $\\epsilon_{t}=|x_{t}-x^{*}|$. If the limit\n\n$$\\lim_{t\\to\\infty}\\frac{|\\epsilon_{t+1}|}{|\\epsilon_{t}|^{\\beta}}=c$$\nexists for $\\beta>0$ and $c\\ne0$, then a method is said to have order of\nconvergence $\\beta$. This basically measures how big the error at the\n$t+1$th iteration is relative to that at the $t$th iteration, with the\napproximation that $|\\epsilon_{t+1}|\\approx c|\\epsilon_{t}|^{\\beta}$.\n\nBisection doesn't formally satisfy the criterion needed to make use of\nthis definition, but roughly speaking it has linear convergence\n($\\beta=1$), so the magnitude of the error decreases by a factor of $c$\nat each step. Next we'll see that N-R has quadratic convergence\n($\\beta=2$), which is fast.\n\nTo analyze convergence of N-R, consider a Taylor expansion of the\ngradient at the minimum, $x^{*}$, around the current value, $x_{t}$:\n$$f^{\\prime}(x^{*})=f^{\\prime}(x_{t})+(x^{*}-x_{t})f^{\\prime\\prime}(x_{t})+\\frac{1}{2}(x^{*}-x_{t})^{2}f^{\\prime\\prime\\prime}(\\xi_{t})=0,$$\nfor some $\\xi_{t}\\in[x^{*},x_{t}]$. Making use of the N-R update\nequation:\n$x_{t+1}=x_{t}-\\frac{f^{\\prime}(x_{t})}{f^{\\prime\\prime}(x_{t})}$ to\nsubstitute , and some algebra, we have\n$$\\frac{|x^{*}-x_{t+1}|}{(x^{*}-x_{t})^{2}}=\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(\\xi_{t})}{f^{\\prime\\prime}(x_{t})}.$$\nIf the limit of the ratio on the right hand side exists and is equal to\n$c$:\n$$c=\\lim_{x_{t}\\to x^{*}}\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(\\xi_{t})}{f^{\\prime\\prime}(x_{t})}=\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(x^{*})}{f^{\\prime\\prime}(x^{*})}$$\nthen we see that $\\beta=2$.\n\nIf $c$ were one, then we see that if we have $k$ digits of accuracy at\n$t$, we'd have $2k$ digits at $t+1$ (e.g., $|\\epsilon_{t}|=0.01$ results\nin $|\\epsilon_{t+1}|=0.0001$), which justifies the characterization of\nquadratic convergence being fast. In practice $c$ will moderate the rate\nof convergence. The smaller $c$ the better, so we'd like to have the\nsecond derivative be large and the third derivative be small. The\nexpression also indicates we'll have a problem if\n$f^{\\prime\\prime}(x_{t})=0$ at any point (think about what this\ncorresponds to graphically - what is our next step when\n$f^{\\prime\\prime}(x_{t})=0$?). The characteristics of the derivatives\ndetermine the domain of attraction (the region in which we'll converge\nrather than diverge) of the minimum.\n\nGivens and Hoeting show that using the secant-based approximation to the\nsecond derivative in N-R has order of convergence, $\\beta\\approx1.62$.\n\nHere's an example of convergence comparing bisection and N-R. First, Newton-Raphson:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnp.set_printoptions(precision=10)\n\n# Define the original function\ndef f(x):\n    return np.cos(x)\n\n# Define the gradient\ndef fp(x):\n    return -np.sin(x)\n\n# Define the second derivative\ndef fpp(x):\n    return -np.cos(x)\n\nxstar = np.pi  # known minimum\n\n## Newton-Raphson (N-R) method\nx0 = 2\nn_it = 10\nxvals = np.zeros(n_it)\nxvals[0] = x0\nfor t in range(1, n_it):\n    xvals[t] = xvals[t - 1] - fp(xvals[t - 1]) / fpp(xvals[t - 1])\n\nprint(xvals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.           4.1850398633 2.4678936745 3.2661862776 3.1409439123\n 3.1415926537 3.1415926536 3.1415926536 3.1415926536 3.1415926536]\n```\n:::\n:::\n\n\nNext, here is bisection:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n## Bisection method\ndef bisec_step(interval, fp):\n    interval = interval.copy()\n    xt = np.mean(interval)\n    if fp(interval[0]) * fp(xt) <= 0:\n        interval[1] = xt\n    else:\n        interval[0] = xt\n    return interval\n\nn_it = 30\na0 = 2\nb0 = (3 * np.pi / 2) - (xstar - a0)\ninterval = np.zeros((n_it, 2))\ninterval[0,:] = [a0, b0]\n\nfor t in range(1, n_it):\n    interval[t,:] = bisec_step(interval[t-1,:], fp)\n\nprint(np.mean(interval, axis=1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.7853981634 3.1780972451 2.9817477042 3.0799224747 3.1290098599\n 3.1535535525 3.1412817062 3.1474176293 3.1443496678 3.142815687\n 3.1420486966 3.1416652014 3.1414734538 3.1415693276 3.1416172645\n 3.141593296  3.1415813118 3.1415873039 3.1415903    3.141591798\n 3.141592547  3.1415929215 3.1415927343 3.1415926406 3.1415926875\n 3.1415926641 3.1415926524 3.1415926582 3.1415926553 3.1415926538]\n```\n:::\n:::\n\n\n# 5. Multivariate optimization\n\nOptimizing as the dimension of the space gets larger becomes\nincreasingly difficult:\n\n1.  In high dimensions, there are many possible directions to go.\n\n2.  One can end up having to do calculations with large vectors and\n    matrices.\n\n3.  Multimodality increasingly becomes a concern (and can be hard to\n    detect).\n\nFirst we'll discuss the idea of profiling to reduce dimensionality and\nthen we'll talk about various numerical techniques, many of which build\noff of Newton's method (using second derivative information). We'll\nfinish by talking about methods that only use the gradient (and not the\nsecond derivative) and methods that don't use any derivative\ninformation.\n\n## Profiling\n\nA core technique for likelihood optimization is to analytically maximize\nover any parameters for which this is possible. Suppose we have two sets\nof parameters, $\\theta_{1}$ and $\\theta_{2}$, and we can analytically\nmaximize w.r.t $\\theta_{2}$. This will give us\n$\\hat{\\theta}_{2}(\\theta_{1})$, a function of the remaining parameters\nover which analytic maximization is not possible. Plugging in\n$\\hat{\\theta}_{2}(\\theta_{1})$ into the objective function (in this case\ngenerally the likelihood or log likelihood) gives us the profile (log)\nlikelihood solely in terms of the obstinant parameters. For example,\nsuppose we have the regression likelihood with correlated errors:\n$$Y\\sim\\mathcal{N}(X\\beta,\\sigma^{2}\\Sigma(\\rho)),$$ where\n$\\Sigma(\\rho)$ is a correlation matrix that is a function of a\nparameter, $\\rho$. The maximum w.r.t. $\\beta$ is easily seen to be the\nGLS estimator\n$\\hat{\\beta}(\\rho)=(X^{\\top}\\Sigma(\\rho)^{-1}X)^{-1}X^{\\top}\\Sigma(\\rho)^{-1}Y$.\n(In general such a maximum is a function of all of the other parameters,\nbut conveniently it's only a function of $\\rho$ here.) This gives us the\ninitial profile likelihood\n$$\\frac{1}{(\\sigma^{2})^{n/2}|\\Sigma(\\rho)|^{1/2}}\\exp\\left(-\\frac{(Y-X\\hat{\\beta}(\\rho))^{-\\top}\\Sigma(\\rho)^{-1}(Y-X\\hat{\\beta}(\\rho))}{2\\sigma^{2}}\\right).$$\nWe then notice that the likelihood is maximized w.r.t. $\\sigma^{2}$ at\n$$\\hat{\\sigma^{2}}(\\rho)=\\frac{(Y-X\\hat{\\beta}(\\rho))^{\\top}\\Sigma(\\rho)^{-1}(Y-X\\hat{\\beta}(\\rho))}{n}.$$\nThis gives us the final profile likelihood,\n$$\\frac{1}{|\\Sigma(\\rho)|^{1/2}}\\frac{1}{(\\hat{\\sigma^{2}}(\\rho))^{n/2}}\\exp(-\\frac{1}{2}n),$$\na function of $\\rho$ only, for which numerical optimization is much\nsimpler.\n\n## Newton-Raphson (Newton's method)\n\nFor multivariate $x$ we have the Newton-Raphson update\n$x_{t+1}=x_{t}-f^{\\prime\\prime}(x_{t})^{-1}f^{\\prime}(x_{t})$, or in our\nother notation, $$x_{t+1}=x_{t}-H_{f}(x_{t})^{-1}\\nabla f(x_{t}).$$\n\nLet's consider a very simple example of nonlinear least squares.\nWe'll use the famous Mauna Loa atmospheric carbon dioxide record.\n\nLet's suppose (I have no real reason to think this) that we\nthink that the data can be well-represented by this nonlinear model:\n$$Y_{i}=\\beta_{0}+\\beta_{1}\\exp(t_i/\\beta_{2}+\\epsilon_{i}.$$\n\nSome of the things we need to worry about with Newton's method in\ngeneral about are (1) good starting values, (2) positive definiteness of\nthe Hessian, and (3) avoiding errors in deriving the derivatives.\n\nA note on the positive definiteness: since the Hessian may not be\npositive definite (although it may well be, provided the function is\napproximately locally quadratic), one can consider modifying the\nCholesky decomposition of the Hessian to enforce positive definiteness\nby adding diagonal elements to $H_{f}$ as necessary.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_csv('co2_annmean_mlo.csv', header = 0, names = ['year','co2','unc'])\n\nplt.scatter(data.year, data.co2)\nplt.xlabel('year')\nplt.ylabel(\"CO2\")\nplt.show()\n\n## Center years for better numerical behavior\ndata.year = data.year - np.mean(data.year)\n### Linear fit - not a good model\nX = sm.add_constant(data.year)  \nmodel = sm.OLS(data.co2, X).fit()\n\nplt.scatter(data.year, data.co2)\nplt.plot(data.year, model.fittedvalues, '-')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-7-output-1.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-7-output-2.png){width=575 height=411}\n:::\n:::\n\n\nWe need some starting values. Having centered the year variable,\n$\\beta_2$ seems plausibly like it would be order of magnitude of 10,\nwhich is about the magnitude of the year values.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nbeta2_init = 10\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nplt.scatter(data.year, data.co2)\n\ndef fit(params):\n    return params[0] + params[1] * np.exp(data.year / params[2])\n\nbeta = (beta0_init, beta1_init, beta2_init)\nplt.plot(data.year, fit(beta), '-')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-8-output-1.png){width=575 height=411}\n:::\n:::\n\n\nThat's not great. How about changing the scale of beta2 more?\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nbeta2_init = 100\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nplt.scatter(data.year, data.co2)\nbeta = (beta0_init, beta1_init, beta2_init)\nplt.plot(data.year, fit(beta), '-')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-9-output-1.png){width=575 height=411}\n:::\n:::\n\n\nLet's get derivative information using automatic differentation\n(the algorithmic implementation of the chain rule for derivatives also\nused in gradient descent in deep learning, as well as various other contexts).\nWe'll use Jax, but PyTorch or Tensorflow are other options.\nWe need to use the Jax versions of various numpy operations in order\nto be able to get the derivatives.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport jax.numpy as jnp\nimport jax\n\ndef loss(params):\n    fitted = params[0] + params[1]*jnp.exp(jnp.array(data.year)/params[2])\n    return jnp.sum((fitted - jnp.array(data.co2))**2.0)\n\nderiv1 = jax.grad(loss)\nderiv2 = jax.hessian(loss)\n\nderiv1(jnp.array([beta0_init, beta1_init, beta2_init]))\nhess = deriv2(jnp.array([beta0_init, beta1_init, beta2_init]))\nhess\n\nnp.linalg.eig(hess)[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([ 2.6369040e+02, -2.5794024e-03,  1.3906310e+01], dtype=float32)\n```\n:::\n:::\n\n\nThe Hessian is not positive definite. We could try tricks such\nas adding to the diagonal of the Hessian or using the pseudo-inverse\n(i.e., setting all negative eigenvalues in the inverse to zero).\n\nInstead, let's try a bit more to find starting values where\nthe Hessian is positive definite. The order of magnitude of\nour initial value for $\\beta_2$ seems about right, so let's\ntry halving or doubling it.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nbeta2_init = 50\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nhess = deriv2(jnp.array([beta0_init, beta1_init, beta2_init]))\n\nnp.linalg.eig(hess)[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([3.0403909e+02, 2.4109183e-01, 5.5710766e+01], dtype=float32)\n```\n:::\n:::\n\n\nThat seems better. Let's try with that.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nn_it = 10\nxvals = np.zeros(shape = (n_it, 3))\nxvals[0, :] = (beta0_init, beta1_init, beta2_init)\n\nfor t in range(1, n_it):\n  jxvals = jnp.array(xvals[t-1, :])\n  hess = deriv2(jxvals)\n  e = np.linalg.eig(hess)\n  if(np.any(e[0] < 0)):\n    raise ValueError(\"not positive definite\")\n  xvals[t, :] = xvals[t-1, :] - np.linalg.solve(hess, deriv1(jxvals))\n  print(loss(xvals[t,:]))\n\nbeta_hat = xvals[t,:]\n\nplt.scatter(data.year, data.co2)\nplt.plot(data.year, fit(beta_hat), 'r-')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n38.304596\n30.817617\n30.444086\n30.442629\n30.44241\n30.442383\n30.442421\n30.442448\n30.442455\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](unit11-optim_files/figure-html/cell-12-output-2.png){width=575 height=411}\n:::\n:::\n\n\nThat looks pretty good, but the lack of positive definiteness/sensitivity to starting values should make us\ncautious. That said, in this case we can visually\nassess the fit and see that it looks pretty good.\n\nNext we'll see that some optimization methods used commonly for\nstatistical models (in particular Fisher scoring and iterative\nreweighted least squares (IRLS or IWLS) are just Newton-Raphson in\ndisguise.\n\n## Fisher scoring variant on N-R (optional)\n\nThe Fisher information (FI) is the expected value of the outer product\nof the gradient of the log-likelihood with itself\n\n$$I(\\theta)=E_{f}(\\nabla f(y)\\nabla f(y)^{\\top}),$$\n\nwhere the expected value is with respect to the data distribution. Under regularity\nconditions (true for exponential families), the expectation of the\nHessian of the log-likelihood is minus the Fisher information,\n$E_{f}H_{f}(y)=-I(\\theta)$. We get the observed Fisher information by\nplugging the data values into either expression instead of taking the\nexpected value.\n\nThus, standard N-R can be thought of as using the observed Fisher\ninformation to find the updates. Instead, if we can compute the\nexpectation, we can use minus the FI in place of the Hessian. The result\nis the Fisher scoring (FS) algorithm. Basically instead of using the\nHessian for a given set of data, we are using the FI, which we can think\nof as the average Hessian over repeated samples of data from the data\ndistribution. FS and N-R have the same convergence properties (i.e.,\nquadratic convergence) but in a given problem, one may be\ncomputationally or analytically easier. Givens and Hoeting comment that\nFS works better for rapid improvements at the beginning of iterations\nand N-R better for refinement at the end. $$\\begin{aligned}\n(NR):\\,\\theta_{t+1} & = & \\theta_{t}-H_{f}(\\theta_{t})^{-1}\\nabla f(\\theta_{t})\\\\\n(FS):\\,\\theta_{t+1} & = & \\theta_{t}+I(\\theta_{t})^{-1}\\nabla f(\\theta_{t})\\end{aligned}$$\n\n\nThe Gauss-Newton algorithm for nonlinear least squares involves using\nthe FI in place of the Hessian in determining a Newton-like step.\n`nls()` in R uses this approach. \n\n#### Connections between statistical uncertainty and ill-conditionedness\n\nWhen either the observed or expected FI matrix is nearly singular this\nmeans we have a small eigenvalue in the inverse covariance (the\nprecision), which means a large eigenvalue in the covariance matrix.\nThis indicates some linear combination of the parameters has low\nprecision (high variance), and that in that direction the likelihood is\nnearly flat. As we've seen with N-R, convergence slows with shallow\ngradients, and we may have numerical problems in determining good\noptimization steps when the likelihood is sufficiently flat. So\nconvergence problems and statistical uncertainty go hand in hand. One,\nbut not the only, example of this occurs when we have nearly collinear\nregressors.\n\n## IRLS (IWLS) for Generalized Linear Models (GLMs)\n\nAs many of you know, iterative reweighted least squares (also called\niterative weighted least squares) is the standard method for estimation\nwith GLMs. It involves linearizing the model and using working weights\nand working variances and solving a weighted least squares (WLS) problem\n(recalling that the generic WLS solution is\n$\\hat{\\beta}=(X^{\\top}WX)^{-1}X^{\\top}WY$).\n\nExponential families can be expressed as\n$$f(y;\\theta,\\phi)=\\exp((y\\theta-b(\\theta))/a(\\phi)+c(y,\\phi)),$$ with\n$E(Y)=b^{\\prime}(\\theta)$ and $\\mbox{Var}(Y)=b^{\\prime\\prime}(\\theta)$.\nIf we have a GLM in the canonical parameterization (log link for Poisson\ndata, logit for binomial), we have the natural parameter $\\theta$ equal\nto the linear predictor, $\\theta=\\eta$. A standard linear predictor\nwould simply be $\\eta=X\\beta$.\n\nConsidering N-R for a GLM in the canonical parameterization (and\nignoring $a(\\phi)$, which is one for logistic and Poisson regression),\none can show that the gradient of the GLM log-likelihood is the inner\nproduct of the covariates and a residual vector,\n$\\nabla l(\\beta)=(Y-E(Y))^{\\top}X$, and the Hessian is\n$H_{l}(\\beta)=-X^{\\top}WX$ where $W$ is a diagonal matrix with\n$\\{\\mbox{Var}(Y_{i})\\}$ on the diagonal (the working weights). Note that\nboth $E(Y)$ and the variances in $W$ depend on $\\beta$, so these will\nchange as we iteratively update $\\beta$. Therefore, the N-R update is\n$$\\beta_{t+1}=\\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})$$\nwhere $E(Y)_{t}$ and $W_{t}$ are the values at the current parameter\nestimate, $\\beta_{t}$ . For example, for logistic regression (here with\n$n_{i}=1$), $W_{t,ii}=p_{ti}(1-p_{ti})$ and $E(Y)_{ti}=p_{ti}$ where\n$p_{ti}=\\frac{\\exp(X_{i}^{\\top}\\beta_{t})}{1+\\exp(X_{i}^{\\top}\\beta_{t})}$.\nIn the canonical parameterization of a GLM, the Hessian does not depend\non the data, so the observed and expected FI are the same, and therefore\nN-R and FS are the same.\n\nThe update above can be rewritten in the standard form of IRLS as a WLS\nproblem, $$\\begin{aligned}\n\\beta_{t+1} & = \\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})\\\\\n & = (X^{\\top}W_{_{t}}X)^{-1}(X^{\\top}W_{_{t}}X)\\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})\\\\\n & = (X^{\\top}W_{_{t}}X)^{-1}X^{\\top}W_{t}\\left[X\\beta_{t}+W_{t}^{-1}(Y-E(Y)_{t})\\right]\\\\\n & = (X^{\\top}W_{_{t}}X)^{-1}X^{\\top}W_{t}\\tilde{Y}_{t},\\end{aligned}$$\nwhere the so-called working observations are\n$\\tilde{Y}_{t}=X\\beta_{t}+W_{t}^{-1}(Y-E(Y)_{t})$. Note that these are\non the scale of the linear predictor. The interpretation is that the\nworking observations are equal to the current fitted values,\n$X\\beta_{t}$, plus weighted residuals where the weight (the inverse of\nthe variance) takes the actual residuals and scales to the scale of the\nlinear predictor.\n\nWhile IRLS is standard for GLMs, you can also use general purpose\noptimization routines.\n\nIRLS is a special case of the general Gauss-Newton method for nonlinear\nleast squares.\n\n## Descent methods and Newton-like methods\n\nMore generally a Newton-like method has updates of the form\n$$x_{t+1}=x_{t}-\\alpha_{t}M_{t}^{-1}f^{\\prime}(x_{t}).$$ We can choose\n$M_{t}$ in various ways, including as an approximation to the second\nderivative.\n\nThis opens up several possibilities:\n\n1.  using more computationally efficient approximations to the second\n    derivative,\n\n2.  avoiding steps that do not go in the correct direction (i.e., go\n    uphill when minimizing), and\n\n3.  scaling by $\\alpha_{t}$ so as not to step too far.\n\nLet's consider a variety of strategies.\n\n### Descent methods \n\nThe basic strategy is to choose a good direction and then choose the\nlongest step for which the function continues to decrease. Suppose we\nhave a direction, $p_{t}$. Then we need to move\n$x_{t+1}=x_{t}+\\alpha_{t}p_{t}$, where $\\alpha_{t}$ is a scalar,\nchoosing a good $\\alpha_{t}$. We might use a line search (e.g.,\nbisection or golden section search) to find the local minimum of\n$f(x_{t}+\\alpha_{t}p_{t})$ with respect to $\\alpha_{t}$. However, we\noften would not want to run to convergence, since we'll be taking\nadditional steps anyway.\n\nSteepest descent chooses the direction as the steepest direction\ndownhill, setting $M_{t}=I$, since the gradient gives the steepest\ndirection uphill (the negative sign in the equation below has us move\ndirectly downhill rather than directly uphill). Given the direction, we\nwant to scale the step $$x_{t+1}=x_{t}-\\alpha_{t}f^{\\prime}(x_{t})$$\nwhere the contraction, or step length, parameter $\\alpha_{t}$ is chosen\nsufficiently small to ensure that we descend, via some sort of line\nsearch. The critical downside to steepest descent is that when the\ncontours are elliptical, it tends to zigzag; here's an example. \n\nMy original code for this was in R, so I'm just leaving it that way\nrather than having to do a lot of fine-tuning to get the image to display\nthe way I want in Python.\n\n(Note that I do a full line search (using the golden section method via\n`optimize()`) at each step in the direction of steepest descent - this\nis generally computationally wasteful, but I just want to illustrate how\nsteepest descent can go wrong, even if you go the \"right\" amount in each\ndirection.)\n\n\n\n```\npar(mai = c(.5,.4,.1,.4))\nf <- function(x){\n\tx[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000\n}\nfp <- function(x){\n\tc(2 * x[1]/1000 + 4 * x[2]/1000,\n\t4 * x[1]/1000 + 10 * x[2]/1000)\n}\nlineSearch <- function(alpha, xCurrent, direction, FUN){\n\tnewx <- xCurrent + alpha * direction\n\tFUN(newx)\n}\nnIt <- 50\nxvals <- matrix(NA, nr = nIt, nc = 2)\nxvals[1, ] <- c(7, -4)\nfor(t in 2:50){\n\tnewalpha <- optimize(lineSearch, interval = c(-5000, 5000),\n\t\txCurrent = xvals[t-1, ], direction = fp(xvals[t-1, ]),\n\t\tFUN = f)$minimum \n\txvals[t, ] <- xvals[t-1, ] + newalpha * fp(xvals[t-1, ])\n}\nx1s <- seq(-5, 8, len = 100); x2s = seq(-5, 2, len = 100)\nfx <- apply(expand.grid(x1s, x2s), 1, f)\n## plot f(x) surface on log scale\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100), \n\txlim = c(-5, 8), ylim = c(-5,2)) \nlines(xvals) ## overlay optimization path\n```\n\n![Path of steepest descent](steep-descent.png)\n\nIf the contours are circular, steepest descent works well. Newton's\nmethod deforms elliptical contours based on the Hessian. Another way to\nthink about this is that steepest descent does not take account of the\nrate of change in the gradient, while Newton's method does.\n\nThe general descent algorithm is\n$$x_{t+1}=x_{t}-\\alpha_{t}M_{t}^{-1}f'(x_{t}),$$ where $M_{t}$ is\ngenerally chose to approximate the Hessian and $\\alpha_{t}$ allows us to\nadjust the step in a smart way. Basically, since the negative gradient\ntells us the direction that descends (at least within a small\nneighborhood), if we don't go too far, we should be fine and should work\nour way downhill. One can work this out formally using a Taylor\napproximation to $f(x_{t+1})-f(x_{t})$ and see that we make use of\n$M_{t}$ being positive definite. (Unfortunately backtracking with\npositive definite $M_{t}$ does not give a theoretical guarantee that the\nmethod will converge. We also need to make sure that the steps descend\nsufficiently quickly and that the algorithm does not step along a level\ncontour of $f$.)\n\nThe conjugate gradient algorithm for iteratively solving large systems\nof equations is all about choosing the direction and the step size in a\nsmart way given the optimization problem at hand.\n\n### Quasi-Newton methods such as BFGS\n\nOther replacements for the Hessian matrix include estimates that do not\nvary with $t$ and finite difference approximations. When calculating the\nHessian is expensive, it can be very helpful to substitute an\napproximation.\n\nA basic finite difference approximation requires us to compute finite\ndifferences in each dimension, but this could be computationally\nburdensome. A more efficient strategy for choosing $M_{t+1}$ is to (1)\nmake use of $M_{t}$ and (2) make use of the most recent step to learn\nabout the curvature of $f^{\\prime}(x)$ in the direction of travel. One\napproach is to use a rank one update to $M_{t}$.\n\nA basic strategy is to choose $M_{t+1}$ such that the secant condition\nis satisfied:\n$$M_{t+1}(x_{t+1}-x_{t})=\\nabla f(x_{t+1})-\\nabla f(x_{t}),$$ which is\nmotivated by the fact that the secant approximates the gradient in the\ndirection of travel. Basically this says to modify $M_{t}$ in such a way\nthat we incorporate what we've learned about the gradient from the most\nrecent step. $M_{t+1}$ is not fully determined based on this, and we\ngenerally impose other conditions, in particular that $M_{t+1}$ is\nsymmetric and positive definite. Defining $s_{t}=x_{t+1}-x_{t}$ and\n$y_{t}=\\nabla f(x_{t+1})-\\nabla f(x_{t})$, the unique, symmetric rank\none update (why is the following a rank one update?) that satisfies the\nsecant condition is\n$$M_{t+1}=M_{t}+\\frac{(y_{t}-M_{t}s_{t})(y_{t}-M_{t}s_{t})^{\\top}}{(y_{t}-M_{t}s_{t})^{\\top}s_{t}}.$$\nIf the denominator is positive, $M_{t+1}$ may not be positive definite,\nbut this is guaranteed for non-positive values of the denominator. One\ncan also show that one can achieve positive definiteness by shrinking\nthe denominator toward zero sufficiently.\n\nA standard approach to updating $M_{t}$ is a commonly-used rank two\nupdate that generally results in $M_{t+1}$ being positive definite is\n$$M_{t+1}=M_{t}-\\frac{M_{t}s_{t}(M_{t}s_{t})^{\\top}}{s_{t}^{\\top}M_{t}s_{t}}+\\frac{y_{t}y_{t}^{\\top}}{s_{t}^{\\top}y_{t}},$$\nwhich is known as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update.\nThis is one of the methods used in R in *optim()*.\n\nQuestion: how can we update $M_{t}^{-1}$ to $M_{t+1}^{-1}$ efficiently?\nIt turns out there is a way to update the Cholesky of $M_{t}$\nefficiently and this is a better approach than updating the inverse.\n\nThe order of convergence of quasi-Newton methods is generally slower\nthan the quadratic convergence of N-R because of the approximations but\nstill faster than linear. In general, quasi-Newton methods will do much\nbetter if the scales of the elements of $x$ are similar. Lange suggests\nusing a starting point for which one can compute the expected\ninformation, to provide a good starting value $M_{0}$.\n\nNote that for estimating a covariance based on the numerical information\nmatrix, we would not want to rely on $M_{t}$ from the final iteration,\nas the approximation may be poor. Rather we would spend the effort to\nbetter estimate the Hessian directly at $x^{*}$.\n\n### Stochastic gradient descent\n\nStochastic gradient descent (SGD) is the hot method in machine learning,\ncommonly used for fitting deep neural networks. It allows you to\noptimize an objective function with respect to what is often a very\nlarge number of parameters even when the data size is huge.\n\nGradient descent is a simplification of Newton's method that does not\nrely on the second derivative, but rather chooses the direction using\nthe gradient and then a step size, $\\alpha_{t}$:\n$$x_{t+1}=x_{t}-\\alpha_{t}f^{\\prime}(x_{t})$$\n\nThe basic idea of stochastic gradient descent is to replace the gradient\nwith a function whose expected value is the gradient,\n$E(g(x_{t}))=f^{\\prime}(x_{t})$: $$x_{t+1}=x_{t}-\\alpha_{t}g(x_{t})$$\nThus on average we should go in a good (downhill) direction. Given that\nwe know that strictly following the gradient can lead to slow\nconvergence, it makes some intuitive sense that we could still do ok\nwithout using the exact gradient. One can show formally that SGD will\nconverge for convex functions.\n\nSGD can be used in various contexts, but the common one we will focus on\nis when $$\\begin{aligned}\nf(x) & = \\sum_{i=1}^{n}f_{i}(x)\\\\\nf^{\\prime}(x) & = \\sum_{i=1}^{n}f^{\\prime}_{i}(x)\\end{aligned}$$ for\nlarge $n$. Thus calculation of the gradient is $O(n)$, and we may not\nwant to incur that computational cost. How could we implement SGD in\nsuch a case? At each iteration we could randomly choose an observation\nand compute the contribution to the gradient from that data point, or we\ncould choose a random subset of the data (this is *mini-batch SGD*), or\nthere are variations where we systematically cycle through the\nobservations or cycle through subsets. However, in some situations,\nconvergence is actually much faster when using randomness. And if the\ndata are ordered in some meaningful way we definitely do not want to\ncycle through the observations in that order, as this can result in a\nbiased estimate of the gradient and slow convergence. So one generally\nrandomly shuffles the data before starting SGD. Note that using subsets\nrather than individual observations is likely to be more effective as it\ncan allow us to use optimized matrix/vector computations.\n\nHow should one choose the step size, $\\alpha_{t}$ (also called the\nlearning rate)? One might think that as one gets close to the optimum,\nif one isn't careful, one might simply bounce around near the optimum in\na random way, without actually converging to the optimum. So intuition\nsuggests that $\\alpha_{t}$ should decrease with $t$. Some choices of\nstep size have included:\n\n-   $\\alpha_{t}=1/t$\n-   set a schedule, such that for $T$ iterations, $\\alpha_{t}=\\alpha$,\n    then for the next $T$, $\\alpha_{t}=\\alpha\\gamma$, then for the next\n    $T$, $\\alpha_{t}=\\alpha\\gamma^{2}$. A heuristic is for\n    $\\gamma\\in(0.8,0.9)$.\n-   run with $\\alpha_{t}=\\alpha$ for $T$ iterations, then with\n    $\\alpha_{t}=\\alpha/2$ for $2T$, then with $\\alpha_{t}=\\alpha/4$ for\n    $4T$ and so forth.\n\n## Coordinate descent (Gauss-Seidel)\n\nGauss-Seidel is also known a back-fitting or cyclic coordinate descent.\nThe basic idea is to work element by element rather than having to\nchoose a direction for each step. For example backfitting used to be\nused to fit generalized additive models of the form\n$E(Y)=f_{1}(z_{1})+f_{2}(z_{2})+\\ldots+f_{p}(z_{p})$.\n\nThe basic strategy is to consider the $j$th component of $f^{\\prime}(x)$\nas a univariate function of $x_{j}$ only and find the root, $x_{j,t+1}$\nthat gives $f^{\\prime}_{j}(x_{j,t+1})=0$. One cycles through each\nelement of $x$ to complete a single cycle and then iterates. The appeal\nis that univariate root-finding/minimization is easy, often more stable\nthan multivariate, and quick.\n\nHowever, Gauss-Seidel can zigzag, since you only take steps in one\ndimension at a time, as we see here. (Again the code is in R.)\n\n```\nf <- function(x){\n\treturn(x[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000)\n}\nf1 <- function(x1, x2){ # f(x) as a function of x1\n\treturn(x1^2/1000 + 4*x1*x2/1000 + 5*x2^2/1000)\n}\nf2 <- function(x2, x1){ # f(x) as a function of x2\n\treturn(x1^2/1000 + 4*x1*x2/1000 + 5*x2^2/1000)\n}\nx1s <- seq(-5, 8, len = 100); x2s = seq(-5, 2, len = 100)\nfx <- apply(expand.grid(x1s, x2s), 1, f)\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\nnIt <- 49\nxvals <- matrix(NA, nr = nIt, nc = 2)\nxvals[1, ] <- c(7, -4)\n## 5, -10\nfor(t in seq(2, nIt, by = 2)){\n    ## Note that full optimization along each axis is unnecessarily\n    ## expensive (since we are going to just take another step in the next\n    ## iteration. Just using for demonstration here.\n\tnewx1 <- optimize(f1, x2 = xvals[t-1, 2], interval = c(-40, 40))$minimum\n\txvals[t, ] <- c(newx1, xvals[t-1, 2])\n\tnewx2 <- optimize(f2, x1 = newx1, interval = c(-40, 40))$minimum\n\txvals[t+1, ] <- c(newx1, newx2)\n}\nlines(xvals)\n```\n\n![Coordinate descent](gauss-seidel.png)\n\nIn the notes for Unit 9 on linear algebra, I discussed the use of\nGauss-Seidel to iteratively solve $Ax=b$ in situations where factorizing\n$A$ (which of course is $O(n^{3})$) is too computationally expensive.\n\n#### The lasso\n\nThe *lasso* uses an L1 penalty in regression and related contexts. A\nstandard formulation for the lasso in regression is to minimize\n$$\\|Y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{j}|\\beta_{j}|$$ to find\n$\\hat{\\beta}(\\lambda)$ for a given value of the penalty parameter,\n$\\lambda$. A standard strategy to solve this problem is to use\ncoordinate descent, either cyclically, or by using directional\nderivatives to choose the coordinate likely to decrease the objective\nfunction the most (a greedy strategy). We need to use directional\nderivatives because the penalty function is not differentiable, but does\nhave directional derivatives in each direction. The directional\nderivative of the objective function for $\\beta_{j}$ is\n$$-2\\sum_{i}x_{ij}(Y_{i}-X_{i}^{\\top}\\beta)\\pm\\lambda$$ where we add\n$\\lambda$ if $\\beta_{j}\\geq0$ and you subtract $\\lambda$ if\n$\\beta_{j}<0$. If $\\beta_{j,t}$ is 0, then a step in either direction\ncontributes $+\\lambda$ to the derivative as the contribution of the\npenalty.\n\nOnce we have chosen a coordinate, we set the directional derivative to\nzero and solve for $\\beta_{j}$ to obtain $\\beta_{j,t+1}$.\n\nThe `glmnet` package in R (described in [this Journal of Statistical\nSoftware paper](http://www.jstatsoft.org/article/view/v033i01))\nimplements such optimization for a variety of penalties in linear model\nand GLM settings, including the lasso. This [Mittal et al.\npaper](http://biostatistics.oxfordjournals.org/content/early/2013/10/04/biostatistics.kxt043.short)\ndescribes similar optimization for survival analysis with very large\n$p$, exploiting sparsity in the $X$ matrix for computational efficiency;\nnote that they do not use Newton-Raphson because the matrix operations\nare infeasible computationally.\n\nOne nice idea that is used in lasso and related settings is the idea of\nfinding the regression coefficients for a variety of values of\n$\\lambda$, combined with \"warm starts\". A general approach is to start\nwith a large value of $\\lambda$ for which all the coefficients are zero\nand then decrease $\\lambda$. At each new value of $\\lambda$, use the\nestimated coefficients from the previous value as the starting values.\nThis should allow for fast convergence and gives what is called the\n\"solution path\". Often $\\lambda$ is chosen based on cross-validation.\n\nThe LARS (least angle regression) algorithm uses a similar strategy that\nallows one to compute $\\hat{\\beta}_{\\lambda}$ for all values of\n$\\lambda$ at once.\n\nThe lasso can also be formulated as the constrained minimization of\n$\\|Y-X\\beta\\|_{2}^{2}$ s.t. $\\sum_{j}|\\beta_{j}|\\leq c$, with $c$ now\nplaying the role of the penalty parameter. Solving this minimization\nproblem would take us in the direction of quadratic programming, a\nspecial case of convex programming, discussed in Section 9.\n\n## Nelder-Mead \n\nThis approach avoids using derivatives or approximations to derivatives.\nThis makes it robust, but also slower than Newton-like methods. The\nbasic strategy is to use a simplex, a polytope of $p+1$ points in $p$\ndimensions (e.g., a triangle when searching in two dimensions,\ntetrahedron in three dimensions\\...) to explore the space, choosing to\nshift, expand, or contract the polytope based on the evaluation of $f$\nat the points.\n\nThe algorithm relies on four tuning factors: a reflection factor,\n$\\alpha>0$; an expansion factor, $\\gamma>1$; a contraction factor,\n$0<\\beta<1$; and a shrinkage factor, $0<\\delta<1$. First one chooses an\ninitial simplex: $p+1$ points that serve as the vertices of a convex\nhull.\n\n1.  Evaluate and order the points, $x_{1},\\ldots,x_{p+1}$ based on\n    $f(x_{1})\\leq\\ldots\\leq f(x_{p+1})$. Let $\\bar{x}$ be the average of\n    the first $p$ $x$'s.\n\n2.  (Reflection) Reflect $x_{p+1}$ across the hyperplane (a line when\n    $p+1=3$) formed by the other points to get $x_{r}$, based on\n    $\\alpha$.\n\n    -   $x_{r}=(1+\\alpha)\\bar{x}-\\alpha x_{p+1}$\n\n3.  If $f(x_{r})$ is between the best and worst of the other points, the\n    iteration is done, with $x_{r}$ replacing $x_{p+1}$. We've found a\n    good direction to move.\n\n4.  (Expansion) If $f(x_{r})$ is better than all of the other points,\n    expand by extending $x_{r}$ to $x_{e}$ based on $\\gamma$, because\n    this indicates the optimum may be further in the direction of\n    reflection. If $f(x_{e})$ is better than $f(x_{r})$, use $x_{e}$ in\n    place of $x_{p+1}$. If not, use $x_{r}$. The iteration is done.\n\n    -   $x_{e}=\\gamma x_{r}+(1-\\gamma)\\bar{x}$\n\n5.  If $f(x_{r})$ is worse than all the other points, but better than\n    $f(x_{p+1})$, let $x_{h}=x_{r}$. Otherwise $f(x_{r})$ is worse than\n    $f(x_{p+1})$ so let $x_{h}=x_{p+1}$. In either case, we want to\n    concentrate our polytope toward the other points.\n\n    a.  (Contraction) Contract $x_{h}$ toward the hyperplane formed by\n        the other points, based on $\\beta$, to get $x_{c}$. If the\n        result improves upon $f(x_{h})$ replace $x_{p+1}$ with $x_{c}$.\n        Basically, we haven't found a new point that is better than the\n        other points, so we want to contract the simplex away from the\n        bad point.\n\n        -   $x_{c}=\\beta x_{h}+(1-\\beta)\\bar{x}$\n\n    b.  (Shrinkage) Otherwise (if $x_{c}$ is not better than $x_{h}$),\n        replace $x_{p+1}$ with $x_{h}$ and shrink the simplex toward\n        $x_{1}$. Basically this suggests our step sizes are too large\n        and we should shrink the simplex, shrinking towards the best\n        point.\n\n        -   $x_{i}=\\delta x_{i}+(1-\\delta)x_{1}$ for $i=2,\\ldots,p+1$\n\nConvergence is assessed based on the sample variance of the function\nvalues at the points, the total of the norms of the differences between\nthe points in the new and old simplexes, or the size of the simplex. In\nclass we'll work through some demo code (not shown here) that\nillustrates the individual steps in an iteration of Nelder-Mead.\n\n\nWe can see the points at which the function was evaluated in the same\nquadratic example we saw in previous sections. The left hand panel shows\nthe steps from a starting point somewhat far from the optimum, with the\nfirst 9 points numbered. In this case, we start with points 1, 2, and 3.\nPoint 4 is a reflection. At this point, it looks like point 5 is a\ncontraction but that doesn't exactly follow the algorithm above (since\nPoint 4 is between Points 2 and 3 so the iteration should end without a\ncontraction), so perhaps the algorithm as implemented is a bit different\nthan as described above. In any event, the new set is (2, 3, 4). Then\npoint 6 and point 7 are reflection and expansion steps and the new set\nis (3, 4, 6). Points 8 and 9 are again reflection and expansion steps.\nThe right hand panel shows the steps from a starting point near\n(actually at) the optimum. Points 4 and 5 are reflection and expansion\nsteps, with the next set being (1, 2, 5). Now step 6 is a reflection but\nit is the worst of all the points, so point 7 is a contraction of point\n2 giving the next set (1, 5, 7). Point 8 is then a reflection and point\n9 is a contraction of point 5.\n\n(Again some code in R.)\n\n```\nf <- function(x, plot = TRUE, verbose = FALSE) {\n    result <- x[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000\n    if(verbose) print(result)\n    if(plot && cnt < 10) {\n        points(x[1], x[2], pch = as.character(cnt))\n        if(cnt < 10) cnt <<- cnt + 1 else cnt <<- 1\n        if(interactive())\n            invisible(readline(prompt = \"Press <Enter> to continue...\"))\n    } else if(plot) points(x[1], x[2])\n    return(result)\n}\n\npar(mfrow = c(1,2), mgp = c(1.8,.7,0), mai = c(.5,.45,.1,.5), cex = 0.7)\n\nx1s <- seq(-5, 10, len = 100); x2s = seq(-5, 2, len = 100)\nfx <- apply(expand.grid(x1s, x2s), 1, f, FALSE)\ncnt <- 1\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\ninit <- c(7, -4)\noptim(init, f, method = \"Nelder-Mead\", verbose = FALSE)\n\npar(cex = 0.7)\nx1s <- seq(-.2, .2, len = 100); x2s = seq(-.12, .12, len = 100)\nfx <- apply(expand.grid(x1s, x2s), 1, f, FALSE)\ncnt <- 1\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\ninit <- c(-0, 0)\noptim(init, f, method = \"Nelder-Mead\", verbose = FALSE)\n```\n\n![Nelder-Mead](nelder-mead.png)\n\nHere's an [online graphical\nillustration](http://www.benfrederickson.com/numerical-optimization/) of\nNelder-Mead.\n\nThis is the default in `optim()` in R. It is an option\n(by specifying `method='Nelder-mead'`) for `scipy.optimize.minimize`\n(BFGS or a variant is the default).\n\n## Simulated annealing (SA) (optional)\n\nSimulated annealing is a *stochastic* descent algorithm, unlike the\ndeterministic algorithms we've already discussed. It has a couple\ncritical features that set it aside from other approaches. First, uphill\nmoves are allowed; second, whether a move is accepted is stochastic, and\nfinally, as the iterations proceed the algorithm becomes less likely to\naccept uphill moves.\n\nAssume we are minimizing a negative log likelihood as a function of\n$\\theta$, $f(\\theta)$.\n\nThe basic idea of simulated annealing is that one modifies the objective\nfunction, $f$ in this case, to make it less peaked at the beginning,\nusing a \"temperature\" variable that changes over time. This helps to\nallow moves away from local minima, when combined with the ability to\nmove uphill. The name comes from an analogy to heating up a solid to its\nmelting temperature and cooling it slowly - as it cools the atoms go\nthrough rearrangements and slowly freeze into the crystal configuration\nthat is at the lowest energy level.\n\nHere's the algorithm. We divide up iterations into stages,\n$j=1,2,\\ldots$ in which the temperature variable, $\\tau_{j}$, is\nconstant. Like MCMC, we require a proposal distribution to propose new\nvalues of $\\theta$.\n\n1.  Propose to move from $\\theta_{t}$ to $\\tilde{\\theta}$ from a\n    proposal density, $g_{t}(\\cdot|\\theta_{t})$, such as a normal\n    distribution centered at $\\theta_{t}$.\n\n2.  Accept $\\tilde{\\theta}$ as $\\theta_{t+1}$ according to the\n    probability\n    $\\min(1,\\exp((f(\\theta_{t})-f(\\tilde{\\theta}))/\\tau_{j})$ - i.e.,\n    accept if a uniform random deviate is less than that probability.\n    Otherwise set $\\theta_{t+1}=\\theta_{t}$. Notice that for larger\n    values of $\\tau_{j}$ the differences between the function values at\n    the two locations are reduced (just like a large standard deviation\n    spreads out a distribution). So the exponentiation smooths out the\n    objective function when $\\tau_{j}$ is large.\n\n3.  Repeat steps 1 and 2 $m_{j}$ times.\n\n4.  Increment the temperature and cooling schedule:\n    $\\tau_{j}=\\alpha(\\tau_{j-1})$ and $m_{j}=\\beta(m_{j-1})$. Back to\n    step 1.\n\nThe temperature should slowly decrease to 0 while the number of\niterations, $m_{j}$, should be large. Choosing these 'schedules' is at\nthe core of implementing SA. Note that we always accept downhill moves\nin step 2 but we sometimes accept uphill moves as well.\n\nFor each temperature, SA produces an MCMC based on the Metropolis\nalgorithm. So if $m_{j}$ is long enough, we should sample from the\nstationary distribution of the Markov chain,\n$\\exp(-f(\\theta)/\\tau_{j}))$. Provided we can move between local minima,\nthe chain should gravitate toward the global minima because these are\nincreasingly deep (low values) relative to the local minima as the\ntemperature drops. Then as the temperature cools, $\\theta_{t}$ should\nget trapped in an increasingly deep well centered on the global minimum.\nThere is a danger that we will get trapped in a local minimum and not be\nable to get out as the temperature drops, so the temperature schedule is\nquite important in trying to avoid this.\n\nA wide variety of schedules have been tried. One approach is to set\n$m_{j}=1\\forall j$ and\n$\\alpha(\\tau_{j-1})=\\frac{\\tau_{j-1}}{1+a\\tau_{j-1}}$ for a small $a$.\nFor a given problem it can take a lot of experimentation to choose\n$\\tau_{0}$ and $m_{0}$ and the values for the scheduling functions. For\nthe initial temperature, it's a good idea to choose it large enough that\n$\\exp((f(\\theta_{i})-f(\\theta_{j}))/\\tau_{0})\\approx1$ for any pair\n$\\{\\theta_{i},\\theta_{j}\\}$ in the domain, so that the algorithm can\nvisit the entire space initially.\n\nSimulated annealing can converge slowly. Multiple random starting points\nor stratified starting points can be helpful for finding a global\nminimum. However, given the slow convergence, these can also be\ncomputationally burdensome.\n\n# 6. Basic optimization in Python\n\n## Core optimization functions\n\nScipy provides various useful optimization functions via `scipy.optimize`, including many of the algorithms discussed in this unit.\n\n- `minimize_scalar` implements golden section search (`golden`) and interpolation combined with golden section search (`brent`, akin to `optimize` in R).\n- `minimize` implements various methods for multivariate optimization including Nelder-Mead, BFGS and conjugate gradients (discussed a bit in Unit 9). You can choose which\n    method you prefer and can try multiple methods. You can supply a\n    gradient function for use with the Newton-related\n    methods but it can also calculate numerical derivatives on the fly.\n    You can have the BFGS implementation in `minimize` return the Hessian at the optimum (based on a\n    numerical estimate), which then allows straighforward calculation of\n    asymptotic variances based on the information matrix.\n- One can provide a variety of nonlinear, linear, and simple bounds constraints as well, though certain types of constraints can only be used with certain algorithms.\n\nIn the demo code (not shown here; see the source qmd file), we'll work our way through a real\nexample of optimizing a likelihood for some climate data on extreme\nprecipitation.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_file = os.path.join('..', 'data', 'precipData.txt')\ny_hundredths = np.genfromtxt(data_file, missing_values = 'NA')  # precip in hundredths of inches\ny_hundredths = y_hundredths[~np.isnan(y_hundredths)]\ny = y_hundredths / 100  # precip now in inches\n\nnpy=31+28+31 # number of days in winter season\ncutoff = 1 / 25.4  # Convert 1 mm to inches\nthresh = np.percentile(y[y > cutoff], 98,)\n\n# Create a histogram of the data\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(y, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Precipitation (inches)')\nplt.ylabel('Frequency')\nplt.title('Histogram of All Data')\n\nplt.subplot(1, 2, 2)\nplt.hist(y[y > thresh], bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Precipitation (inches)')\nplt.ylabel('Frequency')\nplt.title(f'Histogram of Wet Days (Precip > {np.round(thresh,2)} inches)')\nplt.tight_layout()\nplt.show()\n\n# Define objective (negative log-likelihood) function\n\ndef pp_negloglik(par, y, thresh, npy):\n    mu, sc, sh = par\n    uInd = y > thresh\n    \n    # Invalid parameter values or data/parameter combos:\n    if sc <= 0:\n        return 1e6\n    if (1 + ((sh * (thresh - mu)) / sc) < 0):\n        return 1e6\n        \n    y = (y - mu) / sc\n    y = 1 + sh * y\n    \n    if np.min(y[uInd]) <= 0:\n        return 1e6\n    else:\n        ytmp = y.copy()\n        ytmp[~uInd] = 1 # 'zeroes' out those below the threshold after applying the log in next line\n        \n        l = np.sum(uInd * np.log(sc)) + np.sum(uInd * np.log(ytmp) * (1 / sh + 1)) + \\\n                   (len(y) / npy) * np.mean((1 + (sh * (thresh - mu)) / sc) ** (-1 / sh))\n                   \n    return l\n\n\n# Initial parameter values\ny_exc = y[y > thresh]\nin2 = np.sqrt(6 * np.var(y_exc)) / np.pi\nin1 = np.mean(y_exc) - 0.57722 * in2\ninit0 = [in1, in2, 0.1]\n\n# Optimization using Nelder-Mead\nstart_time = time.time()\nfit1 = minimize(pp_negloglik, init0, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nend_time = time.time()\nprint(\"Nelder-Mead Optimization:\")\nprint(fit1)\nprint(\"Execution Time:\", end_time - start_time, \"seconds\")\n\n# Optimization using BFGS\nstart_time = time.time()\nfit2 = minimize(pp_negloglik, init0, args=(y, thresh, npy), method='BFGS', options={'disp': True})\nend_time = time.time()\nprint(\"\\nBFGS Optimization:\")\nprint(fit2)\nprint(\"Execution Time:\", end_time - start_time, \"seconds\")\n\nmle = fit2.x\nvar = fit2.hess_inv\nse = np.sqrt(np.diag(fit2.hess_inv))\n\n# Different starting values\ninit1 = [np.mean(y[y > thresh]), np.std(y[y > thresh]), -0.1]\nfit1a = minimize(pp_negloglik, init1, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nfit2a = minimize(pp_negloglik, init1, args=(y, thresh, npy), method='BFGS', options={'disp': True})\n\n# Bad starting value for BFGS\ninit2 = [thresh, 0.01, .5]\nfit1b = minimize(pp_negloglik, init2, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nfit2b = minimize(pp_negloglik, init2, args=(y, thresh, npy), method='BFGS', options={'disp': True})\nfit2b.hess_inv\n\n# Data on a different scale\ny_exc2 = y[y > thresh] * 1000\ny2 = y * 1000\nthresh2 = thresh * 1000\n\ninit3 = [np.mean(y_exc2), np.std(y_exc2), 0.1]\nfit3 = minimize(pp_negloglik, init3, args=(y2, thresh2, npy), method='Nelder-Mead', options={'disp': True})\nfit4 = minimize(pp_negloglik, init3, args=(y2, thresh2, npy), method='BFGS', options={'disp': True})\nfit4.hess_inv\n\n# Plot the objective function\nn_grid = 30\nloc_vals = np.linspace(0, 5, n_grid)\nscale_vals = np.linspace(0.1, 3, n_grid)\nshape_vals = np.linspace(-0.3, 0.3, 16)\n\nloc_grid, scale_grid, shape_grid = np.meshgrid(loc_vals, scale_vals, shape_vals, indexing='ij')\npar_grid = np.column_stack((loc_grid.ravel(), scale_grid.ravel(), shape_grid.ravel()))\nobj = np.apply_along_axis(pp_negloglik, 1, par_grid, y=y, thresh=thresh, npy=npy)\n\nfig, axes = plt.subplots(4, 4, figsize=(14, 10))\nfor i, shape_value in enumerate(shape_vals):\n    ax = axes[i // 4, i % 4]\n    obj_matrix = np.array(obj[par_grid[:,2] == shape_value]).reshape(n_grid, n_grid).T\n    im = ax.imshow(obj_matrix, extent=(0, 5, 0.1, 3), cmap='viridis', vmin=40, vmax=80, aspect='auto', origin='lower')\n    ax.set_title(f\"shape = {shape_value:.2f}\")\n\nfig.colorbar(im, ax=axes, label=\"Objective Function Value\")\nplt.tight_layout()\n\nplt.show()\n```\n:::\n\n\n## Various considerations in using the Python functions\n\nAs we've seen, initial values are important both for avoiding divergence\n(e.g., in N-R), for increasing speed of convergence, and for helping to\navoid local optima. So it is well worth the time to try to figure out a\ngood starting value or multiple starting values for a given problem.\n\nScaling can be important. One useful step is to make sure the problem is\nwell-scaled, namely that a unit step in any parameter has a comparable\nchange in the objective function, preferably approximately a unit change\nat the optimum. Basically if\n$x_{j}$ is varying at $p$ orders of magnitude smaller than the other\n$x$s, we want to reparameterize to $x_{j}^{*}=x_{j}\\cdot10^{p}$ and then\nconvert back to the original scale after finding the answer. Or we may\nwant to work on the log scale for some variables, reparameterizing as\n$x_{j}^{*}=\\log(x_{j})$.\n\nAs far as I can see one needs to do this manually with `minimize` in Python\nbut `optim()` in R allows you to supply scaling information\nthrough the `parscale` component of the `control` argument. \n\nIf the function itself gives very large or small values near the\nsolution, you may want to rescale the entire function to avoid\ncalculations with very large or small numbers. This can avoid problems\nsuch as having apparent convergence because a gradient is near zero,\nsimply because the scale of the function is small. In `optim()` in R, this can\nbe controlled with the `fnscale` component of `control`.\n\n**Always** consider your answer and make sure it makes sense, in particular\nthat you haven't 'converged' to an extreme value on the boundary of the\nspace.\n\nVenables and Ripley suggest that it is often worth supplying analytic\nfirst derivatives rather than having a routine calculate numerical\nderivatives but not worth supplying analytic second derivatives.\nOne possibility is using software such as Mathematica to do symbolic (i.e., analytic) differentiation and\nthen writing code to implement the math of the result.\nAnother is using software that can give derivatives using automatic differentiation\nsuch as `PyTorch`, `jax` and `tensorflow`. We saw an example of using Jax\nearlier in the unit.\n\nIn general for software development it's obviously worth putting more\ntime into figuring out the best optimization approach and supplying\nderivatives. For a one-off analysis, you can try a few different\napproaches and assess sensitivity.\n\nThe nice thing about likelihood optimization is that the asymptotic\ntheory tells us that with large samples, the likelihood is approximately\nquadratic (i.e., the asymptotic normality of MLEs), which makes for a\nnice surface over which to do optimization. When optimizing with respect\nto variance components and other parameters that are non-negative, one\napproach to dealing with the constraints is to optimize with respect to\nthe log of the parameter.\n\n# 7. Combinatorial optimization over discrete spaces \n\nMany statistical optimization problems involve continuous domains, but\nsometimes there are problems in which the domain is discrete. Variable\nselection is an example of this.\n\n*Simulated annealing* can be used for optimizing in a discrete space.\nAnother approach uses *genetic algorithms*, in which one sets up the\ndimensions as loci grouped on a chromosome and has mutation and\ncrossover steps in which two potential solutions reproduce. An example\nwould be in high-dimensional variable selection.\n\n*Stochastic search variable selection* is a popular Bayesian technique\nfor variable selection that involves MCMC.\n\n# 8. Convexity\n\nMany optimization problems involve (or can be transformed into) convex\nfunctions. Convex optimization (also called convex programming) is a big\ntopic and one that we'll only brush the surface of in Sections 8 and 9.\nThe goal here is to give you enough of a sense of the topic that you\nknow when you're working on a problem that might involve convex\noptimization, in which case you'll need to go learn more.\n\nOptimization for convex functions is simpler than for ordinary functions\nbecause we don't have to worry about local optima - any stationary point\n(point where the gradient is zero) is a global minimum. A set $S$ in\n$\\Re^{p}$ is convex if any line segment between two points in $S$ lies\nentirely within $S$. More generally, $S$ is convex if any convex\ncombination is itself in $S$, i.e., $\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\in S$\nfor non-negative weights, $\\alpha_{i}$, that sum to 1. Convex functions\nare defined on convex sets - $f$ is convex if for points in a convex\nset, $x_{i}\\in S$, we have\n$f(\\sum_{i=1}^{m}\\alpha_{i}x_{i})\\leq\\sum_{i=1}^{m}\\alpha_{i}f(x_{i})$.\nStrict convexity is when the inequality is strict (no equality).\n\nThe first-order convexity condition relates a convex function to its\nfirst derivative: $f$ is convex if and only if\n$f(x)\\geq f(y)+\\nabla f(y)^{\\top}(x-y)$ for $y$ and $x$ in the domain of\n$f$. We can interpret this as saying that the first order Taylor\napproximation to $f$ is tangent to and below (or touching) the function\nat all points.\n\nThe second-order convexity condition is that a function is convex if\n(provided its first derivative exists), the derivative is\nnon-decreasing, in which case we have\n$f^{\\prime\\prime}(x)\\geq0\\,\\,\\forall x$ (for univariate functions). If\nwe have $f^{\\prime\\prime}(x)\\leq0\\,\\,\\forall x$ (a concave, or convex\ndown function) we can always consider $-f(x)$, which is convex.\nConvexity in multiple dimensions means that the gradient is\nnondecreasing in all dimensions. If $f$ is twice differentiable, then if\nthe Hessian is positive semi-definite, $f$ is convex.\n\nThere are a variety of results that allow us to recognize and construct\nconvex functions based on knowing what operations create and preserve\nconvexity. The Boyd book is a good source for material on such\noperations. Note that norms are convex functions (based on the triangle\ninequality),\n$\\|\\sum_{i=1}^{n}\\alpha_{i}x_{i}\\|\\leq\\sum_{i=1}^{n}\\alpha_{i}\\|x_{i}\\|$.\n\nWe'll talk about a general algorithm that works for convex functions\n(the MM algorithm) and about the EM algorithm that is well-known in\nstatistics, and is a special case of MM.\n\n## MM algorithm \n\nThe MM algorithm is really more of a principle for constructing problem\nspecific algorithms. MM stands for majorize-minorize. We'll use the\nmajorize part of it to minimize functions - the minorize part is the\ncounterpart for maximizing functions.\n\nSuppose we want to minimize a convex function, $f(x)$. The idea is to\nconstruct a majorizing function, at $x_{t}$, which we'll call $g$. $g$\nmajorizes $f$ at $x_{t}$ if $f(x_{t})=g(x_{t})$ and\n$f(x)\\leq g(x)\\forall x$.\n\nThe iterative algorithm is as follows. Given $x_{t}$, construct a\nmajorizing function $g_{t}(x).$ Then minimize $g_{t}$ w.r.t. $x$ (or at\nleast move downhill, such as with a modified Newton step) to find\n$x_{t+1}$. Then we iterate, finding the next majorizing function,\n$g_{t+1}(x)$. The algorithm is obviously guaranteed to go downhill, and\nideally we use a function $g$ that is easy to work with (i.e., to\nminimize or go downhill with respect to). Note that we haven't done any\nmatrix inversions or computed any derivatives of $f$. Furthermore, the\nalgorithm is numerically stable - it does not over- or undershoot the\noptimum. The downside is that convergence can be quite slow.\n\nThe tricky part is finding a good majorizing function. Basically one\nneeds to gain some skill in working with inequalities. The Lange book\nhas some discussion of this.\n\nAn example is for estimating regression coefficients for median\nregression (aka least absolute deviation regression), which minimizes\n$f(\\theta)=\\sum_{i=1}^{n}|y_{i}-z_{i}^{\\top}\\theta|=\\sum_{i=1}^{n}|r_{i}(\\theta)|$.\nNote that $f(\\theta)$ is convex because affine functions (in this case\n$y_{i}-z_{i}^{\\top}\\theta$) are convex, convex functions of affine\nfunctions are convex, and the summation preserves the convexity. We want\nto minimize $$\\begin{aligned}\nf(\\theta) & = \\sum_{i=1}^{n}|r_{i}(\\theta)|\\\\\n & = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta)^{2}}\\end{aligned}$$\n\nNext, $h(x)=\\sqrt{x}$ is concave, so we can use the following\n(commonly-used) inequality, $h(x)\\leq h(y)+h^{\\prime}(y)(x-y)$ which\nholds for any concave function, $h$, and note that we have equality when\n$y=x$. For $y=\\theta_{t}$, the current value in the iterative\noptimization, we have: $$\\begin{aligned}\nf(\\theta) & = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta)^{2}}\\\\\n & \\leq \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{r_{i}(\\theta)^{2}-r_{i}(\\theta_{t})^{2}}{2\\sqrt{r_{i}(\\theta_{t})^{2}}}\\\\\n & = g_{t}(\\theta)\\end{aligned}$$ where the term on the right of the\nsecond equation is our majorizing function $g(\\theta)$ for the current\n$\\theta_{t}$. We then have $$\\begin{aligned}\ng_{t}(\\theta) & = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{1}{2}\\sum_{i=1}^{n}\\frac{r_{i}(\\theta)^{2}-r_{i}(\\theta_{t})^{2}}{2\\sqrt{r_{i}(\\theta_{t})^{2}}}\\\\\n & = \\frac{1}{2}\\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{1}{2}\\sum_{i=1}^{n}\\frac{r_{i}(\\theta)^{2}}{\\sqrt{r_{i}(\\theta_{t})^{2}}}\\end{aligned}$$\nOur job in this iteration of the algorithm is to minimize $g$ with\nrespect to $\\theta$ (recall that $\\theta_{t}$ is a fixed value), so we\ncan ignore the first sum, which doesn't involve $\\theta$. Minimizing the\nsecond sum can be seen as a weighted least squares problem, where the\nnumerator is the usual sum of squared residuals and the weights are\n$w_{i}=\\frac{1}{\\sqrt{(y_{i}-z_{i}^{\\top}\\theta_{t})^{2}}}$. Intuitively\nthis makes sense: the weight is large when the magnitude of the residual\nis small this makes up for the fact that we are using least squares when\nwe want to mimimize absolute deviations. So our update is:\n$$\\theta_{t+1}=(Z^{\\top}W(\\theta_{t})Z)^{-1}Z^{\\top}W(\\theta_{t})Y,$$\nwhere $W(\\theta_{t})$ is a diagonal matrix with elements\n$w_{1},\\ldots,w_{n}.$\n\nAs usual, we want to think about what could go wrong numerically. If we\nhave some very small magnitude residuals, they will get heavily\nupweighted in this procedure, which might cause instability in our\noptimization.\n\nFor an example of MM being used in practice for a real problem, see Jung\net al. (2014): Biomarker Detection in Association Studies: Modeling SNPs\nSimultaneously via Logistic ANOVA, Journal of the American Statistical\nAssociation 109:1355.\n\n## Expectation-Maximization (EM)\n\nIt turns out the EM algorithm that many of you have heard about is a\nspecial case of MM. For our purpose here, we'll consider maximization.\n\nThe EM algorithm is most readily motivated from a missing data\nperspective. Suppose you want to maximize $L(\\theta|x)=f(x;\\theta)$\nbased on available data in a missing data context. Denote the complete\ndata as $Y=(X,Z)$ with $Z$ is missing. As we'll see, in many cases, $Z$\nis actually a set of latent variables that we introduce into the problem\nto formulate it so we can use EM. The canonical example is when $Z$ are\nmembership indicators in a mixture modeling context. (Note that in the\ncase where you introduce $Z$, that also means that one could also just\ndirectly maximize $L(\\theta|x)$, which in many cases may work better\nthan using the EM algorithm.)\n\nIn general, $\\log L(\\theta;x)$ may be hard to optimize because it\ninvolves an integral over the missing data, $Z$:\n$$f(x;\\theta)=\\int f(x,z;\\theta)dz,$$ but the EM algorithm provides a\nrecipe that makes the optimization straightforward for many problems.\n\nThe algorithm is as follows. Let $\\theta^{t}$ be the current value of\n$\\theta$. Then define\n$$Q(\\theta|\\theta^{t})=E(\\log L(\\theta|Y)|x;\\theta^{t})$$.\n\nThat expectation is an expectation with respect to the conditional distribution,\n$ f(z|x; \\theta = \\theta^t)$.\n\n The algorithm is\n\n1.  E step: Compute $Q(\\theta|\\theta^{t})$, ideally calculating the\n    expectation over the missing data in closed form. Note that\n    $\\log L(\\theta|Y)$ is a function of $\\theta$ so\n    $Q(\\theta|\\theta^{t})$ will involve both $\\theta$ and $\\theta^{t}$.\n\n2.  M step: Maximize $Q(\\theta|\\theta^{t})$ with respect to $\\theta$,\n    finding $\\theta^{t+1}$.\n\n3.  Continue until convergence.\n\nIdeally both the E and M steps can be done analytically. When the M step\ncannot be done analytically, one can employ some of the numerical\noptimization tools we've already seen. When the E step cannot be done\nanalytically, one standard approach is to estimate the expectation by\nMonte Carlo, which produces Monte Carlo EM (MCEM). The strategy is to\ndraw from $z_{j}$ from $f(z|x,\\theta^{t})$ and approximate $Q$ as a\nMonte Carlo average of $\\log f(x,z_{j};\\theta)$, and then optimize over\nthis approximation to the expectation. If one can't draw in closed form\nfrom the conditional density, one strategy is to do a short MCMC to draw\na (correlated) sample.\n\nEM can be show to increase the value of the function at each step using\nJensen's inequality (equivalent to the information inequality that holds\nwith regard to the Kullback-Leibler divergence between two\ndistributions) (Givens and Hoeting, p. 95, go through the details).\nFurthermore, one can show that it amounts, at each step, to maximizing a\nminorizing function for $\\log L(\\theta)$ - the minorizing function\n(effectively $Q$) is tangent to $\\log L(\\theta)$ at $\\theta^{t}$ and\nlies below $\\log L(\\theta)$.\n\nA standard example is a mixture model. (Here we'll assume a mixture of\nnormal distributions, but other distributions could be used.) Therefore\nwe have $$f(x;\\theta)=\\sum_{k=1}^{K}\\pi_{k}f_{k}(x;\\mu_{k},\\sigma_{k})$$\nwhere we have $K$ mixture components and $\\pi_{k}$ are the (marginal)\nprobabilities of being in each component. The complete parameter vector\nis $\\theta=\\{\\{\\pi_{k}\\},\\{\\mu_{k}\\},\\{\\sigma_{k}\\}\\}$. Note that the\nlikelihood is a complicated product (over observations) over the sum\n(over components), so maximization may be difficult. Furthermore, such\nlikelihoods are well-known to be multimodal because of label switching.\n\nTo use EM, we take the group membership indicators for each observation\nas the missing data. For the $i$th observation, we have\n$z_{i}\\in\\{1,2,\\ldots,K\\}$. Introducing these indicators \"breaks the\nmixture\". If we know the memberships for all the observations, it's\noften easy to estimate the parameters for each group based on the\nobservations from that group. For example if the $\\{f_{k}\\}$'s were\nnormal densities, then we can estimate the mean and variance of each\nnormal density using the sample mean and sample variance of the\n$x_{i}$'s that belong to each mixture component. EM will give us a\nvariation on this that uses \"soft\" (i.e., probabilistic) weighting.\n\nThe complete log likelihood given $z$ and $x$ is\n$$\\log\\prod_{i}f(x_{i}|z_{i};\\theta)\\mbox{Pr}(Z_{i}=z_{i};\\theta)$$\nwhich can be expressed as\\\n\\\n$$\\begin{aligned}\n\\log L(\\theta|x,z) & = & \\sum_{i}\\log f(x_{i};\\mu_{z_{i}},\\sigma_{z_{i}})+\\log\\pi_{z_{i}}\\\\\n & = & \\sum_{i}\\sum_{k}I(z_{i}=k)(\\log f_{k}(x_{i};\\mu_{k},\\sigma_{k})+\\log\\pi_{k})\\end{aligned}$$\nwith $Q$ equal to\n$$Q(\\theta|\\theta^{t})=\\sum_{i}\\sum_{k}E(I(z_{i}=k)|x_{i};\\theta^{t})(\\log f_{k}(x_{i};\\mu_{k},\\sigma_{k})+\\log\\pi_{k})$$\nwhere $E(I(z_{i}=k)|x_{i};\\theta^{t})$ is equal to the probability that\nthe $i$th observation is in the $k$th group given $x_{i}$ and\n$\\theta_{t}$, which is calculated from Bayes theorem as\n$$p_{ik}^{t}=\\frac{\\pi_{k}^{t}f_{k}(x_{i};\\mu_{k}^{t},\\sigma_{k}^{t})}{\\sum_{j}\\pi_{j}^{t}f_{j}(x_{i};\\mu_{k}^{t},\\sigma_{k}^{t})}$$\nWe can now separately maximize $Q(\\theta|\\theta^{t})$ with respect to\n$\\pi_{k}$ and $\\mu_{k},\\sigma_{k}$ to find $\\pi_{k}^{t+1}$ and\n$\\mu_{k}^{t+1},\\sigma_{k}^{t+1}$, since the expression is the sum of a\nterm involving the parameters of the distributions and a term involving\nthe mixture probabilities. In the latter case, if the $f_{k}$ are normal\ndistributions, you end up with a weighted sum of normal distributions,\nfor which the estimators of the mean and variance parameters are the\nweighted mean of the observations and the weighted variance.\n\n# 9. Optimization under constraints\n\nConstrained optimization is harder than unconstrained, and inequality\nconstraints harder to deal with than equality constraints.\n\nConstrained optimization can sometimes be avoided by reparameterizing. Some examples include:\n- working on the log scale (e.g., to optimize w.r.t. a variance component or other non-negative parameter)\n- using the logit transformation to optimize with respect to a parameter on $(0,1)$ (or more generally some other bounded interval, after shifting and scaling to $(0,1)$.\n\nOptimization under constraints often goes under the name of\n'programming', with different types of programming for different types\nof objective functions combined with different types of constraints.\n\n## Convex optimization (convex programming)\n\nConvex programming minimizes $f(x)$ s.t. $h_{j}(x)\\leq0,\\,j=1,\\ldots,m$\nand $a_{i}^{\\top}x=b_{i},\\,i=1,\\ldots,q$, where both $f$ and the\nconstraint functions are convex. Note that this includes more general\nequality constraints, as we can write $g(x)=b$ as two inequalities\n$g(x)\\leq b$ and $g(x)\\geq b$. It also includes $h_{j}(x)\\geq b_{j}$ by\ntaking $-h_{j}(x)$. Note that we can always have $h_{j}(x)\\leq b_{j}$\nand convert to the above form by subtracting $b_{j}$ from each side\n(note that this preserves convexity). A vector $x$ is said to be\nfeasible, or in the feasible set, if all the constraints are satisfied\nfor $x$.\n\nThere are good algorithms for convex programming, and it's possible to\nfind solutions when we have hundreds or thousands of variables and\nconstraints. It is often difficult to recognize if one has a convex\nprogram (i.e., if $f$ and the constraint functions are convex), but\nthere are many tricks to transform a problem into a convex program and\nmany problems can be solved through convex programming. So the basic\nchallenge is in recognizing or transforming a problem to one of convex\noptimization; once you've done that, you can rely on existing methods to\nfind the solution.\n\nLinear programming, quadratic programming, second order cone programming\nand semidefinite programming are all special cases of convex\nprogramming. In general, these types of optimization are progressively\nmore computationally complex.\n\nFirst let's see some of the special cases and then discuss the more\ngeneral problem.\n\n## Linear programming: Linear system, linear constraints\n\nLinear programming seeks to minimize $$f(x)=c^{\\top}x$$ subject to a\nsystem of $m$ inequality constraints, $a_{i}^{\\top}x\\leq b_{i}$ for\n$i=1,\\ldots,m$, where $A$ is of full row rank. This can also be written\nin terms of generalized inequality notation, $Ax\\preceq b$. There are\nstandard algorithms for solving linear programs, including the simplex\nmethod and interior point methods.\n\nNote that each equation in the set of equations $Ax=b$ defines a\nhyperplane, so each inequality in $Ax\\preceq b$ defines a half-space.\nMinimizing a linear function (presuming that the minimum exists) must\nmean that we push in the correct direction towards the boundaries formed\nby the hyperplanes, with the solution occuring at a corner (vertex) of\nthe solid formed by the hyperplanes. The simplex algorithm starts with a\nfeasible solution at a corner and moves along edges in directions that\nimprove the objective function.\n\n## General system, equality constraints\n\nSuppose we have an objective function $f(x)$ and we have equality\nconstraints, $Ax=b$. We can manipulate this into an unconstrained\nproblem. The null space of $A$ is the set of $\\delta$ s.t. $A\\delta=0$.\nSo if we start with a candidate $x_{c}$ s.t. $Ax_{c}=b$ (e.g., by using\nthe pseudo inverse, $A^{+}b$), we can form all other candidates (a\ncandidate is an $x$ s.t. $Ax=b$) as $x=x_{c}+\\delta=x_{c}+Bz$ where $B$\nis a set of column basis functions for the null space of $A$ and\n$z\\in\\Re^{p-m}$. Consider $h(z)=f(x_{c}+Bz)$ and note that $h$ is a\nfunction of $p-m$ rather than $p$ inputs. Namely, we are working in a\nreduced dimension space with no constraints. If we assume\ndifferentiability of $f$, we can express\n$\\nabla h(z)=B^{\\top}\\nabla f(x_{c}+Bz)$ and\n$H_{h}(z)=B^{\\top}H_{f}(x_{c}+Bz)B$. Then we can use unconstrained\nmethods to find the point at which $\\nabla h(z)=0$.\n\nHow do we find $B$? One option is to use the $p-m$ columns of $V$ in the\nSVD of $A$ that correspond to singular values that are zero. A second\noption is to take the QR decomposition of $A^{\\top}$. Then $B$ is the\ncolumns of $Q_{2}$, where these are the columns of the (non-skinny) Q\nmatrix corresponding to the rows of $R$ that are zero.\n\nFor more general (nonlinear) equality constraints, $g_{i}(x)=b_{i}$,\n$i=1,\\ldots,q$, we can use the Lagrange multiplier approach to define a\nnew objective function, $$L(x,\\lambda)=f(x)+\\lambda^{\\top}(g(x)-b)$$ for\nwhich, if we set the derivative (with respect to both $x$ and the\nLagrange multiplier vector, $\\lambda$) equal to zero, we have a critical\npoint of the original function and we respect the constraints.\n\nAn example occurs with quadratic programming, under the simplification\nof affine equality constraints (quadratic programming in general\noptimizes a quadratic function under affine inequality constraints -\ni.e., constraints of the form $Ax-b\\preceq0$). For example we might\nsolve a least squares problem subject to linear equality constraints,\n$f(x)=\\frac{1}{2}x^{\\top}Qx+m^{\\top}x+c$ s.t. $Ax=b$, where $Q$ is\npositive semi-definite. The Lagrange multiplier approach gives the\nobjective function\n$$L(x,\\lambda)=\\frac{1}{2}x^{\\top}Qx+m^{\\top}x+c+\\lambda^{\\top}(Ax-b)$$\nand differentiating gives the equations $$\\begin{aligned}\n\\frac{\\partial L(x,\\lambda)}{\\partial x} & =m+Qx+A^{\\top}\\lambda  =  0\\\\\n\\frac{\\partial L(x,\\lambda)}{\\partial\\lambda} & =Ax  =  b,\\end{aligned}$$\nwhich gives us a system of equations that leads to the solution $$\\left(\\begin{array}{c} x \\\\ \\lambda \\end{array} \\right) =\n\\left( \\begin{array}{cc} Q & A^{\\top} \\\\ A & 0 \\end{array}\\right)^{-1}\n\\left(\\begin{array}{c} -m \\\\ b \\end{array}\\right).\\label{eq:quadProg}$$ \n\nUsing known results for\ninverses of matrices split into blocks, one gets that\n$x^{*}=-Q^{-1}m+Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}(AQ^{-1}m+b)$. This\ncan be readily coded up using strategies from Unit 10.\n\n## The dual problem (optional)\n\nSometimes a reformulation of the problem eases the optimization. There\nare different kinds of dual problems, but we'll just deal with the\nLagrangian dual. Let $f(x)$ be the function we want to minimize, under\nconstraints $g_{i}(x)=0;\\,i=1,\\ldots,q$ and\n$h_{j}(x)\\leq0;\\,j=1,\\ldots,m$. Here I've explicitly written out the\nequality constraints to follow the notation in Lange. Consider the\nLangrangian,\n$$L(x,\\lambda,\\mu)=f(x)+\\sum_{i}\\lambda_{i}g_{i}(x)+\\sum_{j}\\mu_{j}h_{j}(x).$$\n\nSolving that can be shown to be equivalent to this optimization:\n$$\\inf_{x}\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}L(x,\\lambda,\\mu)$$ where the\nsupremum ensures that the constraints are satisfied because the\nLagrangian is infinity if the constraints are not satisfied.\n\nLet's consider interchanging the minimization and maximization. For\n$\\mu\\succeq0$, one can show that\n$$\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}\\inf_{x}L(x,\\lambda,\\mu)\\leq\\inf_{x}\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}L(x,\\lambda,\\mu),$$\nbecause $\\inf_{x}L(x,\\lambda,\\mu)\\leq f(x^{*})$ for the minimizing value\n$x^{*}$ (p. 216 of the Boyd book). This gives us the Lagrange dual\nfunction: $$d(\\lambda,\\mu)=\\inf_{x}L(x,\\lambda,\\mu),$$ and the Lagrange\ndual problem is to find the best lower bound:\n$$\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}d(\\lambda,\\mu).$$\n\nThe dual problem is always a convex optimization problem because\n$d(\\lambda,\\mu)$ is concave (because $d(\\lambda,\\mu)$ is a pointwise\ninfimum of a family of affine functions of $(\\lambda,\\mu)$). If the\noptima of the primal (original) problem and that of the dual do not\ncoincide, there is said to be a \"duality gap\". For convex programming,\nif certain conditions are satisfied (called *constraint\nqualifications*), then there is no duality gap, and one can solve the\ndual problem to solve the primal problem. Usually with the standard form\nof convex programming, there is no duality gap. Provided we can do the\nminimization over $x$ in closed form we then maximize $d(\\lambda,\\mu)$\nw.r.t. the Lagrangian multipliers in a new constrained problem that is\nsometimes easier to solve, giving us $(\\lambda^{*},\\mu^{*})$.\n\nOne can show (p. 242 of the Boyd book) that $\\mu_{i}^{*}=0$ unless the\n$i$th constraint is active at the optimum $x^{*}$ and that $x^{*}$\nminimizes $L(x,\\lambda^{*},\\mu^{*})$. So once one has\n$(\\lambda^{*},\\mu^{*})$, one is in the position of minimizing an\nunconstrained convex function. If $L(x,\\lambda^{*},\\mu^{*})$ is strictly\nconvex, then $x^{*}$ is the unique optimum provided $x^{*}$ satisfies\nthe constraints, and no optimum exists if it does not.\n\nHere's a simple example: suppose we want to minimize $x^{\\top}x$ s.t.\n$Ax=b$. The Lagrangian is $L(x,\\lambda)=x^{\\top}x+\\lambda^{\\top}(Ax-b)$.\nSince $L(x,\\lambda)$ is quadratic in $x$, the infimum is found by\nsetting $\\nabla_{x}L(x,\\lambda)=2x+A^{\\top}\\lambda=0$, yielding\n$x=-\\frac{1}{2}A^{\\top}\\lambda$. So the dual function is obtained by\nplugging this value of $x$ into $L(x,\\lambda)$, which gives\n$$d(\\lambda)=-\\frac{1}{4}\\lambda^{\\top}AA^{\\top}\\lambda-b^{\\top}\\lambda,$$\nwhich is concave quadratic. In this case we can solve the original\nconstrained problem in terms of this unconstrained dual problem.\n\nAnother example is the primal and dual forms for finding the SVM\nclassifier (see [the Wikipedia\narticle](https://en.wikipedia.org/wiki/Support_vector_machine#Primal_form)).\nIn this algorithm, we want to develop a classifier using $n$ pairs of\n$y\\in\\Re^{1}$ and $x\\in\\Re^{p}$. The dual form is easily derived because\nthe minimization over $x$ occurs in a function that is quadratic in $x$.\nExpressing the problem in the primal form gives an optimization in\n$\\Re^{p}$ while doing so in the dual form gives an optimization in\n$\\Re^{n}$. So one reason to use the dual form would be if you have\n$n\\ll p$.\n\n## KKT conditions (optional)\n\nKarush-Kuhn-Tucker (KKT) theory provides sufficient conditions under\nwhich a constrained optimization problem has a minimum, generalizing the\nLagrange multiplier approach. The Lange and Boyd books have whole\nsections on this topic.\n\nSuppose that the function and the constraint functions are continuously\ndifferentiable near $x^{*}$ and that we have the Lagrangian as before:\n$$L(x,\\lambda,\\mu)=f(x)+\\sum_{i}\\lambda_{i}g_{i}(x)+\\sum_{j}\\mu_{j}h_{j}(x).$$\n\nFor nonconvex problems, if $x^{*}$ and $(\\lambda^{*},\\mu^{*})$ are the\nprimal and dual optimal points and there is no duality gap, then the KKT\nconditions hold: $$\\begin{aligned}\nh_{j}(x^{*}) & \\leq & 0\\\\\ng_{i}(x^{*}) & = & 0\\\\\n\\mu_{j}^{*} & \\geq & 0\\\\\n\\mu_{j}^{*}h_{j}(x^{*}) & = & 0\\\\\n\\nabla f(x^{*})+\\sum_{i}\\lambda_{i}^{*}\\nabla g_{i}(x^{*})+\\sum_{j}\\mu_{j}^{*}\\nabla h_{j}(x^{*}) & = & 0.\\end{aligned}$$\n\nFor convex problems, we also have that if the KKT conditions hold, then\n$x^{*}$ and $(\\lambda^{*},\\mu^{*})$ are primal and dual optimal and\nthere is no duality gap.\n\nWe can consider this from a slightly different perspective, in this case\nrequiring that the Lagrangian be twice differentiable.\n\nFirst we need a definition. A *tangent direction*, $w$, with respect to\n$g(x)$, is a vector for which $\\nabla g_{i}(x)^{\\top}w=0$. If we are at\na point, $x^{*}$, at which the constraint is satisfied,\n$g_{i}(x^{*})=0$, then we can move in the tangent direction (orthogonal\nto the gradient of the constraint function) (i.e., along the level\ncurve) and still satisfy the constraint. This is the only kind of\nmovement that is legitimate (gives us a feasible solution).\n\nIf the gradient of the Lagrangian with respect to $x$ is equal to 0,\n$$\\nabla f(x^{*})+\\sum_{i}\\lambda_{i}\\nabla g_{i}(x^{*})+\\sum_{j}\\mu_{j}\\nabla h_{j}(x^{*})=0,$$\nand if $w^{\\top}H_{L}(x^{*},\\lambda,\\mu)w>0$ (with $H_{L}$ being the\nHessian of the Lagrangian) for all vectors $w$ s.t.\n$\\nabla g(x^{*})^{\\top}w=0$ and, for all active\nconstraints,$\\nabla h(x^{*})^{\\top}w=0$, then $x^{*}$ is a local\nminimum. An active constraint is an inequality for which\n$h_{j}(x^{*})=0$ (rather than $h_{j}(x^{*})<0$, in which case it is\ninactive). Basically we only need to worry about the inequality\nconstraints when we are on the boundary, so the goal is to keep the\nconstraints inactive.\n\nSome basic intuition is that we need positive definiteness only for\ndirections that stay in the feasible region. That is, our only possible\ndirections of movement (the tangent directions) keep us in the feasible\nregion, and for these directions, we need the objective function to be\nincreasing to have a minimum. If we were to move in a direction that\ngoes outside the feasible region, it's ok for the quadratic form\ninvolving the Hessian to be negative.\n\nMany algorithms for convex optimization can be interpreted as methods\nfor solving the KKT conditions.\n\n## Interior-point methods\n\nWe'll briefly discuss one of the standard methods for solving a convex\noptimization problem. The barrier method is one type of interior-point\nalgorithm. It turns out that Newton's method can be used to solve a\nconstrained optimization problem, with twice-differentiable $f$ and\nlinear equality constraints. So the basic strategy of the barrier method\nis to turn the more complicated constraint problem into one with only\nlinear equality constraints.\n\nRecall our previous notation, in which convex programming minimizes\n$f(x)$ s.t. $h_{i}(x)\\leq0,\\,j=1,\\ldots,m$ and\n$a_{i}^{\\top}x=b_{i},\\,i=1,\\ldots,q$, where both $f$ and the constraint\nfunctions are convex. The strategy begins with moving the inequality\nconstraints into the objective function:\n$$f(x)+\\sum_{j=1}^{m}I_{-}(h_{j}(x))$$ where $I_{-}(u)=0$ if $u\\leq0$ and $I_{-}(u)=\\infty$ if $u>0$.\n\nThis is fine, but the new objective function is not differentiable so we\ncan't use a Newton-like approach. Instead, we approximate the indicator\nfunction with a logarithmic function, giving the new objective function\n$$\\tilde{f}(x)=f(x)+\\sum_{j=1}^{m}-(1/t^{*})\\log(-h_{j}(x)),$$ which is\nconvex and differentiable. The new term pushes down the value of the\noverall objective function when $x$ approaches the boundary, nearing\npoints for which the inequality constraints are not met. The\n$-\\sum(1/t^{*})\\log(-h_{j}(x))$ term is called the log barrier, since it\nkeeps the solution in the feasible set (i.e., the set where the\ninequality constraints are satisfied), provided we start at a point in\nthe feasible set. Newton's method with equality constraints ($Ax=b$) is\nthen applied. The key thing is then to have $t^{*}$ get larger (i.e.,\n$t^{*}$ is some increasing function of iteration time $t$) as the\niterations proceed, which allows the solution to get closer to the\nboundary if that is indeed where the minimum lies.\n\nThe basic ideas behind Newton's method with equality constraints are (1)\nstart at a feasible point, $x_{0}$, such that $Ax_{0}=b$, and (2) make\nsure that each step is in a feasible direction, $A(x_{t+1}-x_{t})=0$. To\nmake sure the step is in a feasible direction we have to solve a linear\nsystem similar to that in the simplified quadratic programming problem: $$\\left(\\begin{array}{c}\nx_{t+1}-x_{t}\\\\\n\\lambda\n\\end{array}\\right)=\\left(\\begin{array}{cc}\nH_{\\tilde{f}}(x_{t}) & A^{\\top}\\\\\nA & 0\n\\end{array}\\right)^{-1}\\left(\\begin{array}{c}\n-\\nabla\\tilde{f}(x_{t})\\\\\n0\n\\end{array}\\right),$$ which shouldn't be surprising since the whole idea\nof Newton's method is to substitute a quadratic approximation for the\nactual objective function.\n\n## Software for constrained and convex optimization\n\nFor general convex optimization in Python see the `cvxopt` package. Some\nother resources to consider are\n\n-   MATLAB, in particular the `fmincon()` function, the CVX system, and\n    MATLAB's linear and quadratic programming abilities.\n-   The CVXR package in R.\n\nI haven't looked into CVXR in detail but given the developers include\nStephen Boyd, who is a convex optimization guru, it's worth checking\nout.\n\n`cvxopt` has specific solves (see `help(cvxopt.solvers)` for different\nkinds of convex optimization. A general purpose one is `cvxopt.solvers.cp`.\nSpecifying the problem (the objective function, nonlinear constraints, and linear constraints) using the software is somewhat involved, so I haven't worked out an example here.\n\n\n\n# 10. Summary\n\nThe different methods of optimization have different advantages and\ndisadvantages.\n\nAccording to Lange, MM and EM are numerically stable and computationally\nsimple but can converge very slowly. Newton's method shows very fast\nconvergence but has the downsides we've discussed. Quasi-Newton methods\nfall in between. Convex optimization generally comes up when optimizing\nunder constraints.\n\nOne caution about optimizing under constraints is that you just get a\npoint estimate; quantifying uncertainty in your estimator is more\ndifficult. One strategy is to ignore the inactive inequality constraints\nand reparameterize (based on the active equality constraints) to get an\nunconstrained problem in a lower-dimensional space. Then you can make\nuse of the Hessian in the usual fashion to estimate the information\nmatrix.\n\n",
    "supporting": [
      "unit11-optim_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}