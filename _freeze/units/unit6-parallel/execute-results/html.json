{
  "hash": "e9481d37d16f89cb58da80e479d12c6f",
  "result": {
    "markdown": "---\ntitle: \"Parallel processing\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-07-26\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\nfrom: markdown+tex_math_single_backslash\n---\n\n::: {.cell}\n\n:::\n\n\n\n[PDF](./unit6-parallel.pdf){.btn .btn-primary}\n\nReferences:\n\n-   [Tutorial on parallel processing using Python's Dask and R's future packages](https://berkeley-scf.github.io/tutorial-dask-future)\n\n\nThis unit will be fairly Linux-focused as most serious parallel\ncomputation is done on systems where some variant of Linux is running.\nThe single-machine parallelization discussed here should work on Macs\nand Windows, but some of the details of what is happening under the hood\nare different for Windows.\n\n# 1. Some scenarios for parallelization\n\n-   You need to fit a single statistical/machine learning model, such as\n    a random forest or regression model, to your data.\n-   You need to fit three different statistical/machine learning models\n    to your data.\n-   You are running a prediction method on 10 cross-validation folds,\n    possibly using multiple statistical/machine learning models to do\n    prediction.\n-   You are running an ensemble prediction method such as *SuperLearner*\n    or *Bayesian model averaging* over 10 cross-validation folds, with\n    30 statistical/machine learning methods used for each fold.\n-   You are running stratified analyses on a very large dataset (e.g.,\n    running regression models once for each subgroup within a dataset).\n-   You are running a simulation study with n=1000 replicates. Each\n    replicate involves fitting 10 statistical/machine learning methods.\n\nGiven you are in such a situation, can you do things in parallel? Can\nyou do it on your laptop or a single computer? Will it be useful (i.e.,\nfaster or provide access to sufficient memory) to use multiple\ncomputers, such as multiple nodes in a Linux cluster?\n\nAll of the functionality discussed in this Unit applies ONLY if the\niterations/loops of your calculations can be done completely separately\nand do not depend on one another; i.e., you can do the computation as\nseparate processes without communication between the processes. This\nscenario is called an *embarrassingly parallel* computation.\n\n## Embarrassingly parallel (EP) problems\n\nAn EP problem is one that can be solved by doing independent\ncomputations in separate processes without communication between the\nprocesses. You can get the answer by doing separate tasks and then\ncollecting the results. Examples in statistics include\n\n1.  simulations with many independent replicates\n2.  bootstrapping\n3.  stratified analyses\n4.  random forests\n5.  cross-validation.\n\nThe standard setup is that we have the same code running on different\ndatasets. (Note that different processes may need different random\nnumber streams, as we will discuss in the Simulation Unit.)\n\nTo do parallel processing in this context, you need to have control of\nmultiple processes. Note that on a shared system with\nqueueing/scheduling software set up, this will generally mean requesting\naccess to a certain number of processors and then running your job in\nsuch a way that you use multiple processors.\n\nIn general, except for some modest overhead, an EP problem can ideally\nbe solved with $1/p$ the amount of time for the non-parallel\nimplementation, given $p$ CPUs. This gives us a speedup of $p$, which is\ncalled linear speedup (basically anytime the speedup is of the form $kp$\nfor some constant $k$).\n\n# 2. Overview of parallel processing\n\n## Computer architecture\n\nComputers now come with multiple processors for doing computation.\nBasically, physical constraints have made it harder to keep increasing\nthe speed of individual processors, so the chip industry is now putting\nmultiple processing units in a given computer and trying/hoping to rely\non implementing computations in a way that takes advantage of the\nmultiple processors.\n\nEveryday personal computers usually have more than one processor (more\nthan one chip) and on a given processor, often have more than one core\n(multi-core). A multi-core processor has multiple processors on a single\ncomputer chip. On personal computers, all the processors and cores share\nthe same memory.\n\nSupercomputers and computer clusters generally have tens, hundreds, or\nthousands of 'nodes', linked by a fast local network. Each node is\nessentially a computer with its own processor(s) and memory. Memory is\nlocal to each node (distributed memory). One basic principle is that\ncommunication between a processor and its memory is much faster than\ncommunication between processors with different memory. An example of a\nmodern supercomputer is the Cori supercomputer at Lawrence Berkeley\nNational Lab, which has 12,076 nodes, and a total of 735,200 cores. Each\nnode has either 96 or 128 GB of memory for a total of 1.3 PB of memory.\n\nFor our purposes, there is little practical distinction between\nmulti-processor and multi-core situations. The main issue is whether\nprocesses share memory or not. In general, I won't distinguish between\ncores and processors. We'll just focus on the number of cores on given\npersonal computer or a given node in a cluster.\n\n## Some useful terminology:\n\n-   *cores*: We'll use this term to mean the different processing units\n    available on a single machine or node of a cluster.\n-   *nodes*: We'll use this term to mean the different computers, each\n    with their own distinct memory, that make up a cluster or\n    supercomputer.\n-   *processes*: instances of a program(s) executing on a machine;\n    multiple processes may be executing at once. A given program may\n    start up multiple processes at once. Ideally we have no more\n    processes than cores on a node.\n-   *workers*: the individual processes that are carrying out the\n    (parallelized) computation. We'll use *worker* and *process*\n    interchangeably.\n-   *tasks*: individual units of computation; one or more tasks will be\n    executed by a given process on a given core.\n-   *threads*: multiple paths of execution within a single process; the\n    operating system sees the threads as a single process, but one can think of them\n    as 'lightweight' processes. Ideally when considering the processes\n    and their threads, we would the same number of cores as we have\n    processes and threads combined.\n-   *forking*: child processes are spawned that are identical to the\n    parent, but with different process IDs and their own memory. In some\n    cases if objects are not changed, the objects in the child process\n    may refer back to the original objects in the original process,\n    avoiding making copies.\n-   *sockets*: some of R's parallel functionality involves creating new\n    R processes (e.g., starting processes via `Rscript`) and\n    communicating with them via a communication technology called\n    sockets.\n-   *scheduler*: a program that manages users' jobs on a cluster.\n    *Slurm* is a commonly used scheduler.\n-   *load-balanced*: when all the cores that are part of a computation\n    are busy for the entire period of time the computation is running.\n\n## Distributed vs. shared memory\n\nThere are two basic flavors of parallel processing (leaving aside GPUs):\ndistributed memory and shared memory. With shared memory, multiple\nprocessors (which I'll call cores for the rest of this document) share\nthe same memory. With distributed memory, you have multiple nodes, each\nwith their own memory. You can think of each node as a separate computer\nconnected by a fast network.\n\n### Shared memory\n\nFor shared memory parallelism, each core is accessing the same memory so\nthere is no need to pass information (in the form of messages) between\ndifferent machines. However, unless one is using threading (or in some\ncases when one has processes created by forking), objects will still be\ncopied when creating new processes to do the work in parallel. With\nthreaded computations, multiple threads can access object(s) without\nmaking explicit copies. But in some programming contexts one needs to be\ncareful that the threads on different cores doesn't mistakenly overwrite\nplaces in memory that are used by other cores (this is not an issue in\nR).\n\nWe'll cover two types of shared memory parallelism approaches in this\nunit:\n\n-   threaded linear algebra\n-   multicore functionality\n\n#### Threading\n\nThreads are multiple paths of execution within a single process. If you\nare monitoring CPU usage (such as with `top` in Linux or Mac) and\nwatching a job that is executing threaded code, you'll see the process\nusing more than 100% of CPU. When this occurs, the process is using\nmultiple cores, although it appears as a single process rather than as\nmultiple processes.\n\nNote that this is a different notion than a processor that is\nhyperthreaded. With hyperthreading a single core appears as two cores to\nthe operating system.\n\n### Distributed memory\n\nParallel programming for distributed memory parallelism requires passing\nmessages between the different nodes. The standard protocol for doing\nthis is MPI, of which there are various versions, including `openMPI`.\n\nWhile there are various Python and R that use MPI behind the scenes, we'll only cover distributed\nmemory parallelization via `Dask`, which doesn't use\nMPI.\n\n## Some other approaches to parallel processing\n\n### GPUs\n\nGPUs (Graphics Processing Units) are processing units originally\ndesigned for rendering graphics on a computer quickly. This is done by\nhaving a large number of simple processing units for massively parallel\ncalculation. The idea of general purpose GPU (GPGPU) computing is to\nexploit this capability for general computation.\n\nMost researchers don't program for a GPU directly but rather use\nsoftware (often machine learning software such as Tensorflow or PyTorch,\nor other software that automatically uses the GPU such as JAX)\n that has been programmed to take advantage of a GPU if one is\navailable. The computations that run on the GPU are run in GPU *kernels*,\nwhich are functions that are launched on the GPU. The overall workflow\nruns on the CPU and then particular (usually computationally-intensive\ntasks for which parallelization is helpful) tasks are handed off to the GPU.\nGPUs and similar devices (e.g., TPUs) are often called \"co-processors\"\nin recognition of this style of workflow.\n\nThe memory on a GPU is distinct from main memory on the computer, so\nwhen writing code that will use the GPU, one generally wants to avoid\nhaving large amounts of data needing to be transferred back and forth between\nmain (CPU) memory and GPU memory. Also, since there is overhead in\nlaunching a GPU kernel, one wants to avoid launching a lot of kernels\nrelative to the amount of work being done by each kernel. \n\n### Spark and Hadoop\n\nSpark and Hadoop are systems for implementing computations in a\ndistributed memory environment, using the MapReduce approach, as\ndiscussed in Unit 7.\n\n### Cloud computing\n\nAmazon (Amazon Web Services' EC2 service), Google (Google Cloud\nPlatform's Compute Engine service) and Microsoft (Azure) offer computing\nthrough the cloud. The basic idea is that they rent out their servers on\na pay-as-you-go basis. You get access to a virtual machine that can run\nvarious versions of Linux or Microsoft Windows server and where you\nchoose the number of processing cores you want. You configure the\nvirtual machine with the applications, libraries, and data you need and\nthen treat the virtual machine as if it were a physical machine that you\nlog into as usual. You can also assemble multiple virtual machines into\nyour own virtual cluster and use platforms such as Spark on the cloud\nprovider's virtual machines.\n\n# 3. Parallelization strategies\n\nSome of the considerations that apply when thinking about how effective\na given parallelization approach will be include:\n\n-   the amount of memory that will be used by the various processes,\n-   the amount of communication that needs to happen -- how much data\n    will need to be passed between processes,\n-   the latency of any communication - how much delay/lag is there in\n    sending data between processes or starting up a worker process, and\n-   to what extent do processes have to wait for other processes to\n    finish before they can do their next step.\n\nThe following are some basic principles/suggestions for how to\nparallelize your computation.\n\n-   Should I use one machine/node or many machines/nodes?\n    -   If you can do your computation on the cores of a single node\n        using shared memory, that will be faster than using the same\n        number of cores (or even somewhat more cores) across multiple\n        nodes. Similarly, jobs with a lot of data/high memory\n        requirements that one might think of as requiring Spark or\n        Hadoop may in some cases be much faster if you can find a single\n        machine with a lot of memory.\n    -   That said, if you would run out of memory on a single node, then\n        you'll need to use distributed memory.\n-   What level or dimension should I parallelize over?\n    -   If you have nested loops, you generally only want to parallelize\n        at one level of the code. That said, in this unit we'll see some\n        tools for parallelizing at multiple levels. Keep in mind whether\n        your linear algebra is being threaded. Often you will want to\n        parallelize over a loop and not use threaded linear algebra\n        within the iterations of the loop.\n    -   Often it makes sense to parallelize the outer loop when you have\n        nested loops.\n    -   You generally want to parallelize in such a way that your code\n        is load-balanced and does not involve too much communication.\n-   How do I balance communication overhead with keeping my cores busy?\n    -   If you have very few tasks, particularly if the tasks take\n        different amounts of time, often some processors will be idle\n        and your code poorly load-balanced.\n    -   If you have very many tasks and each one takes little time, the\n        overhead of starting and stopping the tasks will reduce\n        efficiency.\n-   Should multiple tasks be pre-assigned (statically assigned) to a\n    process (i.e., a worker) (sometimes called *prescheduling*) or\n    should tasks be assigned dynamically as previous tasks finish?\n    -   To illustrate the difference, suppose you have 6 tasks and 3\n        workers. If the tasks are pre-assigned, worker 1 might be\n        assigned tasks 1 and 4 at the start, worker 2 assigned tasks 2\n        and 5, and worker 3 assigned tasks 3 and 6. If the tasks are\n        dynamically assigned, worker 1 would be assigned task 1, worker\n        2 task 2, and worker 3 task 3. Then whichever worker finishes\n        their task first (it woudn't necessarily be worker 1) would be\n        assigned task 4 and so on.\n    -   Basically if you have many tasks that each take similar time,\n        you want to preschedule the tasks to reduce communication. If\n        you have few tasks or tasks with highly variable completion\n        times, you don't want to preschedule, to improve load-balancing.\n    -   For R in particular, some of R's parallel functions allow you to\n        say whether the tasks should be prescheduled. In the future\n        package, `future_lapply` has arguments `future.scheduling` and\n        `future.chunk.size`. Similarly, there is the `mc.preschedule`\n        argument in `mclapply()`.\n\n# 4. Introduction to Dask \n\nBefore we illustrate implementation of various kinds of parallelization,\nI'll give an overview of the `Dask` package, which we'll use for many\nof the implementations. \n\nDask has similar functionality to R's future package for parallelizing\nacross one or more machines/nodes. In addition, it has the important\nfeature of handling distributed datasets - datasets that are split into\nchunks/shareds and operated on in parallel. We'll see more about\ndistributed datasets in Unit 7 but here we'll introduce the basic\nfunctionality.\n\nThere are lots (and lots) of other packages in Python that also provide\nfunctionality for parallel processing, including `ipyparallel`, `ray`,\n`multiprocessing`, and `pp`.\n\n## Overview: Key idea\n\nA key idea in Dask (and in R's `future` package and the `ray` package for Python) involve abstracting\nthe parallelization away from the computational resources that the\ncode will be run on. We want to:\n\n-   Separate what to parallelize from how and where the parallelization\n    is actually carried out.\n-   Allow different users to run the same code on different computational\n    resources (without touching the actual code that does the\n    computation).\n\nThe computational resources on which the code is run is sometimes called the *backend*.\n\n## Overview of parallel backends\n\nOne sets the *scheduler* to control how parallelization is done, whether\nto run code on multiple machines, and how many cores on each machine to use.\n\nFor example to parallelize across multiple cores via separate Python\nprocesses, we'd do this.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \n```\n:::\n\n\n\nThis table shows the different types of schedulers.\n\n|      Type     |               Description              |  Multi-node |  Copies of objects made? |\n|  -------------| ---------------------------------------| ------------| -------------------------|\n|   synchronous |        not in parallel (serial)        |      no     |            no |\n|    threads    |  threads within current Python session |      no     |            no |\n|    processes  |       background Python sessions       |      no     |            yes |\n|   distributed |  Python sessions across multiple nodes |     yes     |            yes |\n\n\nComments:\n\n1.  Note that because of Python's Global Interpreter Lock (GIL) (which\n    prevents threading of Python code), many computations done in pure\n    Python code won't be parallelized using the 'threads' scheduler;\n    however computations on numeric data in numpy arrays, Pandas\n    dataframes and other C/C++/Cython-based code will parallelize.\n2.  It's fine to use the distributed scheduler on one machine, such as\n    your laptop. According to the Dask documentation, it has advantages\n    over multiprocessing, including the diagnostic dashboard (see the\n    tutorial) and better handling of when copies need to be made. In\n    addition, one needs to use it for parallel map operations (see next\n    section).\n\n## Accessing variables and workers in the worker processes\n\nDask usually does a good job of identifying the packages and (global) variables\nyou use in your parallelized code and importing those packages on the workers and copying necessary variables to the workers.\n\nHere's a toy example that shows that the `numpy` package and a global variable `n` are automatically available in the worker processes without any action on our part.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4, chunksize = 1)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe91c2dc890>\n```\n:::\n\n```{.python .cell-code}\nimport numpy as np\nn = 10\n\ndef myfun(idx):\n   return np.random.normal(size = n)\n\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(myfun)(i))  # add lazy task\n\ntasks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[Delayed('myfun-29a9b5c5-3ead-439f-b448-2a5f0e120477'), Delayed('myfun-a97b301a-8c03-4dcf-a714-4d257fbb69b0'), Delayed('myfun-bf4cc41d-4a72-4599-8d7d-e663f392b672'), Delayed('myfun-ccc64af4-5de7-4816-b483-1b8dccea607e'), Delayed('myfun-6ef1521b-72b5-42f7-8237-770fd806d678'), Delayed('myfun-a1478bf8-cdd5-42e1-8884-ebf3dcb033aa'), Delayed('myfun-cdb17603-0ce0-463a-92f5-ba39284f9c75'), Delayed('myfun-0a348419-2931-4a09-8efe-f708570a6c15')]\n```\n:::\n\n```{.python .cell-code}\nresults = dask.compute(tasks)  # compute all in parallel\n```\n:::\n\n\n\nIn other contexts (in various languages) you may need to explicitly copy objects to the workers (or load packages on the workers). This is sometimes called *exporting* variables. \n\n# 5. Illustrating the principles in specific case studies\n\n## Scenario 1: one model fit\n\n**Scenario**: You need to fit a single statistical/machine learning\nmodel, such as a random forest or regression model, to your data.\n\n### Scenario 1A:\n\nA given method may have been written to use parallelization and you\nsimply need to figure out how to invoke the method for it to use\nmultiple cores.\n\nFor example the documentation for the `RandomForestClassifier`\nin scikit-learn's `ensemble` module indicates it can use multiple\ncores -- note the `n_jobs` argument (not shown here because\nthe help info is very long).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sklearn.ensemble\nhelp(sklearn.ensemble.RandomForestClassifier)\n```\n:::\n\n\n\nYou'll usually need to look for an argument with one of the words *threads*, *processes*,\n*cores*, *cpus*, *jobs*, etc. in the argument name.\n\n### Scenario 1B:\n\nIf a method does linear algebra computations on large matrices/vectors,\nPython (and R) can call out to parallelized linear algebra packages (the BLAS and\nLAPACK).\n\nThe BLAS is the library of basic linear algebra operations (written in\nFortran or C). A fast BLAS can greatly speed up linear algebra in R\nrelative to the default BLAS that comes with R. Some fast BLAS libraries\nare\n\n-   Intel's *MKL*; available for educational use for free\n-   *OpenBLAS*; open source and free\n-   *vecLib* for Macs; provided with your Mac\n\nIn addition to being fast when used on a single core, all of these BLAS\nlibraries are threaded - if your computer has multiple cores and there\nare free resources, your linear algebra will use multiple cores,\nprovided your program is linked against the threaded BLAS installed on\nyour machine and provided the environment variable `OMP_NUM_THREADS` is\nnot set to one. (Macs make use of `VECLIB_MAXIMUM_THREADS` rather than\n`OMP_NUM_THREADS`.)\n\nIt's also possible to use an optimized BLAS with Python's `numpy` and\n`scipy` packages, on either Linux or using the Mac's *vecLib* BLAS.\nDetails will depend on how you install Python, numpy, and scipy.\n\nDask and some other packages also provide threading, but pure Python code is not threaded.\n\nHere's some code that illustrates the speed of using a threaded BLAS:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport time\n\nx = np.random.normal(size = (6000, 6000))\n\nstart_time = time.time()\nx = np.dot(x.T, x) \nU = np.linalg.cholesky(x)\nelapsed_time = time.time() - start_time\nprint(\"Elapsed Time (8 threads):\", elapsed_time)\n```\n:::\n\n\n\nWe'd need to restart Python after setting `OMP_NUM_THREADS` to 1 in order\nto compare the time when run in parallel vs. on a single core.\nThat's hard to demonstrate in this generated document, but when I ran it,\nit took 6.6 seconds, compared to 3 seconds using 8 cores.\n\nNote that for smaller linear algebra problems, we may not see any speed-up\nor even that the threaded calculation might be slower because of\noverhead in setting up the parallelization and because the parallelized\nlinear algebra calculation involves more actual operations than when done\nserially.\n\n## Scenario 2: three different prediction methods on your data\n\n**Scenario**: You need to fit three different statistical/machine\nlearning models to your data.\n\nWhat are some options?\n\n-   use one core per model\n-   if you have rather more than three cores, apply the ideas here\n    combined with Scenario 1 above - with access to a cluster and\n    parallelized implementations of each model, you might use one node\n    per model\n\nHere we'll use the `processes` scheduler.\nIn principal given this relies on numpy code, we could have also used the `threads` scheduler,\nbut I'm not seeing effective parallelization when I try that. \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\nimport time\nimport numpy as np\n\ndef gen_and_mean(func, n, par1, par2):\n    return np.mean(func(par1, par2, size = n))\n\ndask.config.set(scheduler='processes', num_workers = 3, chunksize = 1)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe91b7ea1d0>\n```\n:::\n\n```{.python .cell-code}\nn = 100000000\nt0 = time.time()\ntasks = []\ntasks.append(dask.delayed(gen_and_mean)(np.random.normal, n, 0, 1))\ntasks.append(dask.delayed(gen_and_mean)(np.random.gamma, n, 1, 1))\ntasks.append(dask.delayed(gen_and_mean)(np.random.uniform, n, 0, 1))\nresults = dask.compute(tasks)\nprint(time.time() - t0) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.788240909576416\n```\n:::\n\n```{.python .cell-code}\nt0 = time.time()\np = gen_and_mean(np.random.normal, n, 0, 1)\nq = gen_and_mean(np.random.gamma, n, 1, 1)\ns = gen_and_mean(np.random.uniform, n, 0, 1)\nprint(time.time() - t0) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5.311657667160034\n```\n:::\n:::\n\n\n\nQuestion: Why might this not have shown a perfect three-fold speedup?\n\nIf we look at the delayed objects, we see that each one is a representation of the computation that needs to be\ndone and that execution happens lazily. Also note that `dask.compute` executes *synchronously*, which means\nthe main process waits until the `dask.compute` call is complete before allowing other commands to be run.\nThis synchronous evaluation is also called\na *blocking* call because execution of the task in the worker processes blocks the main process.\nIn contrast, if control returns to the user before the worker processes are done, that would be\n*asynchronous* evaluation (aka, a *non-blocking* call).\n\nYou could also have used tools like a parallel map here\nas well, as we'll discuss next.\n\nNote: the use of `chunksize = 1` forces Dask to immediately start one task on each worker. Without that argument, by default it groups tasks so as to reduce the overhead of starting each task individually, but when we have few tasks, that prevents effective parallelization.\n\n## Scenario 3: 10-fold CV and 10 or fewer cores\n\n**Scenario**: You are running a prediction method on 10 cross-validation\nfolds.\n\nThis illustrates the idea of running some number of tasks using the\ncores available on a single machine. \n\nHere I'll illustrate using a parallel map, using this simulated dataset and\nbasic use of `RandomForestRegressor()`.\n\nFirst, let's set up our fit function and simulate some data.\n\nIn this case our fit function uses global variables. The reason for this is that\nwe'll use Dask's `map` function, which allows us to pass only a single argument.\nWe could bundle the input data with the `fold_idx` value and pass as a larger\nobject, but here we'll stick with the simplicity of global variables.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\n\ndef cv_fit(fold_idx):\n    train_idx = folds != fold_idx\n    test_idx = folds == fold_idx\n    X_train = X.iloc[train_idx]\n    X_test = X.iloc[test_idx]\n    Y_train = Y[train_idx]\n    model = RandomForestRegressor()\n    model.fit(X_train, Y_train)\n    predictions = model.predict(X_test)\n    return predictions\n\n\nnp.random.seed(1)\n\n# Generate data\nn = 1000\np = 50\nX = pd.DataFrame(np.random.normal(size = (n, p)), columns=[f\"X{i}\" for i in range(1, p + 1)])\nY = X['X1'] + np.sqrt(np.abs(X['X2'] * X['X3'])) + X['X2'] - X['X3'] + np.random.normal(size = n)\n\nn_folds = 10\nseq = np.arange(n_folds)\nfolds = np.random.permutation(np.repeat(seq, 100))\n```\n:::\n\n\n\nTo do a parallel map, we need to use the distributed scheduler, but it's\nfine to do that with multiple cores on a single machine (such as a\nlaptop).\n\n\n\n\n::: {.cell hash='unit6-parallel_cache/html/unnamed-chunk-7_379418c366623a15372854c6b592470c'}\n\n```{.python .cell-code}\nn_cores = 2\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\ntasks = c.map(cv_fit, range(n_folds))\nresults = c.gather(tasks)\n# We'd need to sort the results appropriately to align them with the observations.\n```\n:::\n\n\n\nNow suppose you have 4 cores (and therefore won't have an equal number\nof tasks per core). The approach in the next scenario should work\nbetter.\n\n## Scenario 4: parallelizing over prediction methods\n\n**Scenario**: parallelizing over prediction methods or other cases where\nexecution time varies\n\nIf you need to parallelize over prediction methods or in other contexts\nin which the computation time for the different tasks varies widely, you\nwant to avoid having the parallelization tool group the tasks in\nadvance, because some cores may finish a lot more quickly than others.\nHowever, in some cases (such as the future package in R), this sort of grouping in advance (called\nprescheduling or 'static' allocation of tasks to workers) is the\ndefault, because it reduces overhead (from the latency involved in starting tasks).\n\nWhen using delayed, Dask starts up each delayed evaluation separately (i.e., dynamic allocation).\nThis is good for load-balancing, but each task induces some overhead (a few hundred microseconds).\nEven with a distributed map() it doesn’t appear possible to ask that the tasks be broken up into batches.\n\nSo if you have many quick tasks, you probably want to break them up into batches manually,\nto reduce the impact of the overhead.\n\n### Dynamic allocation\n\nWe’ll set up an artificial example with four slow tasks and 12 fast tasks and see the speed of running with the default of dynamic allocation under the distributed scheduler. Then we’ll compare to the worst-case scenario with all four slow tasks in a single batch.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport scipy.special\n\nn_cores = 4\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\n## 4 slow tasks and 12 fast ones.\nn = np.repeat([10**7, 10**5, 10**5, 10**5], 4)\n\ndef fun(i):\n    print(f\"Working on {i}.\")\n    return np.mean(scipy.special.loggamma(np.exp(np.random.normal(size = n[i]))))\n    \n\nt0 = time.time()\nout = fun(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWorking on 1.\n```\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nout = fun(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWorking on 5.\n```\n:::\n\n```{.python .cell-code}\nt0 = time.time()\ntasks = c.map(fun, range(len(n)))\nresults = c.gather(tasks)  \nprint(time.time() - t0)  # 0.81 sec.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.068439245223999\n```\n:::\n\n```{.python .cell-code}\ncluster.close()\n```\n:::\n\n\n\nNote that with relatively few tasks per core here, we could have gotten unlucky\nif the tasks were in a random order and multiple slow tasks happen to be done\nby a single worker.\n\n### Static allocation \n\nNext, note that by default the ‘processes’ scheduler sets up tasks in batches, with a default chunksize of 6. In this case that means that the first 4 (slow) tasks are all allocated to a single worker.\n\n[TODO: revisit this to check timing\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndask.config.set(scheduler='processes', num_workers = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe907c1c890>\n```\n:::\n\n```{.python .cell-code}\ndef myfun(idx):\n   return np.random.normal(size = n)\n\n\ntasks = []\np = len(n)\nfor i in range(p):\n    tasks.append(dask.delayed(fun)(i))  # add lazy task\n\nt0 = time.time()\nresults = dask.compute(tasks)  # compute all in parallel\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.264296531677246\n```\n:::\n:::\n\n\n\nWe could avoid that by setting `chunksize = 1` (as was shown in our original example of using the `processes` scheduler).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndask.config.set(scheduler='processes', num_workers = 4, chunksize = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe907454890>\n```\n:::\n\n```{.python .cell-code}\ntasks = []\np = len(n)\nfor i in range(p):\n    tasks.append(dask.delayed(fun)(i))  # add lazy task\n\nt0 = time.time()\nresults = dask.compute(tasks)  # compute all in parallel\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.1715400218963623\n```\n:::\n:::\n\n\n\n\n\n## Scenario 5: 10-fold CV across multiple methods with many more than 10 cores\n\n**Scenario**: You are running an ensemble prediction method such as\nSuperLearner or Bayesian model averaging on 10 cross-validation folds,\nwith many statistical/machine learning methods.\n\nHere you want to take advantage of all the cores you have available, so\nyou can't just parallelize over folds.\n\nFirst we'll discuss how to deal with the nestedness of the problem and\nthen we'll talk about how to make use of many cores across multiple\nnodes to parallelize over a large number of tasks.\n\n### Scenario 5A: nested parallelization\n\nOne can always flatten the looping, either in a for loop or in similar\nways when using apply-style statements.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## original code: multiple loops \nfor fold in range(n):\n  for method in range(M):\n     ### code here \n  \n## revised code: flatten the loops \nfor idx in range(n*M): \n    fold = idx // M \n    method = idx % M  \n    print(idx, fold, method)### code here \n```\n:::\n\n\n\nRather than flattening the loops at the loop level (which you'd need to do to use `map`), one could just\ngenerate a list of delayed tasks within the nested loops.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfor fold in range(n):\n  for method in range(M):\n     tasks.append(dask.delayed(myfun)(fold,method))\n```\n:::\n\n\n\n\nThe future package in R has some nice functionality for easily parallelizing with nested loops.\n\n\n### Scenario 5B: Parallelizing across multiple nodes\n\nIf you have access to multiple machines networked together, including a\nLinux cluster, you can use the tools in the future package across\nmultiple nodes (either in a nested parallelization situation with many\ntotal tasks or just when you have lots of unnested tasks to parallelize\nover). Here we'll just illustrate how to use multiple nodes, but if you\nhad a nested parallelization case you can combine the ideas just above\nwith the use of multiple nodes.\n\nSimply start Python as you usually would.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dask.distributed import Client, SSHCluster\n# First host is the scheduler.\ncluster = SSHCluster(\n    [\"gandalf.berkeley.edu\", \"radagast.berkeley.edu\", \"radagast.berkeley.edu\",\n    \"arwen.berkeley.edu\", \"arwen.berkeley.edu\"]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2023-09-08 15:52:01,654 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:01,656 - distributed.scheduler - INFO - State start\n2023-09-08 15:52:01,660 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:01,662 - distributed.scheduler - INFO -   Scheduler at: tcp://128.32.135.47:36063\n2023-09-08 15:52:06,765 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:06,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://128.32.135.101:37257'\n2023-09-08 15:52:06,766 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:06,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://128.32.135.101:43071'\n2023-09-08 15:52:06,973 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:06,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://128.32.135.115:44851'\n2023-09-08 15:52:06,973 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:06,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://128.32.135.115:38455'\n2023-09-08 15:52:08,493 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,498 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-56xfpey9', purging\n2023-09-08 15:52:08,493 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,499 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ztdgwdzw', purging\n2023-09-08 15:52:08,586 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,590 - distributed.worker - INFO -       Start worker at: tcp://128.32.135.101:36445\n2023-09-08 15:52:08,586 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,590 - distributed.worker - INFO -       Start worker at: tcp://128.32.135.101:33093\n2023-09-08 15:52:08,595 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,601 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7n0pdilr', purging\n2023-09-08 15:52:08,595 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,601 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e9ghtjq5', purging\n2023-09-08 15:52:08,667 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,671 - distributed.worker - INFO -       Start worker at: tcp://128.32.135.115:42149\n2023-09-08 15:52:08,667 - distributed.deploy.ssh - INFO - 2023-09-08 15:52:08,671 - distributed.worker - INFO -       Start worker at: tcp://128.32.135.115:34583\n```\n:::\n\n```{.python .cell-code}\nc = Client(cluster)\n\n## On the SCF, Savio and other clusters using the SLURM scheduler,\n## you can figure out the machine names like this, repeating the first machihe for the scheduler:\n## machines = subprocess.check_output(\"srun hostname\", shell = True,\n##            universal_newlines = True).strip().split('\\n')\n## machines = [machines[0]] + machines\n\ndef fun(i, n=10**6):\n    return np.mean(np.random.normal(size = n))\n\nn_tasks = 120\n\ntasks = c.map(fun, range(n_tasks))\nresults = c.gather(tasks)\n\n## And just to check we are actually using the various machines:\nimport subprocess\n\nc.gather(c.map(lambda x: subprocess.check_output(\"hostname\", shell = True), range(4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[b'radagast\\n', b'radagast\\n', b'arwen\\n', b'arwen\\n']\n```\n:::\n\n```{.python .cell-code}\ncluster.close()\n```\n:::\n\n\n\n\n## Scenario 6: Stratified analysis on a very large dataset\n\n**Scenario**: You are doing stratified analysis on a very large dataset\nand want to avoid unnecessary copies.\n\nIn many parallelization tools, if you try to parallelize this case on a single node, you end up making copies of the original dataset, which both takes up time and eats up memory.\n\nHere when we use the `processes` scheduler, we make copies for each task.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef do_analysis(i,x):\n    return np.mean(x)\n\nn_cores = 4\n\nx = np.random.normal(size = 5*10**7)   # our big \"dataset\"\n\ndask.config.set(scheduler='processes', num_workers = n_cores, chunksize = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe9058bda50>\n```\n:::\n\n```{.python .cell-code}\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n7.644139051437378\n```\n:::\n:::\n\n\n\n\nA better approach is to use the `distributed` scheduler (which is fine to use on a single machine or multiple machines), which makes one copy per worker instead of one per task, provided you apply `delayed()` to the global data object.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\nx = dask.delayed(x)\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/system/linux/mambaforge-3.11/lib/python3.11/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 47.69 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n```\n:::\n\n```{.python .cell-code}\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.3384032249450684\n```\n:::\n\n```{.python .cell-code}\ncluster.close()\n```\n:::\n\n\n\nThat seems to work, though Dask suggests sending the data to the workers in advance. I’m not sure of the distinction between what it is recommending and use of `dask.delayed(x)`.\n\nEven better would be to use the `threads` scheduler, in which case all workers can access the same data objects with no copying (but of course we cannot modify the data in that case without potentially causing problems for the other tasks). Without the copying, this is really fast.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef do_analysis(i,x):\n    print(id(x))\n    return np.mean(x)\n\n\ndask.config.set(scheduler='threads', num_workers = n_cores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dask.config.set object at 0x7fe9057b4890>\n```\n:::\n\n```{.python .cell-code}\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n140638798528112\n140638798528112\n140638798528112\n140638798528112\n140638798528112\n140638798528112\n140638798528112\n140638798528112\n```\n:::\n\n```{.python .cell-code}\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12806940078735352\n```\n:::\n:::\n\n\n\n\n\n## Scenario 7: Simulation study with n=1000 replicates: parallel random number generation\n\nWe’ll probably skip this for now and come back to it when we discuss random number generation in the Simulation Unit.\n\n\nThe key thing when thinking about random numbers in a parallel context\nis that you want to avoid having the same 'random' numbers occur on\nmultiple processes. On a computer, random numbers are not actually\nrandom but are generated as a sequence of pseudo-random numbers designed\nto mimic true random numbers. The sequence is finite (but very long) and\neventually repeats itself. When one sets a seed, one is choosing a\nposition in that sequence to start from. Subsequent random numbers are\nbased on that subsequence. All random numbers can be generated from one\nor more random uniform numbers, so we can just think about a sequence of\nvalues between 0 and 1.\n\n**Scenario**: You are running a simulation study with n=1000 replicates.\n\nEach replicate involves fitting two statistical/machine learning\nmethods.\n\nHere, unless you really have access to multiple hundreds of cores, you\nmight as well just parallelize across replicates.\n\nHowever, you need to think about random number generation. One option is to set the random number seed to different values for each replicate. One danger in setting the seed like that is that the random numbers in the different replicate could overlap somewhat. This is probably somewhat unlikely if you are not generating a huge number of random numbers, but it’s unclear how safe it is.\n\nWe can use functionality with numpy's PCG64 or MT19937 generators to be completely safe in our parallel random number generation. Each provide a `jumped()` function that moves the RNG ahead as if one had generated a very large number of random variables ($2^{128}) for the Mersenne Twister and nearly that for the PCG64).\n\nHere’s how we can set up the use of the PCG64 generator:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbitGen = np.random.PCG64(1)\nrng = np.random.Generator(bitGen)\nrng.random(size = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.51182162, 0.9504637 , 0.14415961])\n```\n:::\n:::\n\n\n\nNow let’s see how to jump forward. And then verify that jumping forward two increments is the same as making two separate jumps.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 1.23362391,  0.42793616, -1.90447637])\n```\n:::\n\n```{.python .cell-code}\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(2)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-0.31752967,  1.22269493,  0.28254622])\n```\n:::\n\n```{.python .cell-code}\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-0.31752967,  1.22269493,  0.28254622])\n```\n:::\n:::\n\n\n\nWe can also use `jumped()` with the Mersenne Twister.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbitGen = np.random.MT19937(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 0.12667829, -2.1031878 , -1.53950735])\n```\n:::\n:::\n\n\n\n\nSo the strategy to parallelize across tasks (or potentially workers if random number generation is done sequentially for tasks done by a single worker) is to give each task the same seed and use `jumped(i)` where `i` indexes the tasks (or workers).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef myrandomfun(i):\n    bitGen = np.random.PCG(1)\n    bitGen = bitGen.jumped(i)\n    # insert code with random number generation\n```\n:::\n\n\n\nOne caution is that it appears that the period for PCG64 is $2^{128}$ and that `jumped(1)` jumps forward by nearly that many random numbers. That seems quite strange, and I don’t understand it.\n\nAlternatively as [recommended in the docs](https://numpy.org/doc/stable/reference/random/bit_generators/pcg64.html):\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn_tasks = 10\nsg = np.random.SeedSequence(1)\nrngs = [Generator(PCG64(s)) for s in sg.spawn(n_tasks)]\n## Now pass elements of rng into your function that is being computed in parallel\n\ndef myrandomfun(rng):\n    # insert code with random number generation, such as:\n    z = rng.normal(size = 5)\n```\n:::\n\n\n\nIn R, the rlecuyer package deals with this. The L’Ecuyer algorithm has a period of $2^{191}$, which it divides into subsequences of length $2^{127}$.\n\n# 6. Additional details and topics (optional)\n\n## Avoiding repeated calculations by calling compute once\n\nAs far as I can tell, Dask avoids keeping all the pieces of a distributed object or computation in memory. However, in many cases this can mean repeating computations or re-reading data if you need to do multiple operations on a dataset.\n\nFor example, if you are create a Dask distributed dataset from data on disk, I think this means that every distinct set of computations (each computational graph) will involve reading the data from disk again.\n\nOne implication is that if you can include all computations on a large dataset within a single computational graph (i.e., a call to compute) that may be much more efficient than making separate calls.\n\nHere’s an example with Dask dataframe on the airline delay data, where we make sure to do all our computations as part of one graph:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler='processes', num_workers = 6)  \nimport dask.dataframe as ddf\nair = ddf.read_csv('/scratch/users/paciorek/243/AirlineData/csvs/*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1',   # (unexpected) latin1 value(s) 2001 file TailNum field\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object'})\n# specify dtypes so Pandas doesn't complain about column type heterogeneity\n\nimport time\nt0 = time.time()\nair.DepDelay.min().compute()   # about 200 seconds.\nprint(time.time()-t0)\nt0 = time.time()\nair.DepDelay.max().compute()   # about 200 seconds.\nprint(time.time()-t0)\nt0 = time.time()\n(mn, mx) = dask.compute(air.DepDelay.max(), air.DepDelay.min())  # about 200 seconds\nprint(time.time()-t0)\n```\n:::\n\n\n\n## Setting the number of threads (cores used) in threaded code (including parallel linear algebra in Python and R)\n\nIn general, threaded code will detect the number of cores available on a\nmachine and make use of them. However, you can also explicitly control\nthe number of threads available to a process.\n\nFor most threaded code (that based on the openMP protocol), the number\nof threads can be set by setting the OMP_NUM_THREADS environment\nvariable (VECLIB_MAXIMUM_THREADS on a Mac). E.g., to set it for four\nthreads in the bash shell:\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nexport OMP_NUM_THREADS=4\n```\n:::\n\n\n\nDo this before starting your R or Python session or before running your\ncompiled executable.\n\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g.,\nhere with R:\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nOMP_NUM_THREADS=4 R CMD BATCH --no-save job.R job.out\n```\n:::\n\n\n\n\n### Speed and threaded BLAS\n\nIn many cases, using multiple threads for linear algebra operations will\noutperform using a single thread, but there is no guarantee that this\nwill be the case, in particular for operations with small matrices and\nvectors. You can compare speeds by setting OMP_NUM_THREADS to different\nvalues. In cases where threaded linear algebra is slower than\nunthreaded, you would want to set OMP_NUM_THREADS to 1.\n\nMore generally, if you are using the parallel tools in Section 4 to\nsimultaneously carry out many independent calculations (tasks), it is\nlikely to be more effective to use the fixed number of cores available\non your machine so as to split up the tasks, one per core, without\ntaking advantage of the threaded BLAS (i.e., restricting each process to\na single thread).\n\n\n# 8. Introduction to R's future package (optional)\n\nBefore we illustrate implementation of various kinds of parallelization,\nI'll give an overview of the `future` package, which we'll use for many\nof the implementations. The future package has been developed over the\nlast few years and provides some nice functionality that is easier to\nuse and more cohesive than the various other approaches to\nparallelization in R.\n\nOther approaches include `parallel::parLapply`, `parallel::mclapply`,\nthe use of `foreach` without `future`, and the `partools` package.\nThe `partools` package is interesting. It tries to take the parts of\nSpark/Hadoop most relevant for statistics-related work -- a distributed\nfile system and distributed data objects -- and discard the parts that\nare a pain/not useful -- fault tolerance when using many, many\nnodes/machines.\n\n## Overview: Futures and the R future package\n\nWhat is a *future*? It's basically a flag used to tag a given operation\nsuch that when and where that operation is carried out is controlled at\na higher level. If there are multiple operations tagged then this allows\nfor parallelization across those operations.\n\nAccording to Henrik Bengtsson (the `future` package developer) and those\nwho developed the concept:\n\n-   a future is an abstraction for a value that will be available later\n-   the value is the result of an evaluated expression\n-   the state of a future is either unresolved or resolved\n\nWhy use futures? The `future` package allows one to write one's\ncomputational code without hard-coding whether or how parallelization\nwould be done. Instead one writes the code in a generic way and at the\nbeginning of one's code sets the 'plan' for how the parallel computation\nshould be done given the computational resources available. Simply\nchanging the 'plan' changes how parallelization is done for any given\nrun of the code.\n\nMore concisely, the key ideas are:\n\n-   Separate what to parallelize from how and where the parallelization\n    is actually carried out.\n-   Different users can run the same code on different computational\n    resources (without touching the actual code that does the\n    computation).\n\n## Overview of parallel backends\n\nOne uses `plan()` to control how parallelization is done, including what\nmachine(s) to use and how many cores on each machine to use.\n\nFor example,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multiprocess)\n## spreads work across multiple cores\n# alternatively, one can also control number of workers\nplan(multiprocess, workers = 4)\n```\n:::\n\n\n\nThis table gives an overview of the different plans.\n\n|       Type     |                 Description                |  Multi-node |   Copies of objects made?   |\n|  --------------| -------------------------------------------| ------------| ----------------------------|\n|   multisession |  uses additional R sessions as the workers |      no     |             yes |\n|    multicore   |   uses forked R processes as the workers   |      no     |  not if object not modified |\n|     cluster    |     uses R sessions on other machine(s)    |     yes     |             yes |\n\n## Accessing variables and workers in the worker processes\n\nThe future package usually does a good job of identifying the packages and (global) variables\nyou use in your parallelized code and loading those packages on the workers and copying necessary variables to the workers.\nIt uses the `globals` package to do this.\n\nHere's a toy example that shows that `n` and `MASS::geyser` are automatically available in the worker processes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(future)\nlibrary(future.apply)\n\nplan(multisession)\n\nlibrary(MASS)\nn <- nrow(geyser)\n\nmyfun <- function(idx) {\n   # geyser is in MASS package\n   return(sum(geyser$duration) / n)\n}\n\nfuture_sapply(1:5, myfun)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.460814 3.460814 3.460814 3.460814 3.460814\n```\n:::\n:::\n\n\n\nIn other contexts in R (or other languages) you may need to explicitly copy objects to the workers (or load packages on the workers). This is sometimes called *exporting* variables. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}