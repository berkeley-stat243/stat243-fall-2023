{
  "hash": "1c74d361d66568d19590dd592a22aa66",
  "result": {
    "markdown": "---\ntitle: \"Big data and databases\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-07-26\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n[PDF](./unit7-bigData.pdf){.btn .btn-primary}\n\nReferences:\n\n-   [Tutorial on parallel processing using Python's Dask and R's future packages](https://berkeley-scf.github.io/tutorial-dask-future)\n-   [Tutorial on working with large datasets in SQL, R, and\n    Python](https://berkeley-scf.github.io/tutorial-databases)\n-   Murrell: Introduction to Data Technologies\n-   Adler: R in a Nutshell\n-   [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)\n\nI've also pulled material from a variety of other sources, some\nmentioned in context below.\n\nNote that for a lot of the demo code I ran the code separately from rendering this document because of the time involved in working\nwith large datasets.\n\nWe'll focus on Dask and databases/SQL in this Unit. The material on using Spark is provided for reference, but you're not responsible for that material.\nIf you're interested in working with big datasets in R or with tools other than Dask in Python, there is [some material in the tutorial on working with large datasets](https://berkeley-scf.github.io/tutorial-databases/R-and-Python).\n\n\n# 1. A few preparatory notes\n\n## An editorial on 'big data'\n\n'Big data' was trendy these days, though I guess it's not quite the\nbuzzword/buzzphrase that it was a few years ago, given the AI/ML\nrevolution, but of course that revolution is largely based on having\nmassive datasets available online.\n\nPersonally, I think some of the hype around giant datasets is justified and some is hype.\nLarge datasets allow us to address questions that we can't with smaller\ndatasets, and they allow us to consider more sophisticated (e.g.,\nnonlinear) relationships than we might with a small dataset. But they do\nnot directly help with the problem of correlation not being causation.\nHaving medical data on every American still doesn't tell me if higher\nsalt intake causes hypertension. Internet transaction data does not tell\nme if one website feature causes increased viewership or sales. One\neither needs to carry out a designed experiment or think carefully about\nhow to infer causation from observational data. Nor does big data help\nwith the problem that an ad hoc 'sample' is not a statistical sample and\ndoes not provide the ability to directly infer properties of a\npopulation. Consider the immense difficulties we've seen in\nanswering questions about Covid despite large amounts of data, because it\nis incomplete/non-representative. A well-chosen smaller dataset may be much more informative\nthan a much larger, more ad hoc dataset. However, having big datasets\nmight allow you to select from the dataset in a way that helps get at\ncausation or in a way that allows you to construct a\npopulation-representative sample. Finally, having a big dataset also\nallows you to do a large number of statistical analyses and tests, so\nmultiple testing is a big issue. With enough analyses, something will\nlook interesting just by chance in the noise of the data, even if there\nis no underlying reality to it.\n\nDifferent people define the 'big' in big data differently. One\ndefinition involves the actual size of the data, and in some cases the\nspeed with which it is collected. Our efforts here will focus on dataset\nsizes that are large for traditional statistical work but would probably\nnot be thought of as large in some contexts such as Google or the US\nNational Security Agency (NSA). Another definition of 'big data' has\nmore to do with how pervasive data and empirical analyses backed by data\nare in society and not necessarily how large the actual dataset size is.\n\n## Logistics and data size\n\nOne of the main drawbacks with Python (and R) in working with big data is that all\nobjects are stored in memory, so you can't directly work with datasets\nthat are more than 1-20 Gb or so, depending on the memory on your\nmachine.\n\nThe techniques and tools discussed in this Unit (apart from the section\non MapReduce/Spark) are designed for datasets in the range of gigabytes\nto tens of gigabytes, though they may scale to larger if you have a\nmachine with a lot of memory or simply have enough disk space and are\nwilling to wait. If you have 10s of gigabytes of data, you'll be better\noff if your machine has 10s of GBs of memory, as discussed in this Unit.\n\nIf you're scaling to 100s of GBs, terabytes or petabytes, tools such as\ncarefully-administered databases, cloud-based tools such as provided by AWS and Google Cloud Platform,\nand Spark or other such tools are probably your best bet.\n\nNote: in handling big data files, it's best to have the data on the\nlocal disk of the machine you are using to reduce traffic and delays\nfrom moving data over the network.\n\n## What we already know about handling big data!\n\nUNIX operations are generally very fast, so if you can manipulate your\ndata via UNIX commands and piping, that will allow you to do a lot.\nWe've already seen UNIX commands for extracting columns. And various\ncommands such as `grep`, `head`, `tail`, etc. allow you to pick out rows\nbased on certain criteria. As some of you have done in problem sets, one\ncan use `awk` to extract rows. So basic shell scripting may allow you to\nreduce your data to a more manageable size.\n\nThe tool [GNU parallel](https://docs-research-it.berkeley.edu/services/high-performance-computing/user-guide/running-your-jobs/gnu-parallel/)\nallows you to parallelize operations from the command line and is\ncommonly used in working on Linux clusters.\n\nAnd don't forget simple things. If you have a dataset with 30 columns\nthat takes up 10 Gb but you only need 5 of the columns, get rid of the\nrest and work with the smaller dataset. Or you might be able to get the\nsame information from a random sample of your large dataset as you would\nfrom doing the analysis on the full dataset. Strategies like this will\noften allow you to stick with the tools you already know.\n\nAlso, remember that we can often store data more compactly in binary\nformats than in flat text (e.g., csv) files.\n\nFinally, for many applications, storing large datasets in a standard\ndatabase will work well. \n\n# 2. MapReduce, Dask, Hadoop, and Spark\n\nTraditionally, high-performance computing (HPC) has concentrated on\ntechniques and tools for message passing such as MPI and on developing\nefficient algorithms to use these techniques. In the last 20 years,\nfocus has shifted to technologies for processing large datasets that are\ndistributed across multiple machines but can be manipulated as if they\nare one dataset.\n\nTwo commonly-used tools for doing this are Spark and Python's Dask\npackage. We'll cover Dask.\n\n## Overview\n\nA basic paradigm for working with big datasets is the *MapReduce*\nparadigm. The basic idea is to store the data in a distributed fashion\nacross multiple nodes and try to do the computation in pieces on the\ndata on each node. Results can also be stored in a distributed fashion.\n\nA key benefit of this is that if you can't fit your dataset on disk on\none machine you can on a cluster of machines. And your processing of the\ndataset can happen in parallel. This is the basic idea of *MapReduce*.\n\nThe basic steps of *MapReduce* are as follows:\n\n-   read individual data objects (e.g., records/lines from CSVs or\n    individual data files)\n-   *map*: create key-value pairs using the inputs (more formally, the map\n    step takes a key-value pair and returns a new key-value pair)\n-   *reduce*: for each key, do an operation on the associated values and\n    create a result - i.e., aggregate within the values assigned to each\n    key\n-   write out the {key,result} pair\n\nA similar paradigm that is implemented in *dplyr* is the\n[split-apply-combine strategy](http://www.jstatsoft.org/v40/i01/paper),\ndiscussed a bit in Unit 5.\n\nA few additional comments. In our map function, we could exclude values\nor transform them in some way, including producing multiple records from\na single record. And in our reduce function, we can do more complicated\nanalysis. So one can actually do fairly sophisticated things within what\nmay seem like a restrictive paradigm. But we are constrained such that\nin the map step, each record needs to be treated independently and in\nthe reduce step each key needs to be treated independently. This allows\nfor the parallelization.\n\n**One important note is that any operations that require moving a lot of\ndata between the workers can take a long time.** (This is sometimes\ncalled a *shuffle*.) This could happen if, for example, you computed the\nmedian value within each of many groups if the data for each group are\nspread across the workers. In contrast, if we compute the mean or sum,\none can compute the partial sums on each worker and then just add up the\npartial sums.\n\nNote that as discussed in Unit 5 the concepts of *map* and *reduce* are core concepts in\nfunctional programming, and the various `apply` and `lapply` style commands are base R's\nversion of a map operation.\n\n*Hadoop* is an infrastructure for enabling MapReduce across a network of\nmachines. The basic idea is to hide the complexity of distributing the\ncalculations and collecting results. Hadoop includes a file system for\ndistributed storage (HDFS), where each piece of information is stored\nredundantly (on multiple machines). Calculations can then be done in a\nparallel fashion, often on data in place on each machine thereby\nlimiting the amount of communication that has to be done over the\nnetwork. Hadoop also monitors completion of tasks and if a node fails,\nit will redo the relevant tasks on another node. Hadoop is based on\nJava. Given the popularity of Spark, I'm not sure how much usage these\napproaches currently see. Setting up a Hadoop cluster can be tricky.\nHopefully if you're in a position to need to use Hadoop, it will be set\nup for you and you will be interacting with it as a user/data analyst.\n\nOk, so what is Spark? You can think of Spark as in-memory Hadoop. Spark\nallows one to treat the memory across multiple nodes as a big pool of\nmemory. Therefore, Spark should be faster than Hadoop when the data\nwill fit in the collective memory of multiple nodes. In cases where it\ndoes not, Spark will make use of the HDFS (and generally, Spark will be\nreading the data initially from HDFS.) While Spark is more user-friendly\nthan Hadoop, there are also some things that can make it hard to use.\nSetting up a Spark cluster also involves a bit of work, Spark can be\nhard to configure for optimal performance, and Spark calculations have a\ntendency to fail (often involving memory issues) in ways that are hard\nfor users to debug.\n\n## Using Dask for big data processing\n\nUnit 6 on parallelization gives an overview of using Dask in similar\n for flexible parallelization\non different kinds of computational resources (in particular,\nparallelizing across multiple cores on one machine versus parallelizing\nacross multiple cores across multiple machines/ndoes).\n\nHere we'll see the use of Dask to work with distributed datasets. Dask\ncan process datasets (potentially very large ones) by parallelizing\noperations across subsets of the data using multiple cores on one or\nmore machines.\n\nLike Spark, Dask automatically reads data from files in parallel and\noperates on *chunks* (also called *partitions* or *shards*) of the full\ndataset in parallel. There are two big advantages of this:\n\n-   You can do calculations (including reading from disk) in parallel\n    because each worker will work on a piece of the data.\n-   When the data is split across machines, you can use the memory of\n    multiple machines to handle much larger datasets than would be\n    possible in memory on one machine. That said, Dask processes the\n    data in chunks, so one often doesn't need a lot of memory, even just\n    on one machine.\n\nWhile reading from disk in parallel is a good goal, if all the data are\non one hard drive, there are limitations on the speed of reading the\ndata from disk because of having multiple processes all trying to access\nthe disk at once. Supercomputing systems will generally have parallel\nfile systems that support truly parallel reading (and writing, i.e.,\n*parallel I/O*). Hadoop/Spark deal with this by distributing across\nmultiple disks, generally one disk per machine/node.\n\nBecause computations are done in external compiled code (e.g., via\n`numpy`) it's effective to use the `threads` scheduler when operating on\none node to avoid having to copy and move the data.\n\n### Dask dataframes (pandas)\n\nDask dataframes are Pandas-like dataframes where each dataframe is split\ninto groups of rows, stored as smaller Pandas dataframes.\n\nOne can do a lot of the kinds of computations that you would do on a\nPandas dataframe on a Dask dataframe, but many operations are not\npossible. See [here](http://docs.dask.org/en/latest/dataframe-api.html).\n\nBy default dataframes are handled by the `threads` scheduler.\n(Recall we discussed Dask's various schedulers in Unit 6.)\n\nHere's an example of reading from a dataset of flight delays (about 11\nGB data). You can get the data\n[here](https://www.stat.berkeley.edu/share/paciorek/1987-2008.csvs.tgz).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.dataframe as ddf\npath = '/scratch/users/paciorek/243/AirlineData/csvs/'\nair = ddf.read_csv(path + '*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1', # (unexpected) latin1 value(s) in TailNum field in 2001\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object'})\n# specify dtypes so Pandas doesn't complain about column type heterogeneity\nair\n```\n:::\n\n\n\nDask will reads the data in parallel from the various .csv.bz2 files\n(unzipping on the fly), but note the caveat in the previous section\nabout the possibilities for truly parallel I/O.\n\nHowever, recall that Dask uses delayed evaluation. In this case, the reading is delayed until\n`compute()` is called. For that matter, the various other calculations\n(`max`, `groupby`, `mean`) shown below are only done after `compute()` is called.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nair.DepDelay.max().compute()   # this takes a while\nsub = air[(air.UniqueCarrier == 'UA') & (air.Origin == 'SFO')]\nbyDest = sub.groupby('Dest').DepDelay.mean()\nbyDest.compute()               # this takes a while too\n```\n:::\n\n\n\nYou should see this:\n\n```\n    Dest \n    ACV 26.200000 \n    BFL 1.000000 \n    BOI 12.855069 \n    BOS 9.316795 \n    CLE 4.000000\n    ...\n```\n\nNote: calling compute twice is a bad idea as Dask will read in the data\ntwice - more on this in a bit.\n\n### Dask bags\n\nBags are like lists but there is no particular ordering, so it doesn't\nmake sense to ask for the i'th element.\n\nYou can think of operations on Dask bags as being like parallel map\noperations on lists in Python or R.\n\nBy default bags are handled via the `multiprocessing` scheduler.\n\nLet's see some basic operations on a large dataset of Wikipedia log\nfiles. You can get a subset of the Wikipedia data\n[here](https://www.stat.berkeley.edu/share/paciorek/wikistats_example.tar.gz).\n\nHere we again read the data in (which Dask will do in parallel):\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \nimport dask.bag as db\n## This is the full data\n## path = '/scratch/users/paciorek/wikistats/dated_2017/'\n## For demo we'll just use a small subset\npath = '/scratch/users/paciorek/wikistats/dated_2017_small/dated/'\nwiki = db.read_text(path + 'part-0000*gz')\n```\n:::\n\n\n\nHere we'll just count the number of records.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nt0 = time.time()\nwiki.count().compute()\ntime.time() - t0   # 136 sec. for full data\n```\n:::\n\n\n\nAnd here is a more realistic example of filtering (subsetting).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\ndef find(line, regex = 'Armenia'):\n    vals = line.split(' ')\n    if len(vals) < 6:\n        return(False)\n    tmp = re.search(regex, vals[3])\n    if tmp is None:\n        return(False)\n    else:\n        return(True)\n    \n\nwiki.filter(find).count().compute()\narmenia = wiki.filter(find)\nsmp = armenia.take(100) ## grab a handful as proof of concept\nsmp[0:5]\n```\n:::\n\n\n\nNote that it is quite inefficient to do the `find()` (and implicitly\nreading the data in) and then compute on top of that intermediate result\nin two separate calls to `compute()`. Rather, we should set up the code\nso that all the operations are set up before a single call to\n`compute()`. More on this the [Dask/future tutorial](https://berkeley-scf.github.io/tutorial-dask-future/python-dask#63-avoid-repeated-calculations-by-embedding-tasks-within-one-call-to-compute)\n\nSince the data are just treated as raw strings, we might want to\nintroduce structure by converting each line to a tuple and then\nconverting to a data frame.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef make_tuple(line):\n    return(tuple(line.split(' ')))\n\ndtypes = {'date': 'object', 'time': 'object', 'language': 'object',\n'webpage': 'object', 'hits': 'float64', 'size': 'float64'}\n\n## Let's create a Dask dataframe. \n## This will take a while if done on full data.\ndf = armenia.map(make_tuple).to_dataframe(dtypes)\ntype(df)\n\n## Now let's actually do the computation, returning a Pandas df\nresult = df.compute()  \ntype(result)\nresult[0:5]\n```\n:::\n\n\n\n### Dask arrays (numpy)\n\nDask arrays are numpy-like arrays where each array is split up by both\nrows and columns into smaller numpy arrays.\n\nOne can do a lot of the kinds of computations that you would do on a\nnumpy array on a Dask array, but many operations are not possible. See\n[here](http://docs.dask.org/en/latest/array-api.html).\n\nBy default arrays are handled via the `threads` scheduler.\n\n#### Non-distributed arrays\n\nLet's first see operations on a single node, using a single 13 GB two-dimensional\narray. Again, Dask uses lazy evaluation, so creation of the array\ndoesn't happen until an operation requiring output is done.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler = 'threads', num_workers = 4) \nimport dask.array as da\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(10000, 10000))\n# square 10k x 10k chunks\nmycalc = da.mean(x, axis = 1)  # by row\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0  # 41 sec.\n```\n:::\n\n\n\nFor a row-based operation, we would presumably only want to chunk things\nup by row, but this doesn't seem to actually make a difference,\npresumably because the mean calculation can be done in pieces and only a\nsmall number of summary statistics moved between workers.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\n# x = da.from_array(x, chunks=(2500, 40000))  # adjust chunk size of existing array\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(2500, 40000))\nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 42 sec.\n```\n:::\n\n\n\nOf course, given the lazy evaluation, this timing comparison is not just\ntiming the actual row mean calculations.\n\nBut this doesn't really clarify the story...\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nimport numpy as np\nimport time\nt0 = time.time()\nx = np.random.normal(0, 1, size=(40000,40000))\ntime.time() - t0   # 110 sec.\n# for some reason the from_array and da.mean calculations are not done lazily here\nt0 = time.time()\ndx = da.from_array(x, chunks=(2500, 40000))\ntime.time() - t0   # 27 sec.\nt0 = time.time()\nmycalc = da.mean(x, axis = 1)  # what is this doing given .compute() also takes time?\ntime.time() - t0   # 28 sec.\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 21 sec.\n```\n:::\n\n\n\nDask will avoid storing all the chunks in memory. (It appears to just\ngenerate them on the fly.) Here we have an 80 GB array but we never use\nmore than a few GB of memory (based on `top` or `free -h`).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nx = da.random.normal(0, 1, size=(100000,100000), chunks=(10000, 10000))\nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 205 sec.\nrs[0:5]\n```\n:::\n\n\n\n#### Distributed arrays\n\nUsing arrays distributed across multiple machines should be straightforward based on using *Dask distributed*. However,\none would want to be careful about creating arrays by distributing the\ndata from a single Python process as that would involve copying between\nmachines.\n\n\n# 3. Databases\n\nThis material is drawn from the tutorial on [Working with large datasets\nin SQL, R, and\nPython](https://berkeley-scf.github.io/tutorial-databases), though I\nwon't hold you responsible for all of the database/SQL material in that\ntutorial, only what appears here in this Unit.\n\n## Overview\n\nBasically, standard SQL databases are *relational* databases that are a\ncollection of rectangular format datasets (*tables*, also called\n*relations*), with each table similar to R or Pandas data frames, in\nthat a table is made up of columns, which are called *fields* or\n*attributes*, each containing a single *type* (numeric, character, date,\ncurrency, enumerated (i.e., categorical), ...) and rows or records\ncontaining the observations for one entity. Some of the tables in a\ngiven database will generally have fields in common so it makes sense to\nmerge (i.e., join) information from multiple tables. E.g., you might\nhave a database with a table of student information, a table of teacher\ninformation and a table of school information, and you might join\nstudent information with information about the teacher(s) who taught the\nstudents. Databases are set up to allow for fast querying and merging\n(called joins in database terminology).\n\nFormally, databases are stored on disk, while Python and R store\ndatasets in memory. This would suggest that databases will be slow to\naccess their data but will be able to store more data than can be loaded\ninto an Python or R session. However, databases can be quite fast due in\npart to disk caching by the operating system as well as careful\nimplementation of good algorithms for database operations. For more\ninformation about disk caching see the tutorial.\n\n## Interacting with a database\n\nYou can interact with databases in a variety of database systems\n(*DBMS*=database management system). Some popular systems are SQLite,\nMySQL, PostgreSQL, Oracle and Microsoft Access. We'll concentrate on\naccessing data in a database rather than management of databases. SQL is\nthe Structured Query Language and is a special-purpose high-level\nlanguage for managing databases and making queries. Variations on SQL\nare used in many different DBMS.\n\nQueries are the way that the user gets information (often simply subsets\nof tables or information merged across tables). The result of an SQL\nquery is in general another table, though in some cases it might have\nonly one row and/or one column.\n\nMany DBMS have a client-server model. Clients connect to the server,\nwith some authentication, and make requests (i.e., queries).\n\nThere are often multiple ways to interact with a DBMS, including\ndirectly using command line tools provided by the DBMS or via Python or\nR, among others.\n\nWe'll concentrate on SQLite (because it is simple to use on a single\nmachine). SQLite is quite nice in terms of being self-contained - there\nis no server-client model, just a single file on your hard drive that\nstores the database and to which you can connect to using the SQLite\nshell, R, Python, etc. However, it does not have some useful\nfunctionality that other DBMS have. For example, you can't use `ALTER\nTABLE` to modify column types or drop columns.\n\n## Database schema and normalization\n\nTo truly leverage the conceptual and computational power of a database\nyou'll want to have your data in a normalized form, which means\nspreading your data across multiple tables in such a way that you don't\nrepeat information unnecessarily.\n\nThe *schema* is the metadata about the tables in the database and the\nfields (and their types) in those tables.\n\nLet's consider this using an educational example. Suppose we have a\nschool with multiple teachers teaching multiple classes and multiple\nstudents taking multiple classes. If we put this all in one table\norganized per student, the data might have the following fields:\n\n-   student ID\n-   student grade level\n-   student name\n-   class 1\n-   class 2\n-    ...\n-   class n\n-   grade in class 1\n-   grade in class 2\n-    ...\n-   grade in class n\n-   teacher ID 1\n-   teacher ID 2\n-    ...\n-   teacher ID n\n-   teacher name 1\n-   teacher name 2\n-    ...\n-   teacher name n\n-   teacher department 1\n-   teacher department 2\n-    ...\n-   teacher department n\n-   teacher age 1\n-   teacher age 2\n-    ...\n-   teacher age n\n\nThere are a lot of problems with this:\n\n1.  A lot of information is repeated across rows (e.g., teacher age for students who have the same teacher)\n    - this is a waste of space\n    - it is hard/error-prone to update values in the database (e.g., after a teacher's birthday), because a given value needs to be updated in multiple places\n2.  There are potentially a lot of empty cells (e.g., for a student who takes fewer than 'n' classes). This will generally result in a waste of space.\n3.  It's hard to see the information that is not organized uniquely by row -- i.e., it's much easier to understand the information at the student level than the teacher level\n4.  We have to know in advance how big 'n' is. Then if a single student takes more than 'n' classes, the whole database needs to be restructured.\n\nIt would get even worse if there was a field related to teachers for\nwhich a given teacher could have multiple values (e.g., teachers could\nbe in multiple departments). This would lead to even more redundancy -\neach student-class-teacher combination would be crossed with all of the\ndepartments for the teacher (so-called multivalued dependency in\ndatabase theory).\n\nAn alternative organization of the data would be to have each row\nrepresent the enrollment of a student in a class.\n\n-   student ID\n-   student name\n-   class\n-   grade in class\n-   student grade level\n-   teacher ID\n-   teacher department\n-   teacher age\n\nThis has some advantages relative to our original organization in terms\nof not having empty cells, but it doesn't solve the other three\nissues above.\n\nInstead, a natural way to order this database is with the following\nfour tables.\n\n-   Student\n    -   ID\n    -   name\n    -   grade_level\n\n-   Teacher\n    -   ID\n    -   name\n    -   department\n    -   age\n\n-   Class\n    -   ID\n    -   topic\n    -   class_size\n    -   teacher_ID\n\n-   ClassAssignment\n    -   student_ID\n    -   class_ID\n    -   grade\n\nThe `ClassAssignment` table has one row per student-class pair. Having a table like\nthis handles \"ragged\" data where the number of observations per unit (in this case\nclasses per student) varies. Using such tables is a common pattern when considering how to normalize a database.\nIt's also a core part of the idea of \"tidy data\" and data in *long* format, seen in the `tidyr` package.\n\nThen we do queries to pull information from multiple tables. We do the\njoins based on *keys*, which are the fields in each table that allow us\nto match rows from different tables.\n\n(That said, if all anticipated uses of a database will end up\nrecombining the same set of tables, we may want to have a denormalized\nschema in which those tables are actually combined in the database. It\nis possible to be too pure about normalization! We can also create a\nvirtual table, called a *view*, as discussed later.)\n\n### Keys\n\nA *key* is a field or collection of fields that give(s) a unique value\nfor every row/observation. A table in a database should then have a\n*primary key* that is the main unique identifier used by the DBMS.\n*Foreign keys* are columns in one table that give the value of the\nprimary key in another table. When information from multiple tables is\njoined together, the matching of a row from one table to a row in\nanother table is generally done by equating the primary key in one table\nwith a foreign key in a different table.\n\nIn our educational example, the primary keys would presumably be:\n`Student.ID`, `Teacher.ID`, `Class.ID`, and for ClassAssignment a primary key\nmade of two\nfields: `{ClassAssignment.studentID, ClassAssignment.class_ID}`.\n\nSome examples of foreign keys would be:\n\n-   `student_ID` as the foreign key in `ClassAssignment` for joining with\n    `Student` on `Student.ID`\n\n-   `teacher_ID` as the foreign key in `Class` for joining with `Teacher`\n    based on `Teacher.ID`\n\n-   `class_ID` as the foreign key in `ClassAssignment` for joining with\n    `Class` based on `Class.ID`\n\n### Queries that join data across multiple tables\n\nSuppose we want a result that has the grades of all students in 9th\ngrade. For this we need information from the `Student` table (to determine\ngrade level) and information from the `ClassAssignment` table (to\ndetermine the class grade). More specifically we need a query that:\n\n - joins `Student` with `ClassAssignment` based on matching rows in `Student` with\n rows in `ClassAssignment` where `Student.ID` is the same as `ClassAssignment.student_ID` and\n - filters the rows based on `Student.grade_level`:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT Student.ID, grade FROM Student, ClassAssignment WHERE \n  Student.ID = ClassAssignment.student_ID and Student.grade_level = 9;\n```\n:::\n\n\nNote that the query is a *join* (specifically an *inner join*), which is\nlike `merge()` (or `dplyr::join`) in R. We don't specifically use the JOIN keyword, but one\ncould do these queries explicitly using JOIN, as we'll see later.\n\n## Stack Overflow metadata example\n\nI've obtained data from [Stack Overflow](https://stackoverflow.com), the\npopular website for asking coding questions, and placed it into a\nnormalized database. The SQLite version has metadata (i.e., it lacks the\nactual text of the questions and answers) on all of the questions and\nanswers posted in 2016.\n\nWe'll explore SQL functionality using this example database.\n\nNow let's consider the Stack Overflow data. Each question may have\nmultiple answers and each question may have multiple (topic) tags.\n\nIf we tried to put this into a single table, the fields could look like\nthis if we have one row per question:\n\n-   question ID\n-   ID of user submitting question\n-   question title\n-   tag 1\n-   tag 2\n-    ...\n-   tag n\n-   answer 1 ID\n-   ID of user submitting answer 1\n-   age of user submitting answer 1\n-   name of user submitting answer 1\n-   answer 2 ID\n-   ID of user submitting answer 2\n-   age of user submitting answer 2\n-   name of user submitting answer 2\n-   ...\n\nor like this if we have one row per question-answer pair:\n\n-   question ID\n-   ID of user submitting question\n-   question title\n-   tag 1\n-   tag 2\n-    ...\n-   tag n\n-   answer ID\n-   ID of user submitting answer\n-   age of user submitting answer\n-   name of user submitting answer\n\nAs we've discussed neither of those schema is particularly desirable.\n\n**Challenge**: How would you devise a schema to normalize the data.\nI.e., what set of tables do you think we should create?\n\nYou can view [one reasonable schema](normalized_example.png).\nThe lines between tables indicate the relationship of foreign keys in\none table to primary keys in another table. The schema in the actual\ndatabase of Stack Overflow data we'll use in the examples here is similar\nto but not identical to that.\n\nYou can download a [copy of the SQLite version of the Stack Overflow 2016\ndatabase](http://www.stat.berkeley.edu/share/paciorek/stackoverflow-2016.db).\n\n## Accessing databases in Python\n\nPython provides a variety of front-end packages for manipulating databases from a\nvariety of DBMS (SQLite, DuckDB, MySQL, PostgreSQL, among others). Basically,\nyou start with a bit of code that links to\nthe actual database, and then you can easily query the database using SQL\nsyntax regardless of the back-end. The Python function calls that wrap\naround the SQL syntax will also look the same regardless of the back-end\n(basically `execute(\"SOME SQL STATEMENT\")`).\n\n[TODO: look into SQLAlchemy]\n\nWith SQLite, Python processes make calls against the stand-alone SQLite\ndatabase (.db) file, so there are no SQLite-specific processes. With a\nclient-server DBMS like PostgreSQL, Python processes call out to separate\nPostgres processes; these are started from the overall Postgres\nbackground process\n\nYou can access and navigate an SQLite database from Python as follows.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sqlite3 as sq\ndir_path = '../data'  # Replace with the actual path\ndb_filename = 'stackoverflow-2016.db'\n\ncon = sq.connect(os.path.join(dir_path, db_filename))\ndb = con.cursor()\ndb.execute(\"select * from questions limit 5\")  # simple query \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<sqlite3.Cursor object at 0x7fdc254c4d40>\n```\n:::\n\n```{.python .cell-code}\ndb.fetchall() # retrieve results\n\n## http://www.stat.berkeley.edu/share/paciorek/stackoverflow-2016.db\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(34552550, '2016-01-01 00:00:03', 0, 108, 'Scope between methods', 5684416), (34552551, '2016-01-01 00:00:07', 1, 151, 'Rails - Unknown Attribute - Unable to add a new field to a form on create/update', 2457617), (34552552, '2016-01-01 00:00:39', 2, 1942, \"Selenium Firefox webdriver won't load a blank page after changing Firefox preferences\", 5732525), (34552554, '2016-01-01 00:00:50', 0, 153, 'Android Studio styles.xml Error', 5735112), (34552555, '2016-01-01 00:00:51', -1, 54, 'Java: reference to non-finial local variables inside a thread', 4646288)]\n```\n:::\n:::\n\n\n\nWe can (fairly) easily see the tables  (this is easier from R):\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef db_list_tables(db):\n    db.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = db.fetchall()\n    return [table[0] for table in tables]\n\ndb_list_tables(db)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['questions', 'answers', 'questions_tags', 'users']\n```\n:::\n:::\n\n\nand the fields in table, presuming you've just queried the table:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndb.execute(\"select * from questions limit 5\")\n# If you've already queried a table, see `description` for the fields:\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<sqlite3.Cursor object at 0x7fdc254c4d40>\n```\n:::\n\n```{.python .cell-code}\n[item[0] for item in db.description]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['questionid', 'creationdate', 'score', 'viewcount', 'title', 'ownerid']\n```\n:::\n:::\n\n\n\nHere's how to make a basic SQL query. One can either make the query and\nget the results in one go or make the query and separately fetch the\nresults. Here we've selected the first five rows (and all columns, based\non the \\* wildcard) and brought them into Python as list of tuples.\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-15_2e4a0a21bc0986772cdbd9cfab44109d'}\n\n```{.python .cell-code}\nresults = db.execute(\"select * from questions limit 5\").fetchall()  # simple query \ntype(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'list'>\n```\n:::\n\n```{.python .cell-code}\ntype(results[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'tuple'>\n```\n:::\n\n```{.python .cell-code}\nquery = db.execute(\"select * from questions\")  # simple query \nresults2 = query.fetchmany(5)\nresults == results2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n:::\n\n\n\nTo disconnect from the database:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndb.close()\n```\n:::\n\n\n\n## Basic SQL for choosing rows and columns from a table\n\nSQL is a declarative language that tells the database system what\nresults you want. The system then parses the SQL syntax and determines\nhow to implement the query.\n\n> **Note**: An *imperative* language is one where you provide the sequence of commands you want to be run, in order. A *declarative* language is one where you declare what result you want and rely on the system that interprets the commands how to actually do it. Most of the languages we're generally familiar with are imperative. (That said, even in languages like Python, function calls in many ways simply say what we want rather than exactly how the computer should carry out the granular operations.)\n\nHere are some examples using the Stack Overflow database.\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-17_a18e30133c2f1f8a88b97f4913903b07'}\n\n```{.python .cell-code}\n## find the largest viewcounts in the questions table\ndb.execute(\n'select title, viewcount from questions order by viewcount desc limit 10').fetchall()\n## now get the questions that are viewed the most\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('How to solve \"server DNS address could not be found\" error in windows 10?', 196469), (\"Code signing is required for product type 'Application' in SDK 'iOS 10.0' - StickerPackExtension requires a development team error\", 174790), ('\"Gradle Version 2.10 is required.\" Error', 134399), (\"Android- Error:Execution failed for task ':app:transformClassesWithDexForRelease'\", 129874), ('Fatal error: Uncaught Error: Call to undefined function mysql_connect()', 129624), ('Unsupported major.minor version 52.0 in my app', 127764), (\"Response to preflight request doesn't pass access control check AngularJs\", 126752), ('NPM vs. Bower vs. Browserify vs. Gulp vs. Grunt vs. Webpack', 112000), ('Git refusing to merge unrelated histories', 109422), ('\"SyntaxError: Unexpected token < in JSON at position 0\" in React App', 106995)]\n```\n:::\n\n```{.python .cell-code}\ndb.execute('select * from questions where viewcount > 100000').fetchall()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(35429801, '2016-02-16 10:21:09', 400, 100125, 'This action could not be completed. Try Again (-22421)', 5881764), (37280274, '2016-05-17 15:21:49', 23, 106995, '\"SyntaxError: Unexpected token < in JSON at position 0\" in React App', 4043633), (37937984, '2016-06-21 07:23:00', 202, 109422, 'Git refusing to merge unrelated histories', 2670370), (35062852, '2016-01-28 13:28:39', 730, 112000, 'NPM vs. Bower vs. Browserify vs. Gulp vs. Grunt vs. Webpack', 2761509), (35588699, '2016-02-23 21:37:06', 57, 126752, \"Response to preflight request doesn't pass access control check AngularJs\", 2896963), (35990995, '2016-03-14 15:01:17', 104, 127764, 'Unsupported major.minor version 52.0 in my app', 1629278), (34579099, '2016-01-03 16:55:16', 8, 129624, 'Fatal error: Uncaught Error: Call to undefined function mysql_connect()', 3656666), (35890257, '2016-03-09 11:25:05', 51, 129874, \"Android- Error:Execution failed for task ':app:transformClassesWithDexForRelease'\", 1118886), (34814368, '2016-01-15 15:24:36', 206, 134399, '\"Gradle Version 2.10 is required.\" Error', 3319176), (37806538, '2016-06-14 08:16:21', 223, 174790, \"Code signing is required for product type 'Application' in SDK 'iOS 10.0' - StickerPackExtension requires a development team error\", 1554347), (36668374, '2016-04-16 18:57:19', 20, 196469, 'How to solve \"server DNS address could not be found\" error in windows 10?', 1707976)]\n```\n:::\n:::\n\n\n\nLet's lay out the various verbs in SQL. Here's the form of a standard\nquery (though the ORDER BY is often omitted and sorting is\ncomputationally expensive):\n\n```\nSELECT <column(s)> FROM <table> WHERE <condition(s) on column(s)> ORDER BY <column(s)>\n```\n\nSQL keywords are often written in ALL CAPITALS, although I won't\nnecessarily do that here.\n\nAnd here is a table of some important keywords:\n\n|            Keyword          |                         Usage  |\n|  ----------------------------| ----------------------------------------------------|\n|             SELECT           |                    select columns   |\n|              FROM            |              which table to operate on   |\n|             WHERE            |  filter (choose) rows satisfying certain conditions   |\n|   LIKE, IN, \\<, \\>, ==, etc. |              used as part of conditions   |\n|            ORDER BY          |                sort based on columns   |\n\n\nFor comparisons in a `WHERE` clause, some common syntax for setting\nconditions includes `LIKE` (for patterns), `=`, `>`, `<`, `>=`, `<=`, `!=`.\n\nSome other keywords are: `DISTINCT`, `ON`, `JOIN`, `GROUP BY`, `AS`, `USING`, `UNION`,\n`INTERSECT`, `SIMILAR TO`.\n\n**Question**: how would we find the oldest users in the database?\n\n## Grouping / stratifying\n\nA common pattern of operation is to stratify the dataset, i.e., collect\nit into mutually exclusive and exhaustive subsets. One would then\ngenerally do some operation on each subset. In SQL this is done with the\nGROUP BY keyword.\n\nHere's a basic example where we count the occurrences of different tags.\nNote that we use `as` to define a name for the new column that is\ncreated based on the aggregation operation (`count` in this case).\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-18_9a24a15bc3f3274d48f0e08da974c009'}\n\n```{.python .cell-code}\ndb.execute(\"select tag, count(*) as n from questions_tags \\\n           group by tag order by n desc limit 25\").fetchall()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('javascript', 290966), ('java', 219155), ('android', 184272), ('php', 177969), ('python', 171745), ('c#', 163637), ('html', 126851), ('jquery', 123707), ('ios', 95722), ('css', 86470), ('angularjs', 76951), ('c++', 76260), ('mysql', 75458), ('swift', 61485), ('sql', 58346), ('node.js', 52827), ('r', 48079), ('arrays', 46739), ('json', 45250), ('ruby-on-rails', 39036), ('sql-server', 37077), ('c', 36080), ('asp.net', 35610), ('excel', 29924), ('angular2', 28832)]\n```\n:::\n:::\n\n\n\nIn general `GROUP BY` statements will involve some aggregation operation\non the subsets. Options include: `COUNT`, `MIN`, `MAX`, `AVG`, `SUM`.\n\nIf you filter after using `GROUP BY`, you need to use `having` instead of `where`.\n\n**Challenge**: Write a query that will count the number of answers for\neach question, returning the most answered questions.\n\n## Getting unique results (DISTINCT)\n\nA useful SQL keyword is `DISTINCT`, which allows you to eliminate\nduplicate rows from any table (or remove duplicate values when one only\nhas a single column or set of values).\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-19_d1f8cd62469c4c3f65ba33aaca826a5e'}\n\n```{.python .cell-code}\ntag_names = db.execute(\"select distinct tag from questions_tags\").fetchall()\ntag_names[0:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('c#',), ('razor',), ('flags',), ('javascript',), ('rxjs',)]\n```\n:::\n\n```{.python .cell-code}\ndb.execute(\"select count(distinct tag) from questions_tags\").fetchall()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(41006,)]\n```\n:::\n:::\n\n\n\n## Simple SQL joins\n\nOften to get the information we need, we'll need data from multiple\ntables. To do this we'll need to do a database join, telling the\ndatabase what columns should be used to match the rows in the different\ntables.\n\nThe syntax generally looks like this (again the `WHERE` and `ORDER BY` are\noptional):\n\n```\nSELECT <column(s)> FROM <table1> JOIN <table2> ON <columns to match on> WHERE <condition(s) on column(s)> ORDER BY <column(s)>\n```\n\nLet's see some joins using the different syntax on the Stack Overflow\ndatabase. In particular let's select only the questions with the tag\npython.\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-20_7494dd7c82c4a8b297dac8cbf95ae8c3'}\n\n```{.python .cell-code}\nresult1 = db.execute(\"select * from questions join questions_tags \\\n        on questions.questionid = questions_tags.questionid \\\n        where tag = 'python'\").fetchall()\n```\n:::\n\n\n\nIt turns out you can do it without using the JOIN keyword.\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-21_e9baaea6def4dda413be01863f0d2d9d'}\n\n```{.python .cell-code}\nresult2 = db.execute(\"select * from questions, questions_tags \\\n        where questions.questionid = questions_tags.questionid and \\\n        tag = 'python'\").fetchall()\n\nresult1[0:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(34553559, '2016-01-01 04:34:34', 3, 96, 'Python nested loops only working on the first pass', 845642, 34553559, 'python'), (34556493, '2016-01-01 13:22:06', 2, 30, 'bool operator in for Timestamp in Series does not work', 4458602, 34556493, 'python'), (34557898, '2016-01-01 16:36:04', 3, 143, 'Pairwise haversine distance calculation', 2927983, 34557898, 'python'), (34560088, '2016-01-01 21:10:32', 1, 126, \"Stopwatch (chronometre) doesn't work\", 5736692, 34560088, 'python'), (34560213, '2016-01-01 21:25:26', 1, 127, 'How to set the type of a pyqtSignal (variable of class X) that takes a X instance as argument', 5636400, 34560213, 'python')]\n```\n:::\n\n```{.python .cell-code}\nresult1 == result2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n:::\n\n\n\nHere's a three-way join (using both types of syntax) with some\nadditional use of aliases to abbreviate table names. What does this\nquery ask for?\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-22_05ac7426399406f278b8621add9aceb4'}\n\n```{.python .cell-code}\nresult1 = db.execute(\"select * from \\\n        questions Q \\\n        join questions_tags T on Q.questionid = T.questionid \\\n        join users U on Q.ownerid = U.userid \\\n        where tag = 'python' and \\\n        age > 60\").fetchall()\n\nresult2 = db.execute(\"select * from \\\n        questions Q, questions_tags T, users U where \\\n        Q.questionid = T.questionid and \\\n        Q.ownerid = U.userid and \\\n        tag = 'python' and \\\n        age > 60\").fetchall()\n\nresult1 == result2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n:::\n\n\n\n**Challenge**: Write a query that would return all the answers to\nquestions with the Python tag.\n\n**Challenge**: Write a query that would return the users who have\nanswered a question with the Python tag.\n\n## Temporary tables and views\n\nYou can think of a view as a temporary table that is the result of a\nquery and can be used in subsequent queries. In any given query you can\nuse both views and tables. The advantage is that they provide modularity\nin our querying. For example, if a given operation (portion of a query)\nis needed repeatedly, one could abstract that as a view and then make\nuse of that view.\n\nSuppose we always want the age and displayname of owners of questions to\nbe readily available. Once we have the view we can query it like a\nregular table.\n\n\n::: {.cell hash='unit7-bigData_cache/pdf/unnamed-chunk-23_72ed927351661cc4a0e69fd5a46850ac'}\n\n```{.python .cell-code}\ndb.execute(\"create view questionsAugment as select \\\n                questionid, questions.creationdate, score, viewcount, \\\n                title, ownerid, age, displayname \\\n                from questions join users \\\n                on questions.ownerid = users.userid\")\n## you'll see the return value is '0'\n               \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<sqlite3.Cursor object at 0x7fdc254c4d40>\n```\n:::\n\n```{.python .cell-code}\ndb.execute(\"select * from questionsAugment where age > 70 limit 5\").fetchall()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(36143022, '2016-03-21 22:44:16', 2, 158, 'Record aggregate with dynamic choice', 40851, 71, 'Simon Wright'), (40596400, '2016-11-14 19:30:32', 1, 28, 'Why does Xcode report \"Variable \\'tuple.0\\' used before being initialized\" after I\\'ve assigned all elements of a Swift tuple?', 74966, 71, 'hkatz'), (40612851, '2016-11-15 14:51:51', 1, 32, 'Converting a Swift array of Ints into an array of its running subtotals', 74966, 71, 'hkatz'), (38532865, '2016-07-22 18:10:34', 0, 134, 'Algorithm to generate all permutations of pairs without repetition', 98062, 71, 'Rocky Luck'), (36189874, '2016-03-23 22:25:56', 0, 32, 'Mediawiki 1.26.2 upgrade - categories listed in a single column', 332786, 71, 'Sakshale')]\n```\n:::\n:::\n\n\n\nOne use of a view would be to create a mega table that stores all the\ninformation from multiple tables in the (unnormalized) form you might\nhave if you simply had one data frame in Python or R.\n\n\n\n\n\n## More on joins\n\nWe've seen a bunch of joins but haven't discussed the full taxonomy of\ntypes of joins. There are various possibilities for how to do a join\ndepending on whether there are rows in one table that do not match any\nrows in another table.\n\n**Inner joins**: In database terminology an inner join is when the\nresult has a row for each match of a row in one table with the rows in\nthe second table, where the matching is done on the columns you\nindicate. If a row in one table corresponds to more than one row in\nanother table, you get all of the matching rows in the second table,\nwith the information from the first table duplicated for each of the\nresulting rows. For example in the Stack Overflow data, an inner join of\nquestions and answers would pair each question with each of the answers\nto that question. However, questions without any answers or (if this\nwere possible) answers without a corresponding question would not be\npart of the result.\n\n**Outer joins**: Outer joins add additional rows from one table that do\nnot match any rows from the other table as follows. A *left outer join*\ngives all the rows from the first table but only those from the second\ntable that match a row in the first table. A *right outer join* is the\nconverse, while a *full outer join* includes at least one copy of all\nrows from both tables. So a left outer join of the Stack Overflow\nquestions and answers tables would, in addition to the matched questions\nand their answers, include a row for each question without any answers,\nas would a full outer join. In this case there should be no answers that\ndo not correspond to question, so a right outer join should be the same\nas an inner join.\n\n**Cross joins**: A cross join gives the Cartesian product of the two\ntables, namely the pairwise combination of every row from each table,\nanalogous to `expand.grid()` in R. I.e., take a row from the first table\nand pair it with each row from the second table, then repeat that for\nall rows from the first table. Since cross joins pair each row in one\ntable with all the rows in another table, the resulting table can be\nquite large (the product of the number of rows in the two tables). In\nthe Stack Overflow database, a cross join would pair each question with\nevery answer in the database, regardless of whether the answer is an\nanswer to that question.\n\nSimply listing two or more tables separated by commas as we saw earlier\nis the same as a *cross join*. Alternatively, listing two or more tables\nseparated by commas, followed by conditions that equate rows in one\ntable to rows in another is equivalent to an *inner join*.\n\nIn general, inner joins can be seen as a form of cross join followed by\na condition that enforces matching between the rows of the table. More\nbroadly, here are four equivalent joins that all perform the equivalent\nof an inner join:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n## explicit inner join:\nselect * from table1 join table2 on table1.id = table2.id \n## non-explicit join without JOIN\nselect * from table1, table2 where table1.id = table2.id \n## cross-join followed by matching\nselect * from table1 cross join table2 where table1.id = table2.id \n## explicit inner join with 'using'\nselect * from table1 join table2 using(id)\n```\n:::\n\n\n**Challenge**: Create a view with one row for every question-tag pair,\nincluding questions without any tags.\n\n**Challenge**: Write a query that would return the displaynames of all\nof the users who have *never* posted a question. The NULL keyword will\ncome in handy it's like 'NA' in R. Hint: NULLs should be produced if you\ndo an outer join.\n\n## Indexes\n\nAn index is an ordering of rows based on one or more fields. DBMS use\nindexes to look up values quickly, either when filtering (if the index\nis involved in the `WHERE` condition) or when doing joins (if the index is\ninvolved in the `JOIN` condition). So in general you want your tables to\nhave indexes.\n\nDBMS use indexing to provide sub-linear time lookup. Without indexes, a\ndatabase needs to scan through every row sequentially, which is called\nlinear time lookup if there are n rows, the lookup is O(n) in\ncomputational cost. With indexes, lookup may be logarithmic O(log(n))\n(if using tree-based indexes) or constant time O(1) (if using hash-based\nindexes). A binary tree-based search is logarithmic; at each step\nthrough the tree you can eliminate half of the possibilities.\n\nHere's how we create an index, with some time comparison for a simple\nquery.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nt0 = time.time()\nresults = db.execute(\n  \"select * from questions where viewcount > 10000\").fetchall()\nprint(time.time() - t0)  # 10 seconds\nt0 = time.time()\ndb.execute(\n  \"create index count_index on questions (viewcount)\") # 19 seconds\nprint(time.time() - t0)  # 10 seconds\nt0 = time.time()\ndb.execute(\n  \"select * from questions where viewcount > 10000\").fetchall()  # 3 seconds\nprint(time.time() - t0)  # 10 seconds\n```\n:::\n\n\nIn other contexts, an index can save huge amounts of time. So if you're\nworking with a database and speed is important, check to see if there\nare indexes. That said, as seen above it takes time to create the index,\nso you'd only want to create it if you were doing multiple queries\nthat could take advantage of the index. See the databases tutorial\nfor more discussion of how using indexes in a lookup is not always advantageous.\n\n## Creating database tables\n\nOne can create tables from within the 'sqlite' command line interfaces\n(discussed in the tutorial), but often one would do this from\nPython or R. Here's the syntax from Python, creating the table\nfrom a Pandas dataframe.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## create data frame 'student_data' in some fashion\ncon = sq.connect(db_path)\nstudent_data.to_sql('student', con if_exists='replace', index=False)\n```\n:::\n\n\n# 4. Sparsity\n\nA lot of statistical methods are based on sparse matrices. These\ninclude:\n\n-   Matrices representing the neighborhood structure (i.e., conditional\n    dependence structure) of networks/graphs.\n-   Matrices representing autoregressive models (neighborhood structure\n    for temporal and spatial data)\n-   A statistical method called the *lasso* is used in high-dimensional\n    contexts to give sparse results (sparse parameter vector estimates,\n    sparse covariance matrix estimates)\n-   There are many others (I've been lazy here in not coming up with a\n    comprehensive list, but trust me!)\n\nWhen storing and manipulating sparse matrices, there is no need to store\nthe zeros, nor to do any computation with elements that are zero. \n\nPython, R, and MATLAB all have functionality for storing and computing\nwith sparse matrices. We'll see this a bit more in the linear algebra\nunit.\n\nHere's a [blog\npost](http://blog.revolutionanalytics.com/2011/05/the-neflix-prize-big-data-svd-and-r.html)\ndescribing the use of sparse matrix manipulations for analysis of the\nNetflix Prize data.\n\n# 5. Using statistical concepts to deal with computational bottlenecks\n\nAs statisticians, we have a variety of statistical/probabilistic tools\nthat can aid in dealing with big data.\n\n1.  Usually we take samples because we cannot collect data on the entire\n    population. But we can just as well take a sample because we don't\n    have the ability to process the data from the entire population. We\n    can use standard uncertainty estimates to tell us how close to the\n    true quantity we are likely to be. And we can always take a bigger\n    sample if we're not happy with the amount of uncertainty.\n2.  There are a variety of ideas out there for making use of sampling to\n    address big data challenges. One idea (due in part to Prof. Michael\n    Jordan here in Statistics/EECS) is to compute estimates on many\n    (relatively small) bootstrap samples from the data (cleverly\n    creating a reduced-form version of the entire dataset from each\n    bootstrap sample) and then combine the estimates across the samples.\n    Here's [the arXiv paper](http://arxiv.org/abs/1112.5016) on this\n    topic, also published as Kleiner et al. in Journal of the Royal\n    Statistical Society (2014) 76:795.\n3.  Randomized algorithms: there has been a lot of attention recently to\n    algorithms that make use of randomization. E.g., in optimizing a\n    likelihood, you might choose the next step in the optimization based\n    on random subset of the data rather than the full data. Or in a\n    regression context you might choose a subset of rows of the design\n    matrix (the matrix of covariates) and corresponding observations,\n    weighted based on the statistical leverage ([recall the discussion\n    of regression diagnostics in a regression course) of the\n    observations. Here's another [arXiv\n    paper](http://arxiv.org/abs/1104.5557) that provides some ideas in\n    this area.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}