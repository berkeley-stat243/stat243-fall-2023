{
  "hash": "1360e4a939e0518dd97d256a882c8c9a",
  "result": {
    "markdown": "---\ntitle: \"Simulation\"\nauthor: \"Chris Paciorek\"\ndate: \"2023-10-17\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n\n\n[PDF](./unit9-sim.pdf){.btn .btn-primary}\n\n\nReferences:\n\n-   Gentle: Computational Statistics\n-   Monahan: Numerical Methods of Statistics\n\nMany (most?) statistical papers include a simulation (i.e., Monte Carlo)\nstudy. Many papers on machine learning methods also include a simulation\nstudy. The basic idea is that closed-form mathematical analysis of the properties of\na statistical or machine learning method/model is often hard to do. Even\nif possible, it usually involves approximations or simplifications. A\ncanonical situation in statistics is that we have an asymptotic result\nand we want to know what happens in finite samples, but often we do not\neven have the asymptotic result. Instead, we can estimate mathematical\nexpressions using random numbers. So we design a simulation study to\nevaluate the method/model or compare multiple methods. The result is\nthat the researcher carries out an experiment (on the computer, sometimes called *in silico*), generally varying\ndifferent factors to see what has an effect on the outcome of interest.\n\nThe basic strategy generally involves simulating data and then using the\nmethod(s) on the simulated data, summarizing the results to\nassess/compare the method(s).\n\nMost simulation studies aim to approximate an integral, generally an\nexpected value (mean, bias, variance, MSE, probability, etc.). In low\ndimensions, methods such as Gaussian quadrature are best for estimating\nan integral but these methods don't scale well, so in higher dimensions (e.g., the usual situation with $n$ observations) we\noften use Monte Carlo techniques.\n\nTo be more concrete:\n\n-   If we have a *method for estimating a model parameter* (including\n    estimating uncertainty), such as a regression coefficient, what properties do\n    we want the method to have and what criteria could we use?\n\n-   If we have a *prediction method* (including prediction uncertainty),\n    what properties do we want the method to have and what criteria\n    could we use?\n\n-   If we have a *method for doing a hypothesis test*, what criteria\n    would we use to assess the hypothesis test? What properties do we\n    want the test to have?\n\n-   If we have a *method for finding a confidence interval or a prediction interval*, what\n    criteria would we use to assess the interval?\n\n\n# 1. Monte Carlo considerations\n\n## Motivating example\n\nLet's consider linear regression, with observations\n$Y=(y_{1},y_{2},\\ldots,y_{n})$ and an $n\\times p$ matrix of predictors/covariates/features/variables\n$X$, where\n$\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}Y$. If we assume that we have\n$EY=X\\beta$ and $\\mbox{Var}(Y)=\\sigma^{2}I$, then we can determine\nanalytically that we have $$\\begin{aligned}\nE\\hat{\\beta} & = & \\beta\\\\\n\\mbox{Var}(\\hat{\\beta})=E((\\hat{\\beta}-E\\hat{\\beta})^{2}) & = & \\sigma^{2}(X^{\\top}X)^{-1}\\\\\n\\mbox{MSPE}(Y^{*})=E(Y^{*}-\\hat{Y})^{2}) & = & \\sigma^{2}(1+X^{*\\top}(X^{\\top}X)^{-1}X^{*}).\\end{aligned}$$\nwhere $Y^{*}$is some new observation we'd like to predict given $X^{*}$.\n\nBut suppose that we're interested in the properties of standard regression\nestimation when in reality the mean is not linear in $X$ or the\nproperties of the errors are more complicated than having independent\nhomoscedastic errors. (This is always the case, but the issue is how far\nfrom the truth the standard assumptions are.) Or suppose we have a modified procedure to produce\n$\\hat{\\beta}$, such as a procedure that is robust to outliers. In those\ncases, we cannot compute the expectations above analytically.\n\nInstead we decide to use a Monte Carlo estimate. To keep the notation\nmore simple, let's just consider one element of the vector $\\beta$\n(i.e., one of the regression coefficients) and continue to call that\n$\\beta$. If we randomly generate $m$ different datasets from some\ndistribution $f$, and $\\hat{\\beta}_{i}$ is the estimated coefficient\nbased on the $i$th dataset: $Y_{i}=(y_{i1},y_{i2},\\ldots,y_{in})$, then\nwe can estimate $E\\hat{\\beta}$ under that distribution $f$ as\n$$\\hat{E}(\\hat{\\beta})=\\bar{\\hat{\\beta}}=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\beta}_{i}$$\nOr to estimate the variance, we have\n$$\\widehat{\\mbox{Var}}(\\hat{\\beta})=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{\\beta}_{i}-\\bar{\\hat{\\beta}})^{2}.$$\nIn evaluating the performance of regression under non-standard\nconditions or the performance of our robust regression procedure, what\ndecisions do we have to make to be able to carry out our Monte Carlo\nprocedure?\n\nNext let's think about Monte Carlo methods in general.\n\n## Monte Carlo (MC) basics\n\n### Monte Carlo overview\n\nThe basic idea is that we often want to estimate\n$\\phi\\equiv E_{f}(h(Y))$ for $Y\\sim f$. Note that if $h$ is an indicator\nfunction, this includes estimation of probabilities, e.g., for a scalar\n$Y$, we have\n$p=P(Y\\leq y)=F(y)=\\int_{-\\infty}^{y}f(t)dt=\\int I(t\\leq y)f(t)dt=E_{f}(I(Y\\leq y))$.\nWe would estimate variances or MSEs by having $h$ involve squared terms.\n\nWe get an MC estimate of $\\phi$ based on an iid sample of a large number\nof values of $Y$ from $f$:\n$$\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i}),$$ which is justified by\nthe Law of Large Numbers:\n$$\\lim_{m\\to\\infty}\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=E_{f}h(Y).$$\n\nNote that in most simulation studies, $Y$ is an entire dataset (predictors/covariates), and the \"iid\nsample\" means generating $m$ different datasets from $f$, i.e.,\n$Y_{i}\\in\\{Y_{1},\\ldots,Y_{m}\\}$ not $m$ different scalar values. If the\ndataset has $n$ observations, then $Y_{i}=(Y_{i1},\\ldots,Y_{in})$.\n\n#### Back to the regression example\n\nLet's relate that back to our regression example. In that particular\ncase, if we're interested in whether the regression estimator is biased,\nwe want to know: $$\\phi=E\\hat{\\beta},$$ where $h(Y) = \\hat{\\beta}(Y)$. We can use the Monte Carlo\nestimate of $\\phi$:\n$$\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\beta}_{i}=\\widehat{E(\\hat{\\beta})}.$$\n\nIf we are interested in the variance of the regression estimator, we have\n\n$$\\phi=\\mbox{Var}(\\hat{\\beta})=E_{f}((\\hat{\\beta}-E\\hat{\\beta})^{2})$$\nand we can use the Monte Carlo estimate of $\\phi$:\n$$\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{\\beta}_{i}-E\\hat{\\beta})^{2}=\\widehat{\\mbox{Var}(\\hat{\\beta)}}$$\nwhere $$h(Y)=(\\hat{\\beta}-E\\hat{\\beta})^{2}.$$\n\nFinally note that we also need to use the Monte Carlo estimate of\n$E\\hat{\\beta}$ in the Monte Carlo estimation of the variance.\n\nWe might also be interested in the coverage of a confidence interval. In\nthat case we have $$h(Y)=1_{\\beta\\in CI(Y)}$$ and we can estimate the\ncoverage as\n$$\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}1_{\\beta\\in CI(y_{i})}.$$\nOf course we want that $\\hat{\\phi}\\approx1-\\alpha$ for a $100(1-\\alpha)$\nconfidence interval. In the standard case of a 95% interval we want\n$\\hat{\\phi}\\approx0.95$.\n\n### Simulation uncertainty (i.e., Monte Carlo uncertainty)\n\nSince $\\hat{\\phi}$ is simply an average of $m$ identically-distributed\nvalues, $h(Y_{1}),\\ldots,h(Y_{m})$, the simulation variance of\n$\\hat{\\phi}$ is $\\mbox{Var}(\\hat{\\phi})=\\sigma^{2}/m$, with\n$\\sigma^{2}=\\mbox{Var}(h(Y))$. An estimator of\n$\\sigma^{2}=E_{f}((h(Y)-\\phi)^{2})$ is $$\\begin{aligned}\n\\hat{\\sigma}^{2} & = & \\frac{1}{m-1}\\sum_{i=1}^{m}(h(Y_{i})-\\hat{\\phi})^{2}\\end{aligned}$$\nSo our MC simulation error is based on\n$$\\widehat{\\mbox{Var}}(\\hat{\\phi})=\\frac{\\hat{\\sigma}^{2}}{m}=\\frac{1}{m(m-1)}\\sum_{i=1}^{m}(h(Y_{i})-\\hat{\\phi})^{2}.$$\nNote that this is particularly confusing if we have\n$\\hat{\\phi}=\\widehat{\\mbox{Var}(\\hat{\\beta})}$ because then we have\n$\\widehat{\\mbox{Var}}(\\hat{\\phi})=\\widehat{\\mbox{Var}}(\\widehat{\\mbox{Var}(\\hat{\\beta})})$!\n\nThe simulation variance is $O(\\frac{1}{m})$ because we have $m^{2}$ in\nthe denominator and a sum over $m$ terms in the numerator.\n\nNote that in the simulation setting, the randomness in the system is\nvery well-defined (as it is in survey sampling, but unlike in most other\napplications of statistics), because it comes from the RNG that we\nperform as part of our attempt to estimate $\\phi$. Happily, we are in\ncontrol of $m$, so in principle we can reduce the simulation error to as\nlittle as we desire. Unhappily, as usual, the simulation standard error goes down\nwith the square root of $m$.\n\n> **Important**: This is the uncertainty in our simulation-based estimate\nof some quantity (expectation) of interest. It is NOT the statistical uncertainty in a problem.\n\n#### Back to the regression example\n\nSome examples of simulation variances we might be interested in in the\nregression example include:\n\n-   Uncertainty in our estimate of bias:\n    $\\widehat{\\mbox{Var}}(\\widehat{E(\\hat{\\beta})}-\\beta)$.\n\n-   Uncertainty in the estimated variance of the estimated coefficient:\n    $\\widehat{\\mbox{Var}}(\\widehat{\\mbox{Var}(\\hat{\\beta})})$.\n\n-   Uncertainty in the estimated mean square prediction error:\n    $\\widehat{\\mbox{Var}}(\\widehat{\\mbox{MSPE}(Y^{*})})$.\n\nIn all cases we have to estimate the simulation variance, hence the\n$\\widehat{\\mbox{Var}}()$ notation.\n\n### Final notes\n\nSometimes the $Y_{i}$ are generated in a dependent fashion (e.g.,\nsequential MC or MCMC), in which case this variance estimator,\n$\\widehat{\\mbox{Var}}(\\hat{\\phi})$ does not hold because the samples are\nnot IID, but the estimator $\\hat{\\phi}$ is still a valid, unbiased\nestimator of $\\phi$.\n\n## Variance reduction (optional)\n\nThere are some tools for variance reduction in MC settings. One is\nimportance sampling (see Section 3). Others are the use of control\nvariates and antithetic sampling. I haven't personally run across these\nlatter in practice, so I'm not sure how widely used they are and won't\ngo into them here.\n\nIn some cases we can set up natural strata, for which we know the\nprobability of being in each stratum. Then we would estimate $\\mu$ for\neach stratum and combine the estimates based on the probabilities. The\nintuition is that we remove the variability in sampling amongst the\nstrata from our simulation.\n\nAnother strategy that comes up in MCMC contexts is\n*Rao-Blackwellization*. Suppose we want to know $E(h(X))$ where\n$X=\\{X_{1},X_{2}\\}$. Iterated expectation tells us that\n$E(h(X))=E(E(h(X)|X_{2})$. If we can compute\n$E(h(X)|X_{2})=\\int h(x_{1},x_{2})f(x_{1}|x_{2})dx_{1}$ then we should\navoid introducing stochasticity related to the $X_{1}$ draw (since we\ncan analytically integrate over that) and only average over\nstochasticity from the $X_{2}$ draw by estimating\n$E_{X_{2}}(E(h(X)|X_{2})$. The estimator is\n$$\\hat{\\mu}_{RB}=\\frac{1}{m}\\sum_{i=1}^{m}E(h(X)|X_{2,i})$$ where we\neither draw from the marginal distribution of $X_{2}$, or equivalently,\ndraw $X$, but only use $X_{2}$. Our MC estimator averages over the\nsimulated values of $X_{2}$. This is called Rao-Blackwellization because\nit relates to the idea of conditioning on a sufficient statistic. It has\nlower variance because the variance of each term in the sum of the\nRao-Blackwellized estimator is $\\mbox{Var}(E(h(X)|X_{2})$, which is less\nthan the variance in the usual MC estimator, $\\mbox{Var}(h(X))$, based\non the usual iterated variance formula:\n$V(X)=E(V(X|Y))+V(E(X|Y))\\Rightarrow V(E(X|Y))<V(X)$.\n\n# 2. Design of simulation studies\n\nConsider the paper that is part of PS5. We can think about\ndesigning a simulation study in that context.\n\nFirst, what are the key issues that need to be assessed to evaluate\ntheir methodology?\n\nSecond, what do we need to consider in carrying out a simulation study\nto address those issues? I.e., what are the key decisions to be made in\nsetting up the simulations?\n\n## Basic steps of a simulation study\n\n1.  Specify what makes up an individual experiment (i.e., the individual\n    simulated dataset) given a specific set of inputs: sample size,\n    distribution(s) to use, parameter values, statistic of interest,\n    etc. In other words, exactly how would you generate one simulated\n    dataset?\n\n2.  Often you'll want to see how your results will vary if you change\n    some of the inputs; e.g., sample sizes, parameter values, data\n    generating mechanisms. So determine what factors you'll want to\n    vary. Each unique combination of input values will be a scenario.\n\n3.  Write code to carry out the individual experiment and return the\n    quantity of interest, with arguments to your code being the inputs\n    that you want to vary.\n\n4.  For each combination of inputs you want to explore (each scenario),\n    repeat the experiment $m$ times. Note this is an easily\n    parallel calculation (in both the data generating dimension and the\n    inputs dimension(s)).\n\n5.  Summarize the results for each scenario, quantifying simulation\n    uncertainty.\n\n6.  Report the results in graphical or tabular form.\n\nOften a simulation study will compare multiple methods, so you'll need\nto do steps 3-6 for each method.\n\n## Various considerations\n\nSince a simulation study is an experiment, we should use the same\nprinciples of design and analysis we would recommend when advising a\npracticioner on setting up a scientific experiment.\n\nThese include efficiency, reporting of uncertainty, reproducibility and\ndocumentation.\n\nIn generating the data for a simulation study, we want to think about\nwhat structure real data would have that we want to mimic in the\nsimulation study: distributional assumptions, parameter values,\ndependence structure, outliers, random effects, sample size ($n$), etc.\n\nAll of these may become input variables in a simulation study. Often we\ncompare two or more statistical methods conditioning on the data context\nand then assess whether the differences between methods vary with the\ndata context choices. E.g., if we compare an MLE to a robust estimator,\nwhich is better under a given set of choices about the data generating\nmechanism and how sensitive is the comparison to changing the features\nof the data generating mechanism? So the \"treatment variable\" is the\nchoice of statistical method. We're then interested in sensitivity to\nthe conditions (different input values).\n\nOften we can have a large number of replicates ($m$) because the\nsimulation is fast on a computer, so we can sometimes reduce the\nsimulation error to essentially zero and thereby avoid reporting\nuncertainty. To do this, we need to calculate the simulation standard\nerror, generally, $s/\\sqrt{m}$ and see how it compares to the effect\nsizes. This is particularly important when reporting on the bias of a\nstatistical method.\n\nWe might denote the data, which could be the statistical estimator under\neach of two methods as $Y_{ijklq}$, where $q$ indexes treatment, $j,k,l$\nindex different additional input variables, and $i\\in\\{1,\\ldots,m\\}$\nindexes the replicate. E.g., $j$ might index whether the data are from a\nt or normal, $k$ the value of a parameter, and $l$ the dataset sample\nsize (i.e., different levels of $n$).\n\nOne can think about choosing $m$ based on a basic power calculation,\nthough since we can always generate more replicates, one might just\nproceed sequentially and stop when the precision of the results is\nsufficient.\n\nWhen comparing methods, it's best to use the same simulated datasets for\neach level of the treatment variable and to do an analysis that controls\nfor the dataset (i.e., for the random numbers used), thereby removing\nsome variability from the error term. A simple example is to do a paired\nanalysis, where we look at differences between the outcome for two\nstatistical methods, pairing based on the simulated dataset.\n\nOne can even use the \"same\" random number generation for the replicates\nunder different conditions. E.g., in assessing sensitivity to a $t$ vs.\nnormal data generating mechanism, we might generate the normal RVs and\nthen for the $t$ use the same random numbers, in the sense of using the\nsame quantiles of the $t$ as were generated for the normal - this is\npretty easy, as seen below. This helps to control for random differences\nbetween the datasets.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import t, norm\n\ndevs = np.random.normal(size=100)\ntdevs = t.ppf(norm.cdf(devs), df=1)\n\nplt.scatter(devs, tdevs)\nplt.xlabel('devs'); plt.ylabel('tdevs')\nplt.plot([min(devs), max(devs)], [min(devs), max(devs)], color='red')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](unit9-sim_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Experimental Design (optional)\n\nA typical context is that one wants to know the effect of multiple input\nvariables on some outcome. Often, scientists, and even statisticians\ndoing simulation studies will vary one input variable at a time. As we\nknow from standard experimental design, this is inefficient.\n\nThe standard strategy is to discretize the inputs, each into a small\nnumber of levels. If we have a small enough number of inputs and of\nlevels, we can do a full factorial design (potentially with\nreplication). For example if we have three inputs and three levels each,\nwe have $3^{3}$ different treatment combinations. Choosing the levels in\na reasonable way is obviously important.\n\nAs the number of inputs and/or levels increases to the point that we\ncan't carry out the full factorial, a fractional factorial is an option.\nThis carefully chooses which treatment combinations to omit. The goal is\nto achieve balance across the levels in a way that allows us to estimate\nlower level effects (in particular main effects) but not all high-order\ninteractions. What happens is that high-order interactions are aliased\nto (confounded with) lower-order effects. For example you might choose a\nfractional factorial design so that you can estimate main effects and\ntwo-way interactions but not higher-order interactions.\n\nIn interpreting the results, I suggest focusing on the decomposition of\nsums of squares and not on statistical significance. In most cases, we\nexpect the inputs to have at least some effect on the outcome, so the\nnull hypothesis is a straw man. Better to assess the magnitude of the\nimpacts of the different inputs.\n\nWhen one has a very large number of inputs, one can use the Latin\nhypercube approach to sample in the input space in a uniform way,\nspreading the points out so that each input is sampled uniformly. Assume\nthat each input is $\\mathcal{U}(0,1)$ (one can easily transform to\nwhatever marginal distributions you want). Suppose that you can run $m$\nsamples. Then for each input variable, we divide the unit interval into\n$m$ bins and randomly choose the order of bins and the position within\neach bin. This is done independently for each variable and then combined\nto give $m$ samples from the input space. We would then analyze main\neffects and perhaps two-way interactions to assess which inputs seem to\nbe most important.\n\nEven amongst statisticians, taking an experimental design approach to a\nsimulation study is not particularly common, but it's worth considering.\n\n# 3. Implementation of simulation studies\n\nLuke Miratrix (a UCB Stats PhD alum) has prepared a nice tutorial on\ncarrying out a simulation study, including helpful R code. So if the\ndiscussion here is not concrete enough or you want to see how to\neffectively implement such a study, see\n*simulation_tutorial_miratrix.pdf* and the similarly named R code file.\n\n## Computational efficiency\n\nParallel processing is often helpful for simulation studies. The reason\nis that simulation studies are embarrassingly parallel - we can send\neach replicate to a different computer processor and then collect the\nresults back, and the speedup should scale directly with the number of\nprocessors we used. Since we often need to some sort of looping, writing\ncode in C/C++ and compiling and linking to the code from Python may also be a\ngood strategy, albeit one not covered in this course.\n\nA handy function in Python is `itertools.product` to get all combinations of\na set of vectors.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport itertools\n\nthetaLevels = [\"low\", \"med\", \"hi\"]\nn = [10, 100, 1000]\ntVsNorm = [\"t\", \"norm\"]\nlevels = list(itertools.product(thetaLevels, tVsNorm, n))\n```\n:::\n\n\n\n\n## Analysis and reporting\n\nOften results are reported simply in tables, but it can be helpful to\nthink through whether a graphical representation is more informative\n(sometimes it's not or it's worse, but in some cases it may be much\nbetter). Since you'll often have a variety of scenarios to display,\nusing trellis plots in ggplot2 via the `facet_wrap` function will often\nbe a good approach to display how results vary as a function of multiple\ninputs in R. In Python, it looks like there are various ways (`RPlot` in pandas,\nseaborn, plotly), but I don't know what the most standard way is.\n\nYou should set the seed when you start the experiment, so that it's\npossible to replicate it. It's also a good idea to save the current\nvalue of the seed whenever you save interim results, so that you can\nrestart simulations (this is particularly helpful for MCMC) at the exact\npoint you left off, including the random number sequence.\n\nTo enhance reproducibility, it's good practice to post your simulation\ncode (and potentially simulated data) on GitHub, on your website, or as\nsupplementary material with the journal. Another person should be able\nto fully reproduce your results, including the exact random number\ngeneration that you did (e.g., you should provide code for how you set\nthe random seed for your randon number generator).\n\nMany journals are requiring increasingly detailed documentation of the\ncode and data used in your work, including code and data for\nsimulations. Here are the American Statistical Association's\nrequirements on documenting computations in its journals:\n\n\"The ASA strongly encourages authors to submit datasets, code, other\nprograms, and/or extended appendices that are directly relevant to their\nsubmitted articles. These materials are valuable to users of the ASA's\njournals and further the profession's commitment to reproducible\nresearch. Whenever a dataset is used, its source should be fully\ndocumented and the data should be made available as on online\nsupplement. Exceptions for reasons of security or confidentiality may be\ngranted by the Editor. Whenever specific code has been used to implement\nor illustrate the results of a paper, that code should be made available\nif possible. \\[\\....snip\\....\\] Articles reporting results based on\ncomputation should provide enough information so that readers can\nevaluate the quality of the results. Such information includes estimated\naccuracy of results, as well as descriptions of pseudorandom-number\ngenerators, numerical algorithms, programming languages, and major\nsoftware components used.\"\n\n# 4. Random number generation (RNG)\n\nAt the core of simulations is the ability to generate random numbers,\nand based on that, random variables. On a computer, our goal is to\ngenerate sequences of pseudo-random numbers that behave like random\nnumbers but are replicable. The reason that replicability is important\nis so that we can reproduce the simulation.\n\n## Generating random uniforms on a computer\n\nGenerating a sequence of random standard uniforms is the basis for all\ngeneration of random variables, since random uniforms (either a single\none or more than one) can be used to generate values from other\ndistributions. Most random numbers on a computer are *pseudo-random*. The\nnumbers are chosen from a deterministic stream of numbers that behave\nlike random numbers but are actually a finite sequence (recall that both\nintegers and real numbers on a computer are actually discrete and there\nare finitely many distinct values), so it's actually possible to get\nrepeats. The seed of a RNG is the place within that sequence where you\nstart to use the pseudo-random numbers.\n\n### Sequential congruential generators\n\nMany RNG methods are sequential congruential methods. The basic idea is\nthat the next value is $$u_{k}=f(u_{k-1},\\ldots,u_{k-j})\\mbox{mod}\\,m$$\nfor some function, $f$, and some positive integer $m$ . Often $j=1$.\n*mod* just means to take the remainder after dividing by $m$. One then\ngenerates the random standard uniform value as $u_{k}/m$, which by\nconstruction is in $[0,1]$. For our discussion below, it is important\nto distinguish the *state* ($u$) from the output of the RNG.\n\nGiven the construction, such sequences are periodic if the subsequence\never reappears, which is of course guaranteed because there is a finite\nnumber of possible subsequence values given that all the $u_{k}$ values\nare remainders of divisions by a fixed number . One key to a good random\nnumber generator (RNG) is to have a very long period.\n\nAn example of a sequential congruential method is a basic linear\ncongruential generator: $$u_{k}=(au_{k-1}+c)\\mbox{mod}\\,m$$ with integer\n$a$, $m$, $c$, and $u_{k}$ values. (Note that in some cases $c=0$, in which case the periodicity can't exceed $m-1$ as the method is then set up so that we never get $u_{k}=0$ as this causes the\nalgorithm to break.) The seed is\nthe initial state, $u_{0}$ - i.e., the point in the sequence at which we\nstart. By setting the seed you guarantee reproducibility since given a\nstarting value, the sequence is deterministic. In general $a$, $c$ and $m$\nare chosen to be 'large'. The standard values of $m$\nare Mersenne primes, which have the form $2^{p}-1$ (but these are not\nprime for all $p$). Here's an example of a\nlinear congruential sampler (with $c=0$):\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 100\na = 171\nm = 30269\n\nu = np.empty(n)\nu[0] = 7306\n\nfor i in range(1, n):\n    u[i] = (a * u[i-1]) % m\n\nu = u / m\nuFromNP = np.random.uniform(size = n)\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(range(1, n+1), u)\nplt.title(\"manual\")\nplt.xlabel(\"Index\"); plt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 2)\nplt.plot(range(1, n+1), uFromNP)\nplt.title(\"numpy\")\nplt.xlabel(\"Index\"); plt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 3)\nplt.hist(u, bins=25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(array([6., 3., 6., 4., 1., 4., 6., 3., 4., 9., 4., 5., 1., 1., 2., 7., 1.,\n       2., 5., 2., 6., 5., 4., 4., 5.]), array([0.01833559, 0.05743434, 0.09653309, 0.13563183, 0.17473058,\n       0.21382933, 0.25292808, 0.29202683, 0.33112557, 0.37022432,\n       0.40932307, 0.44842182, 0.48752057, 0.52661931, 0.56571806,\n       0.60481681, 0.64391556, 0.68301431, 0.72211305, 0.7612118 ,\n       0.80031055, 0.8394093 , 0.87850804, 0.91760679, 0.95670554,\n       0.99580429]), <BarContainer object of 25 artists>)\n```\n:::\n\n```{.python .cell-code}\nplt.xlabel(\"Value\"); plt.ylabel(\"Frequency\")\n\nplt.subplot(2, 2, 4)\nplt.hist(uFromNP, bins=25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(array([5., 5., 5., 5., 3., 3., 6., 4., 5., 2., 3., 6., 4., 3., 8., 6., 2.,\n       2., 3., 4., 2., 3., 6., 0., 5.]), array([0.00250736, 0.04226924, 0.08203112, 0.12179301, 0.16155489,\n       0.20131677, 0.24107866, 0.28084054, 0.32060242, 0.36036431,\n       0.40012619, 0.43988807, 0.47964995, 0.51941184, 0.55917372,\n       0.5989356 , 0.63869749, 0.67845937, 0.71822125, 0.75798314,\n       0.79774502, 0.8375069 , 0.87726879, 0.91703067, 0.95679255,\n       0.99655443]), <BarContainer object of 25 artists>)\n```\n:::\n\n```{.python .cell-code}\nplt.xlabel(\"Value\"); plt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](unit9-sim_files/figure-html/unnamed-chunk-4-3.png){width=960}\n:::\n:::\n\n\n\nA wide variety of different RNG have been proposed. Many have turned out\nto have substantial defects based on tests designed to assess if the\nbehavior of the RNG mimics true randomness. Some of the behavior we want\nto ensure is uniformity of each individual random deviate, independence\nof sequences of deviates, and multivariate uniformity of subsequences.\nOne test of a RNG that many RNGs don't perform well on is to assess the\nproperties of $k$-tuples - subsequences of length $k$, which should be\nindependently distributed in the $k$-dimensional unit hypercube.\nUnfortunately, linear congruential methods produce values that lie on a\nsimple lattice in $k$-space, i.e., the points are not selected from\n$q^{k}$ uniformly spaced points, where $q$ is the the number of unique\nvalues. Instead, points often lie on parallel lines in the hypercube.\n\nCombining generators can yield better generators. The Wichmann-Hill is\nan option in R and is a combination of three linear congruential\ngenerators with $a=\\{171,172,170\\}$, $m=\\{30269,30307,30323\\}$, and\n$u_{i}=(x_{i}/30269+y_{i}/30307+z_{i}/30323)\\mbox{mod}\\,1$ where $x$,\n$y$, and $z$ are generated from the three individual generators. Let's\nmimic the Wichmann-Hill manually:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRNGkind(\"Wichmann-Hill\")\nset.seed(1)\nsaveSeed <- .Random.seed\nuFromR <- runif(10)\na <- c(171, 172, 170)\nm <- c(30269, 30307, 30323)\nxyz <- matrix(NA, nr = 10, nc = 3)\nxyz[1, ] <- (a * saveSeed[2:4]) %% m\nfor( i in 2:10)\n\txyz[i, ] <- (a * xyz[i-1, ]) %% m\nfor(i in 1:10)\n\tprint(c(uFromR[i],sum(xyz[i, ]/m)%%1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1297134 0.1297134\n[1] 0.9822407 0.9822407\n[1] 0.8267184 0.8267184\n[1] 0.242355 0.242355\n[1] 0.8568853 0.8568853\n[1] 0.8408788 0.8408788\n[1] 0.3421633 0.3421633\n[1] 0.7062672 0.7062672\n[1] 0.6212432 0.6212432\n[1] 0.6537663 0.6537663\n```\n:::\n\n```{.r .cell-code}\n## we should be able to recover the current value of the seed\nxyz[10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24279 14851 10966\n```\n:::\n\n```{.r .cell-code}\n.Random.seed[2:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24279 14851 10966\n```\n:::\n:::\n\n\n\n### PCG generators\n\nSomewhat recently [O'Neal (2014) proposed a new approach](https://www.pcg-random.org/pdf/hmc-cs-2014-0905.pdf) to using the linear congruential generator in a way that gives much better performance than the basic versions of such generators described above. This approach is now the default random number generator in numpy (see `numpy.random.default_rng()`), called the [PCG-64 generator](https://numpy.org/doc/stable/reference/random/bit_generators/pcg64.html#numpy.random.PCG64). 'PCG' stands for permutation congruential generator and encompasses a family of such generators.\n\nThe idea of the PCG approach goes like this:\n\n  - Linear congruential generators (LCG) are simple and fast, but for small values of $m$ don't perform all that well statistically, in particular having values on a lattice as discussed above.\n  - Using a large value of $m$ can actually give good statistical performance.\n  - Applying a technique called  *permutation functions* to the state of the LCG in order to produce the output at each step (the random value returned to the user) can improve the statistical performance even further.\n\nInstead of using relatively small values of $m$ seen above, in the PCG approach one uses $m=2^k$, for 'large enough' $k$, usually 64 or 128. It turns out that if $m=2^k$ then the period of the $b$th bit of the state is $2^b$ where $b=1$ is the right-most bit. Small periods are of course bad for RNG, so the bits with small period cause the LCG to not perform well. Thankfully, one simple fix is simply to discard some number of the right-most bits (this is one form of *bit shift*). Note that if one does this, the output of the RNG is based on a subset of the bits, which means that the number of unique values that can be generated is smaller than the period. This is not a problem given we start with a state with a large number of bits (64 or 128 as mentioned above).\n\nO'Neal then goes further; instead of simply discarding bits, she proposes to either shift bits by a random amount or rotate bits by a random amount, where the random amount is determined by a small number of the initial bits. This improves the statistical performance of the generator. The choice of how to do this gives the various members of the PCG family of generators. The details are fairly complicated (the PCG paper is 50-odd pages) and not important for our purposes here.\n\n### Mersenne Twister\n\nA commonly used generator (including in both R and Python) is the Mersenne Twister.\nIt's the default in R and \"sort of\" the default in numpy (see next section for what I mean by \"sort of\").\n\nThe Mersenne Twister has some theoretical support,\nhas performed reasonably on standard tests of pseudorandom numbers and\nhas been used without evidence of serious failure. (But note that O'Neal criticizes it in\n[her technical report](https://www.pcg-random.org/pdf/hmc-cs-2014-0905.pdf).) Plus it's fast\n(because bitwise operations are fast).  The\nparticular Mersenne twister used has a periodicity of\n$2^{19937}-1\\approx10^{6000}$. Practically speaking this means that if\nwe generated one random uniform per nanosecond for 10 billion years,\nthen we would generate $10^{25}$ numbers, well short of the period. So\nwe don't need to worry about the periodicity! The state (sometimes also called the seed) for the Mersenne\ntwister is a set of 624 32-bit integers plus a position in the set,\nwhere the position is `.Random.seed[2]` in R and (I think) `np.random.get_state()[2]` in Python.\n\nThe Mersenne twister is in the class of generalized feedback shift registers (GFSR). The basic idea of\na GFSR is to come up with a deterministic generator of bits (i.e., a way\nto generate sequences of 0s and 1s), $B_{i}$, $i=1,2,3,\\ldots$. The\npseudo-random numbers are then determined as sequential subsequences of\nlength $L$ from $\\{B_{i}\\}$, considered as a base-2 number and dividing\nby $2^{L}$ to get a number in $(0,1)$. In general the sequence of bits\nis generated by taking $B_{i}$ to be the *exclusive or* \\[i.e., 0+0 = 0;\n0 + 1 = 1; 1 + 0 = 1; 1 + 1 = 0\\] summation of two previous bits further\nback in the sequence where the lengths of the lags are carefully chosen.\n\nnumpy provides access to the Mersenne Twister via the `MT19937` generator;\nmore on this below. It looks like PCG-64 only became available as of numpy version 1.17.\n\n### The period versus the number of unique values generated\n\nThe output of the PCG-64 is 64 bits while for the Mersenne Twister the output is 32 bits.\nThe result is that the generators generate fewer unique values than their periods.\nThis means you could get duplicated values in long runs, but this does not violate the\ncomment about the periodicity of PCG-64 and Mersenne-Twister being longer than $2^{64}$ and $2^{32}$.\nWhy not? Bbecause the two values after the two\nduplicated numbers will not be duplicates of each other -- as noted previously, there is\na distinction between the output presented to the user and the state of\nthe RNG algorithm.\n\n### The seed and the state\n\nSetting the seed picks a position in the periodic sequence of the RNG,\ni.e., in the state of the RNG. The state can be a single number or something\nmuch more complicated. As mentioned above, the state for the Mersenne Twister\nis a set of 624 32-bit integers plus a position in the set. For the PCG-64\nin numpy, the state is two numbers -- the actual state and the increment (`c` above).\nThis means that when the user passes a single number as the seed, there\nneeds to be a procedure that deterministically sets the state based on\nthat single number seed. The details of this are not usually well-documented\nor viewable by the user.\n\nIdeally, nearby seeds generally should not correspond to getting sequences from\nthe RNG stream that are closer to each other than far away seeds.\nAccording to Gentle (CS, p. 327) the input to `set.seed()` in R should be an integer,\n$i\\in\\{0,\\ldots,1023\\}$ , and each of these 1024 values produces\npositions in the RNG sequence that are \"far away\" from each other. I\ndon't see any mention of this in the R documentation for `set.seed()`\nand furthermore, you can pass integers larger than 1023 to `set.seed()`,\nso I'm not sure how much to trust Gentle's claim. More on generating\nparallel streams of random numbers below.\n\nWhen one invokes a RNG without a seed, RNG implementations generally have a method for\nchoosing a seed (often based on the system clock). The numpy documentation\nsays that it \"mixes sources of entropy in a reproducible way\" to do this.\n\nGenerators should give you the same sequence of random numbers, starting\nat a given seed, whether you ask for a bunch of numbers at once, or\nsequentially ask for individual numbers.\n\n#### Additional notes\n\nThere have been some attempts to generate truly random numbers based on\nphysical randomness. One that is based on quantum physics is\n<http://www.idquantique.com/true-random-number-generator/quantis-usb-pcie-pci.html>.\nAnother approach is based on lava lamps!\n\n## RNG in Python\n\n### Choosing a generator\n\nIn numpy, the *default_rng* RNG is PCG-64. It has a period of $2^{128}$ and supports\nadvancing an arbitrary number of steps, as well\nas $2^{127}$ streams (both useful for generating random numbers when parallelizing). The state of the PCG-64 RNG is represented by two\n128-bit unsigned integers, one the actual state and one the value of $c$ (the *increment*).\n\nHowever, while the *default* is PCG-64, simply\nusing the functions available via `np.random` to generate random numbers\nseems to actually use the Mersenne Twister, so the meaning of *default*\nis tricky.\n\nI think that this text from `help(np.random)` explains what is going on:\n\n```\n   Legacy\n   ------\n    \n   For backwards compatibility with previous versions of numpy before 1.17, the\n   various aliases to the global `RandomState` methods are left alone and do not\n   use the new `Generator` API.\n```\n\nWe can change to a specific RNG using syntax (the `Generator` API) like this:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng = np.random.Generator(np.random.MT19937(seed = 1))  # Mersenne Twister\nrng = np.random.Generator(np.random.PCG64(seed = 1))    # PCG-64\n```\n:::\n\n\nbut below note that there is a simpler way to change to the PCG-64.\n\nThen to use that generator when doing operations that generate random numbers, we need\nto use methods accessed via the generator:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng.random.normal(size = 3)     # Now generate based on chosen generator.\n## np.random.normal(size = 3)   # This will NOT use the chosen generator.\n```\n:::\n\n\n\nIn R, the default RNG is the Mersenne twister (`?RNGkind`).\n\n### Using the Mersenne Twister\n\n\nIf we simply start using numpy or scipy to generate random numbers,\nwe'll be using the Mersenne Twister. I believe this is what the\ndocumentation mentioned above means by \"aliases to the global `RandomState` methods\".\n\nWe get replicability by setting the seed to a specific value at the\nbeginning of our simulation. We can then set the seed to that same value\nwhen we want to replicate the simulation.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(1)\nnp.random.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n```\n:::\n\n```{.python .cell-code}\nnp.random.seed(1)\nnp.random.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n```\n:::\n:::\n\n\n\nWe can also save the state of the RNG and pick up where we left off. So\nthis code will pick where you had left off, ignoring what happened in\nbetween saving to `saved_state` and resetting.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(1)\nnp.random.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n```\n:::\n\n```{.python .cell-code}\nsaved_state = np.random.get_state()\nnp.random.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038])\n```\n:::\n:::\n\n\n\nNow we'll do some arbitrary work with random numbers, and see that if we use the saved state\nwe can pick up where we left off above.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntmp = np.random.choice(np.arange(1, 51), size=2000, replace=True) # arbitrary work\n\n## Restore the state.\nnp.random.set_state(saved_state)\nnp.random.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038])\n```\n:::\n:::\n\n\n\nIf we look at `saved_state`, we can confirm it actually corresponds to the Mersenne\nTwister.\n\n### Using PCG64\n\nTo use the PCG-64, we need to explicitly create and\nmake use of the `Generator` object (`rng` here), which is the new numpy\napproach to handling RNG.\n\nWe set the seed  when setting up the generator via `np.random.default_rng(seed)`\n(or `np.random.Generator(np.random.PCG64(seed = 1))`).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng = np.random.default_rng(seed = 1)\nrng.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 0.34558419,  0.82161814,  0.33043708, -1.30315723,  0.90535587])\n```\n:::\n\n```{.python .cell-code}\nrng = np.random.default_rng(seed = 1)\nrng.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 0.34558419,  0.82161814,  0.33043708, -1.30315723,  0.90535587])\n```\n:::\n\n```{.python .cell-code}\nsaved_state = rng.bit_generator.state\nrng.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 0.44637457, -0.53695324,  0.5811181 ,  0.3645724 ,  0.2941325 ])\n```\n:::\n\n```{.python .cell-code}\ntmp = rng.choice(np.arange(1, 51), size=2000, replace=True)\nrng.bit_generator.state = saved_state\nrng.normal(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 0.44637457, -0.53695324,  0.5811181 ,  0.3645724 ,  0.2941325 ])\n```\n:::\n\n```{.python .cell-code}\nsaved_state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'bit_generator': 'PCG64', 'state': {'state': 216676376075457487203159048251690499413, 'inc': 194290289479364712180083596243593368443}, 'has_uint32': 0, 'uinteger': 0}\n```\n:::\n\n```{.python .cell-code}\nsaved_state['state']['state']   # actual state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n216676376075457487203159048251690499413\n```\n:::\n\n```{.python .cell-code}\nsaved_state['state']['inc']     # increment ('c')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n194290289479364712180083596243593368443\n```\n:::\n:::\n\n\n\n`saved_state` contains the actual state and the value of `c`, the increment.\n\n\n## RNG in parallel\n\nWe can generally rely on the RNG in Python and R to give reasonable set of\npseudo-random values. One time when we want to think harder is when doing work with\nRNG in parallel on multiple processors. The worst thing that could\nhappen is that one sets things up in such a way that every process is\nusing the same sequence of random numbers. This could happen if you\nmistakenly set the same seed in each process, e.g., using\n`np.random.seed(1)` on every process. Numpy now provides some nice functionality\nfor parallel RNG, with more details given in the [SCF parallelization tutorial](https://berkeley-scf.github.io/tutorial-parallelization/parallel-python#5-random-number-generation-rng-in-parallel).\n\n# 5. Generating random variables\n\nThere are a variety of methods for generating from common distributions\n(normal, gamma, beta, Poisson, t, etc.). Since these tend to be built\ninto Python and R and presumably use good algorithms, we won't go into them. A\nvariety of statistical computing and Monte Carlo books describe the\nvarious methods. Many are built on the relationships between different\ndistributions - e.g., a beta random variable (RV) can be generated from\ntwo gamma RVs.\n\n## Multivariate distributions\n\nThe `mvtnorm` package supplies code for working with the density and CDF\nof multivariate normal and t distributions.\n\nTo generate a multivariate normal, in Unit 10, we'll see the standard\nmethod based on the Cholesky decomposition:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nL = np.linalg.cholesky(covMat) # L is lower-triangular\nx = L @ np.random.normal(size = covMat.shape[0])\n```\n:::\n\n\n\n\nSide note: for a singular covariance matrix we can use the Cholesky with\npivoting, setting as many rows to zero as the rank deficiency. Then when\nwe generate the multivariate normals, they respect the constraints\nimplicit in the rank deficiency. However, you'll need to reorder the\nresulting vector because of the reordering involved in the pivoted\nCholesky.\n\n## Inverse CDF\n\nMost of you know the inverse CDF method. To generate $X\\sim F$ where $F$\nis a CDF and is an invertible function, first generate\n$Z\\sim\\mathcal{U}(0,1)$, then $x=F^{-1}(z)$. For discrete CDFs, one can\nwork with a discretized version. For multivariate distributions, one can\nwork with a univariate marginal and then a sequence of univariate\nconditionals:\n$f(x_{1})f(x_{2}|x_{1})\\cdots f(x_{k}|x_{k-1},\\ldots,x_{1})$, when the\ndistribution allows this analytic decomposition.\n\n## Rejection sampling\n\nThe basic idea of rejection sampling (RS) relies on the introduction of\nan auxiliary variable, $u$. Suppose $X\\sim F$. Then we can write\n$f(x)=\\int_{0}^{f(x)}du$. Thus $f$ is the marginal density of $X$ in the\njoint density, $(X,U)\\sim\\mathcal{U}\\{(x,u):0<u<f(x)\\}$. Now we'd like\nto use this in a way that relies only on evaluating $f(x)$ without\nhaving to draw from $f$.\n\nTo implement this we draw from a larger set and then only keep draws for\nwhich $u<f(x)$. We choose a density, $g$, that is easy to draw from and\nthat can *majorize* $f$, which means there exists a constant $c$ s.t. ,\n$cg(x)\\geq f(x)$ $\\forall x$. In other words we have that $cg(x)$ is an\nupper envelope for $f(x)$. The algorithm is\n\n1.  generate $x\\sim g$\n2.  generate $u\\sim\\mathcal{U}(0,1)$\n3.  if $u\\leq f(x)/cg(x)$ then use $x$; otherwise go back to step 1\n\nThe intuition here is graphical: we generate from under a curve that is\nalways above $f(x)$ and accept only when $u$ puts us under $f(x)$\nrelative to the majorizing density. A key here is that the majorizing\ndensity have fatter tails than the density of interest, so that the\nconstant $c$ can exist. So we could use a $t$ to generate from a normal\nbut not the reverse. We'd like $c$ to be small to reduce the number of\nrejections because it turns out that\n$\\frac{1}{c}=\\frac{\\int f(x)dx}{\\int cg(x)dx}$ is the acceptance\nprobability. This approach works in principle for multivariate densities\nbut as the dimension increases, the proportion of rejections grows,\nbecause more of the volume under $cg(x)$ is above $f(x)$.\n\nIf $f$ is costly to evaluate, we can sometimes reduce calculation using\na lower bound on $f$. In this case we accept if\n$u\\leq f_{\\mbox{low}}(y)/cg_{Y}(y)$. If it is not, then we need to\nevaluate the ratio in the usual rejection sampling algorithm. This is\ncalled squeezing.\n\nOne example of RS is to sample from a truncated normal. Of course we can\njust sample from the normal and then reject, but this can be\ninefficient, particularly if the truncation is far in the tail (a case\nin which inverse CDF suffers from numerical difficulties). Suppose the\ntruncation point is greater than zero. Working with the standardized\nversion of the normal, you can use an translated exponential with lower\nend point equal to the truncation point as the majorizing density\n[(Robert 1995; Statistics and Computing)](https://link.springer.com/article/10.1007/BF00143942). For truncation less than zero, just make the values negative.\n\n## Adaptive rejection sampling (optional)\n\nThe difficulty of RS is finding a good enveloping function. Adaptive\nrejection sampling refines the envelope as the draws occur, in the case\nof a continuous, differentiable, log-concave density. The basic idea\nconsiders the log of the density and involves using tangents or secants\nto define an upper envelope and secants to define a lower envelope for a\nset of points in the support of the distribution. The result is that we\nhave piecewise exponentials (since we are exponentiating from straight\nlines on the log scale) as the bounds. We can sample from the upper\nenvelope based on sampling from a discrete distribution and then the\nappropriate exponential. The lower envelope is used for squeezing. We\nadd points to the set that defines the envelopes whenever we accept a\npoint that requires us to evaluate $f(x)$ (the points that are accepted\nbased on squeezing are not added to the set).\n\n## Importance sampling\n\nImportance sampling (IS) allows us to estimate expected values. It's an\nextension of the simple Monte Carlo sampling we saw at the beginning of the unit, with\nsome commonalities with rejection sampling.\n\n$$\\phi=E_{f}(h(Y))=\\int h(y)\\frac{f(y)}{g(y)}g(y)dy$$ so\n$\\hat{\\phi}=\\frac{1}{m}\\sum_{i}h(y_{i})\\frac{f(y_{i})}{g(y_{i})}$ for\n$y_{i}$ drawn from $g(y)$, where $w_{i}=f(y_{i})/g(y_{i})$ act as\nweights. (Often in Bayesian contexts, we know $f(y)$ only up to a\nnormalizing constant. In this case we need to use\n$w_{i}^{*}=w_{i}/\\sum_{j}w_{j}$.\n\nHere we don't require the majorizing property, just that the densities\nhave common support, but things can be badly behaved if we sample from a\ndensity with lighter tails than the density of interest. So in general\nwe want $g$ to have heavier tails. More specifically for a low variance\nestimator of $\\phi$, we would want that $f(y_{i})/g(y_{i})$ is large\nonly when $h(y_{i})$ is very small, to avoid having overly influential\npoints.\n\nThis suggests we can reduce variance in an IS context by oversampling\n$y$ for which $h(y)$ is large and undersampling when it is small, since\n$\\mbox{Var}(\\hat{\\phi})=\\frac{1}{m}\\mbox{Var}(h(Y)\\frac{f(Y)}{g(Y)})$.\nAn example is that if $h$ is an indicator function that is 1 only for\nrare events, we should oversample rare events and then the IS estimator\ncorrects for the oversampling.\n\nWhat if we actually want a sample from $f$ as opposed to estimating the\nexpected value above? We can draw $y$ from the unweighted sample,\n$\\{y_{i}\\}$, with weights $\\{w_{i}\\}$. This is called sampling\nimportance resampling (SIR).\n\n## Ratio of uniforms (optional)\n\nIf $U$ and $V$ are uniform in $C=\\{(u,v):\\,0\\leq u\\leq\\sqrt{f(v/u)}$\nthen $X=V/U$ has density proportion to $f$. The basic algorithm is to\nchoose a rectangle that encloses $C$ and sample until we find\n$u\\leq f(v/u)$. Then we use $x=v/u$ as our RV. The larger region\nenclosing $C$ is the majorizing region and a simple approach (if\n$f(x)$and $x^{2}f(x)$ are bounded in $C$) is to choose the rectangle,\n$0\\leq u\\leq\\sup_{x}\\sqrt{f(x)}$,\n$\\inf_{x}x\\sqrt{f(x)}\\leq v\\leq\\sup_{x}x\\sqrt{f(x)}$.\n\nOne can also consider truncating the rectangular region, depending on\nthe features of $f$.\n\nMonahan recommends the ratio of uniforms, particularly a version for\ndiscrete distributions (p. 323 of the 2nd edition).\n",
    "supporting": [
      "unit9-sim_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}