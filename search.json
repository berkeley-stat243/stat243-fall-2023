[
  {
    "objectID": "publish.html",
    "href": "publish.html",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "We use Quarto with GitHub Actions to publish the materials as the course website. All commits should be done to main and not to gh-pages.\nThis documents creation of new pages that require Python or R computation. We render the source document locally, with ‘freeze’ set on the document so that results are stored in _freeze and don’t need to be re-run on every commit.\n\nCopy the preamble from an existing units/*.qmd file.\nUpdate _quarto.yml to reflect the new content (unless you don’t yet want it discoverable online).\nRun quarto render &lt;new-Rmd-or-qmd&gt; locally, which will store computations in the _freeze directory.\nCommit the new page and all changes to the _freeze dir including .json and figure (.png/pdf) files.\nPush to GitHub and the publish action should run fairly quickly via GitHub actions.\n\nTo set up the website for a new course/year, one needs to run quarto publish from within main initially.\nNotes:\n2023-10-25: Today (and on some past occasions), figures don’t show up in the html because the relevant png files are not committed/being deleted from units/unitX_files. Manually adding to gh-pages works for a given commit but then later render/publish causes a git rm of the files. I think what may be the solution is to make sure not to do git commit -am if a git rm of the png files is staged. Instead just use git add on the various files and git commit -m. May also need to git restore the files to be deleted so that later git commit -am' doesn’t cause deletion.\n2023-09-07: GHA can fail with messages about nbformat. Can often fix by re-rendering the problematic qmd. I think this is happening when commits are made to a qmd without rendering that updates the freeze files.\n2023-08-29: when using knitr engine with unit3-bash.qmd (so that one can work with bash chunks), some GHA runs are complaining about missing rmarkdown. But then it sometimes works. Trying to install rmarkdown leads to a permission issue in the system directory that the R package is being installed into. If try to use jupyter engine with bash chunks, you probably need a Jupyter bash kernel, but I am still investigating."
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office hours",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (see Ed Discussion post for link))\n\nTuesday 11 am - noon\nWednesday 3 pm - 4 pm\nThursday 12:30 - 1:30 pm\ngenerally available immediately after class\nfeel free to schedule an appointment or to drop by if my door is open\n\nDeeb:\n\nMonday 4-5:30 pm (Evans 444)\nFridays during unused section time, generally 2-3 pm and 4-4:30 pm (Evans 340)"
  },
  {
    "objectID": "units/unit9-sim.html",
    "href": "units/unit9-sim.html",
    "title": "Simulation",
    "section": "",
    "text": "PDF\nReferences:\nMany (most?) statistical papers include a simulation (i.e., Monte Carlo) study. Many papers on machine learning methods also include a simulation study. The basic idea is that closed-form mathematical analysis of the properties of a statistical or machine learning method/model is often hard to do. Even if possible, it usually involves approximations or simplifications. A canonical situation in statistics is that we have an asymptotic result and we want to know what happens in finite samples, but often we do not even have the asymptotic result. Instead, we can estimate mathematical expressions using random numbers. So we design a simulation study to evaluate the method/model or compare multiple methods. The result is that the researcher carries out an experiment (on the computer, sometimes called in silico), generally varying different factors to see what has an effect on the outcome of interest.\nThe basic strategy generally involves simulating data and then using the method(s) on the simulated data, summarizing the results to assess/compare the method(s).\nMost simulation studies aim to approximate an integral, generally an expected value (mean, bias, variance, MSE, probability, etc.). In low dimensions, methods such as Gaussian quadrature are best for estimating an integral but these methods don’t scale well, so in higher dimensions (e.g., the usual situation with \\(n\\) observations) we often use Monte Carlo techniques.\nTo be more concrete:"
  },
  {
    "objectID": "units/unit9-sim.html#motivating-example",
    "href": "units/unit9-sim.html#motivating-example",
    "title": "Simulation",
    "section": "Motivating example",
    "text": "Motivating example\nLet’s consider linear regression, with observations \\(Y=(y_{1},y_{2},\\ldots,y_{n})\\) and an \\(n\\times p\\) matrix of predictors/covariates/features/variables \\(X\\), where \\(\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}Y\\). If we assume that we have \\(EY=X\\beta\\) and \\(\\mbox{Var}(Y)=\\sigma^{2}I\\), then we can determine analytically that we have \\[\\begin{aligned}\nE\\hat{\\beta} & = & \\beta\\\\\n\\mbox{Var}(\\hat{\\beta})=E((\\hat{\\beta}-E\\hat{\\beta})^{2}) & = & \\sigma^{2}(X^{\\top}X)^{-1}\\\\\n\\mbox{MSPE}(Y^{*})=E(Y^{*}-\\hat{Y})^{2}) & = & \\sigma^{2}(1+X^{*\\top}(X^{\\top}X)^{-1}X^{*}).\\end{aligned}\\] where \\(Y^{*}\\)is some new observation we’d like to predict given \\(X^{*}\\).\nBut suppose that we’re interested in the properties of standard regression estimation when in reality the mean is not linear in \\(X\\) or the properties of the errors are more complicated than having independent homoscedastic errors. (This is always the case, but the issue is how far from the truth the standard assumptions are.) Or suppose we have a modified procedure to produce \\(\\hat{\\beta}\\), such as a procedure that is robust to outliers. In those cases, we cannot compute the expectations above analytically.\nInstead we decide to use a Monte Carlo estimate. To keep the notation more simple, let’s just consider one element of the vector \\(\\beta\\) (i.e., one of the regression coefficients) and continue to call that \\(\\beta\\). If we randomly generate \\(m\\) different datasets from some distribution \\(f\\), and \\(\\hat{\\beta}_{i}\\) is the estimated coefficient based on the \\(i\\)th dataset: \\(Y_{i}=(y_{i1},y_{i2},\\ldots,y_{in})\\), then we can estimate \\(E\\hat{\\beta}\\) under that distribution \\(f\\) as \\[\\hat{E}(\\hat{\\beta})=\\bar{\\hat{\\beta}}=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\beta}_{i}\\] Or to estimate the variance, we have \\[\\widehat{\\mbox{Var}}(\\hat{\\beta})=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{\\beta}_{i}-\\bar{\\hat{\\beta}})^{2}.\\] In evaluating the performance of regression under non-standard conditions or the performance of our robust regression procedure, what decisions do we have to make to be able to carry out our Monte Carlo procedure?\nNext let’s think about Monte Carlo methods in general."
  },
  {
    "objectID": "units/unit9-sim.html#monte-carlo-mc-basics",
    "href": "units/unit9-sim.html#monte-carlo-mc-basics",
    "title": "Simulation",
    "section": "Monte Carlo (MC) basics",
    "text": "Monte Carlo (MC) basics\n\nMonte Carlo overview\nThe basic idea is that we often want to estimate \\(\\phi\\equiv E_{f}(h(Y))\\) for \\(Y\\sim f\\). Note that if \\(h\\) is an indicator function, this includes estimation of probabilities, e.g., for a scalar \\(Y\\), we have \\(p=P(Y\\leq y)=F(y)=\\int_{-\\infty}^{y}f(t)dt=\\int I(t\\leq y)f(t)dt=E_{f}(I(Y\\leq y))\\). We would estimate variances or MSEs by having \\(h\\) involve squared terms.\nWe get an MC estimate of \\(\\phi\\) based on an iid sample of a large number of values of \\(Y\\) from \\(f\\): \\[\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i}),\\] which is justified by the Law of Large Numbers: \\[\\lim_{m\\to\\infty}\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=E_{f}h(Y).\\]\nNote that in most simulation studies, \\(Y\\) is an entire dataset (predictors/covariates), and the “iid sample” means generating \\(m\\) different datasets from \\(f\\), i.e., \\(Y_{i}\\in\\{Y_{1},\\ldots,Y_{m}\\}\\) not \\(m\\) different scalar values. If the dataset has \\(n\\) observations, then \\(Y_{i}=(Y_{i1},\\ldots,Y_{in})\\).\n\nBack to the regression example\nLet’s relate that back to our regression example. In that particular case, if we’re interested in whether the regression estimator is biased, we want to know: \\[\\phi=E\\hat{\\beta},\\] where \\(h(Y) = \\hat{\\beta}(Y)\\). We can use the Monte Carlo estimate of \\(\\phi\\): \\[\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\beta}_{i}=\\widehat{E(\\hat{\\beta})}.\\]\nIf we are interested in the variance of the regression estimator, we have\n\\[\\phi=\\mbox{Var}(\\hat{\\beta})=E_{f}((\\hat{\\beta}-E\\hat{\\beta})^{2})\\] and we can use the Monte Carlo estimate of \\(\\phi\\): \\[\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}h(Y_{i})=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{\\beta}_{i}-E\\hat{\\beta})^{2}=\\widehat{\\mbox{Var}(\\hat{\\beta)}}\\] where \\[h(Y)=(\\hat{\\beta}-E\\hat{\\beta})^{2}.\\]\nFinally note that we also need to use the Monte Carlo estimate of \\(E\\hat{\\beta}\\) in the Monte Carlo estimation of the variance.\nWe might also be interested in the coverage of a confidence interval. In that case we have \\[h(Y)=1_{\\beta\\in CI(Y)}\\] and we can estimate the coverage as \\[\\hat{\\phi}=\\frac{1}{m}\\sum_{i=1}^{m}1_{\\beta\\in CI(y_{i})}.\\] Of course we want that \\(\\hat{\\phi}\\approx1-\\alpha\\) for a \\(100(1-\\alpha)\\) confidence interval. In the standard case of a 95% interval we want \\(\\hat{\\phi}\\approx0.95\\).\n\n\n\nSimulation uncertainty (i.e., Monte Carlo uncertainty)\nSince \\(\\hat{\\phi}\\) is simply an average of \\(m\\) identically-distributed values, \\(h(Y_{1}),\\ldots,h(Y_{m})\\), the simulation variance of \\(\\hat{\\phi}\\) is \\(\\mbox{Var}(\\hat{\\phi})=\\sigma^{2}/m\\), with \\(\\sigma^{2}=\\mbox{Var}(h(Y))\\). An estimator of \\(\\sigma^{2}=E_{f}((h(Y)-\\phi)^{2})\\) is \\[\\begin{aligned}\n\\hat{\\sigma}^{2} & = & \\frac{1}{m-1}\\sum_{i=1}^{m}(h(Y_{i})-\\hat{\\phi})^{2}\\end{aligned}\\] So our MC simulation error is based on \\[\\widehat{\\mbox{Var}}(\\hat{\\phi})=\\frac{\\hat{\\sigma}^{2}}{m}=\\frac{1}{m(m-1)}\\sum_{i=1}^{m}(h(Y_{i})-\\hat{\\phi})^{2}.\\] Note that this is particularly confusing if we have \\(\\hat{\\phi}=\\widehat{\\mbox{Var}(\\hat{\\beta})}\\) because then we have \\(\\widehat{\\mbox{Var}}(\\hat{\\phi})=\\widehat{\\mbox{Var}}(\\widehat{\\mbox{Var}(\\hat{\\beta})})\\)!\nThe simulation variance is \\(O(\\frac{1}{m})\\) because we have \\(m^{2}\\) in the denominator and a sum over \\(m\\) terms in the numerator.\nNote that in the simulation setting, the randomness in the system is very well-defined (as it is in survey sampling, but unlike in most other applications of statistics), because it comes from the RNG that we perform as part of our attempt to estimate \\(\\phi\\). Happily, we are in control of \\(m\\), so in principle we can reduce the simulation error to as little as we desire. Unhappily, as usual, the simulation standard error goes down with the square root of \\(m\\).\n\nImportant: This is the uncertainty in our simulation-based estimate of some quantity (expectation) of interest. It is NOT the statistical uncertainty in a problem.\n\n\nBack to the regression example\nSome examples of simulation variances we might be interested in in the regression example include:\n\nUncertainty in our estimate of bias: \\(\\widehat{\\mbox{Var}}(\\hat{E}(\\hat{\\beta})-\\beta)\\).\nUncertainty in the estimated variance of the estimated coefficient: \\(\\widehat{\\mbox{Var}}(\\widehat{\\mbox{Var}}(\\hat{\\beta}))\\).\nUncertainty in the estimated mean square prediction error: \\(\\widehat{\\mbox{Var}}(\\widehat{\\mbox{MSPE}}(Y^{*}))\\).\n\nIn all cases we have to estimate the simulation variance, hence the \\(\\widehat{\\mbox{Var}}()\\) notation.\n\n\n\nFinal notes\nSometimes the \\(Y_{i}\\) are generated in a dependent fashion (e.g., sequential MC or MCMC), in which case this variance estimator, \\(\\widehat{\\mbox{Var}}(\\hat{\\phi})\\) does not hold because the samples are not IID, but the estimator \\(\\hat{\\phi}\\) is still a valid, unbiased estimator of \\(\\phi\\)."
  },
  {
    "objectID": "units/unit9-sim.html#variance-reduction-optional",
    "href": "units/unit9-sim.html#variance-reduction-optional",
    "title": "Simulation",
    "section": "Variance reduction (optional)",
    "text": "Variance reduction (optional)\nThere are some tools for variance reduction in MC settings. One is importance sampling (see Section 3). Others are the use of control variates and antithetic sampling. I haven’t personally run across these latter in practice, so I’m not sure how widely used they are and won’t go into them here.\nIn some cases we can set up natural strata, for which we know the probability of being in each stratum. Then we would estimate \\(\\mu\\) for each stratum and combine the estimates based on the probabilities. The intuition is that we remove the variability in sampling amongst the strata from our simulation.\nAnother strategy that comes up in MCMC contexts is Rao-Blackwellization. Suppose we want to know \\(E(h(X))\\) where \\(X=\\{X_{1},X_{2}\\}\\). Iterated expectation tells us that \\(E(h(X))=E(E(h(X)|X_{2})\\). If we can compute \\(E(h(X)|X_{2})=\\int h(x_{1},x_{2})f(x_{1}|x_{2})dx_{1}\\) then we should avoid introducing stochasticity related to the \\(X_{1}\\) draw (since we can analytically integrate over that) and only average over stochasticity from the \\(X_{2}\\) draw by estimating \\(E_{X_{2}}(E(h(X)|X_{2})\\). The estimator is \\[\\hat{\\mu}_{RB}=\\frac{1}{m}\\sum_{i=1}^{m}E(h(X)|X_{2,i})\\] where we either draw from the marginal distribution of \\(X_{2}\\), or equivalently, draw \\(X\\), but only use \\(X_{2}\\). Our MC estimator averages over the simulated values of \\(X_{2}\\). This is called Rao-Blackwellization because it relates to the idea of conditioning on a sufficient statistic. It has lower variance because the variance of each term in the sum of the Rao-Blackwellized estimator is \\(\\mbox{Var}(E(h(X)|X_{2})\\), which is less than the variance in the usual MC estimator, \\(\\mbox{Var}(h(X))\\), based on the usual iterated variance formula: \\(V(X)=E(V(X|Y))+V(E(X|Y))\\Rightarrow V(E(X|Y))&lt;V(X)\\)."
  },
  {
    "objectID": "units/unit9-sim.html#basic-steps-of-a-simulation-study",
    "href": "units/unit9-sim.html#basic-steps-of-a-simulation-study",
    "title": "Simulation",
    "section": "Basic steps of a simulation study",
    "text": "Basic steps of a simulation study\n\nSpecify what makes up an individual experiment (i.e., the individual simulated dataset) given a specific set of inputs: sample size, distribution(s) to use, parameter values, statistic of interest, etc. In other words, exactly how would you generate one simulated dataset?\nOften you’ll want to see how your results will vary if you change some of the inputs; e.g., sample sizes, parameter values, data generating mechanisms. So determine what factors you’ll want to vary. Each unique combination of input values will be a scenario.\nWrite code to carry out the individual experiment and return the quantity of interest, with arguments to your code being the inputs that you want to vary.\nFor each combination of inputs you want to explore (each scenario), repeat the experiment \\(m\\) times. Note this is an easily parallel calculation (in both the data generating dimension and the inputs dimension(s)).\nSummarize the results for each scenario, quantifying simulation uncertainty.\nReport the results in graphical or tabular form.\n\nOften a simulation study will compare multiple methods, so you’ll need to do steps 3-6 for each method."
  },
  {
    "objectID": "units/unit9-sim.html#various-considerations",
    "href": "units/unit9-sim.html#various-considerations",
    "title": "Simulation",
    "section": "Various considerations",
    "text": "Various considerations\nSince a simulation study is an experiment, we should use the same principles of design and analysis we would recommend when advising a practicioner on setting up a scientific experiment.\nThese include efficiency, reporting of uncertainty, reproducibility and documentation.\nIn generating the data for a simulation study, we want to think about what structure real data would have that we want to mimic in the simulation study: distributional assumptions, parameter values, dependence structure, outliers, random effects, sample size (\\(n\\)), etc.\nAll of these may become input variables in a simulation study. Often we compare two or more statistical methods conditioning on the data context and then assess whether the differences between methods vary with the data context choices. E.g., if we compare an MLE to a robust estimator, which is better under a given set of choices about the data generating mechanism and how sensitive is the comparison to changing the features of the data generating mechanism? So the “treatment variable” is the choice of statistical method. We’re then interested in sensitivity to the conditions (different input values).\nOften we can have a large number of replicates (\\(m\\)) because the simulation is fast on a computer, so we can sometimes reduce the simulation error to essentially zero and thereby avoid reporting uncertainty. To do this, we need to calculate the simulation standard error, generally, \\(s/\\sqrt{m}\\) and see how it compares to the effect sizes. This is particularly important when reporting on the bias of a statistical method.\nWe might denote the data, which could be the statistical estimator under each of two methods as \\(Y_{ijklq}\\), where \\(q\\) indexes treatment, \\(j,k,l\\) index different additional input variables, and \\(i\\in\\{1,\\ldots,m\\}\\) indexes the replicate. E.g., \\(j\\) might index whether the data are from a t or normal, \\(k\\) the value of a parameter, and \\(l\\) the dataset sample size (i.e., different levels of \\(n\\)).\nOne can think about choosing \\(m\\) based on a basic power calculation, though since we can always generate more replicates, one might just proceed sequentially and stop when the precision of the results is sufficient.\nWhen comparing methods, it’s best to use the same simulated datasets for each level of the treatment variable and to do an analysis that controls for the dataset (i.e., for the random numbers used), thereby removing some variability from the error term. A simple example is to do a paired analysis, where we look at differences between the outcome for two statistical methods, pairing based on the simulated dataset.\nOne can even use the “same” random number generation for the replicates under different conditions. E.g., in assessing sensitivity to a \\(t\\) vs. normal data generating mechanism, we might generate the normal RVs and then for the \\(t\\) use the same random numbers, in the sense of using the same quantiles of the \\(t\\) as were generated for the normal - this is pretty easy, as seen below. This helps to control for random differences between the datasets.\n\nfrom scipy.stats import t, norm\n\ndevs = np.random.normal(size=100)\ntdevs = t.ppf(norm.cdf(devs), df=1)\n\nplt.scatter(devs, tdevs)\nplt.xlabel('devs'); plt.ylabel('tdevs')\nplt.plot([min(devs), max(devs)], [min(devs), max(devs)], color='red')\nplt.show()"
  },
  {
    "objectID": "units/unit9-sim.html#experimental-design-optional",
    "href": "units/unit9-sim.html#experimental-design-optional",
    "title": "Simulation",
    "section": "Experimental Design (optional)",
    "text": "Experimental Design (optional)\nA typical context is that one wants to know the effect of multiple input variables on some outcome. Often, scientists, and even statisticians doing simulation studies will vary one input variable at a time. As we know from standard experimental design, this is inefficient.\nThe standard strategy is to discretize the inputs, each into a small number of levels. If we have a small enough number of inputs and of levels, we can do a full factorial design (potentially with replication). For example if we have three inputs and three levels each, we have \\(3^{3}\\) different treatment combinations. Choosing the levels in a reasonable way is obviously important.\nAs the number of inputs and/or levels increases to the point that we can’t carry out the full factorial, a fractional factorial is an option. This carefully chooses which treatment combinations to omit. The goal is to achieve balance across the levels in a way that allows us to estimate lower level effects (in particular main effects) but not all high-order interactions. What happens is that high-order interactions are aliased to (confounded with) lower-order effects. For example you might choose a fractional factorial design so that you can estimate main effects and two-way interactions but not higher-order interactions.\nIn interpreting the results, I suggest focusing on the decomposition of sums of squares and not on statistical significance. In most cases, we expect the inputs to have at least some effect on the outcome, so the null hypothesis is a straw man. Better to assess the magnitude of the impacts of the different inputs.\nWhen one has a very large number of inputs, one can use the Latin hypercube approach to sample in the input space in a uniform way, spreading the points out so that each input is sampled uniformly. Assume that each input is \\(\\mathcal{U}(0,1)\\) (one can easily transform to whatever marginal distributions you want). Suppose that you can run \\(m\\) samples. Then for each input variable, we divide the unit interval into \\(m\\) bins and randomly choose the order of bins and the position within each bin. This is done independently for each variable and then combined to give \\(m\\) samples from the input space. We would then analyze main effects and perhaps two-way interactions to assess which inputs seem to be most important.\nEven amongst statisticians, taking an experimental design approach to a simulation study is not particularly common, but it’s worth considering."
  },
  {
    "objectID": "units/unit9-sim.html#computational-efficiency",
    "href": "units/unit9-sim.html#computational-efficiency",
    "title": "Simulation",
    "section": "Computational efficiency",
    "text": "Computational efficiency\nParallel processing is often helpful for simulation studies. The reason is that simulation studies are embarrassingly parallel - we can send each replicate to a different computer processor and then collect the results back, and the speedup should scale directly with the number of processors we used. Since we often need to some sort of looping, writing code in C/C++ and compiling and linking to the code from Python may also be a good strategy, albeit one not covered in this course.\nA handy function in Python is itertools.product to get all combinations of a set of vectors.\n\nimport itertools\n\nthetaLevels = [\"low\", \"med\", \"hi\"]\nn = [10, 100, 1000]\ntVsNorm = [\"t\", \"norm\"]\nlevels = list(itertools.product(thetaLevels, tVsNorm, n))"
  },
  {
    "objectID": "units/unit9-sim.html#analysis-and-reporting",
    "href": "units/unit9-sim.html#analysis-and-reporting",
    "title": "Simulation",
    "section": "Analysis and reporting",
    "text": "Analysis and reporting\nOften results are reported simply in tables, but it can be helpful to think through whether a graphical representation is more informative (sometimes it’s not or it’s worse, but in some cases it may be much better). Since you’ll often have a variety of scenarios to display, using trellis plots in ggplot2 via the facet_wrap function will often be a good approach to display how results vary as a function of multiple inputs in R. In Python, it looks like there are various ways (RPlot in pandas, seaborn, plotly), but I don’t know what the most standard way is.\nYou should set the seed when you start the experiment, so that it’s possible to replicate it. It’s also a good idea to save the current value of the seed whenever you save interim results, so that you can restart simulations (this is particularly helpful for MCMC) at the exact point you left off, including the random number sequence.\nTo enhance reproducibility, it’s good practice to post your simulation code (and potentially simulated data) on GitHub, on your website, or as supplementary material with the journal. Another person should be able to fully reproduce your results, including the exact random number generation that you did (e.g., you should provide code for how you set the random seed for your randon number generator).\nMany journals are requiring increasingly detailed documentation of the code and data used in your work, including code and data for simulations. Here are the American Statistical Association’s requirements on documenting computations in its journals:\n“The ASA strongly encourages authors to submit datasets, code, other programs, and/or extended appendices that are directly relevant to their submitted articles. These materials are valuable to users of the ASA’s journals and further the profession’s commitment to reproducible research. Whenever a dataset is used, its source should be fully documented and the data should be made available as on online supplement. Exceptions for reasons of security or confidentiality may be granted by the Editor. Whenever specific code has been used to implement or illustrate the results of a paper, that code should be made available if possible. [.…snip.…] Articles reporting results based on computation should provide enough information so that readers can evaluate the quality of the results. Such information includes estimated accuracy of results, as well as descriptions of pseudorandom-number generators, numerical algorithms, programming languages, and major software components used.”"
  },
  {
    "objectID": "units/unit9-sim.html#generating-random-uniforms-on-a-computer",
    "href": "units/unit9-sim.html#generating-random-uniforms-on-a-computer",
    "title": "Simulation",
    "section": "Generating random uniforms on a computer",
    "text": "Generating random uniforms on a computer\nGenerating a sequence of random standard uniforms is the basis for all generation of random variables, since random uniforms (either a single one or more than one) can be used to generate values from other distributions. Most random numbers on a computer are pseudo-random. The numbers are chosen from a deterministic stream of numbers that behave like random numbers but are actually a finite sequence (recall that both integers and real numbers on a computer are actually discrete and there are finitely many distinct values), so it’s actually possible to get repeats. The seed of a RNG is the place within that sequence where you start to use the pseudo-random numbers.\n\nSequential congruential generators\nMany RNG methods are sequential congruential methods. The basic idea is that the next value is \\[u_{k}=f(u_{k-1},\\ldots,u_{k-j})\\mbox{mod}\\,m\\] for some function, \\(f\\), and some positive integer \\(m\\) . Often \\(j=1\\). mod just means to take the remainder after dividing by \\(m\\). One then generates the random standard uniform value as \\(u_{k}/m\\), which by construction is in \\([0,1]\\). For our discussion below, it is important to distinguish the state (\\(u\\)) from the output of the RNG.\nGiven the construction, such sequences are periodic if the subsequence ever reappears, which is of course guaranteed because there is a finite number of possible subsequence values given that all the \\(u_{k}\\) values are remainders of divisions by a fixed number . One key to a good random number generator (RNG) is to have a very long period.\nAn example of a sequential congruential method is a basic linear congruential generator: \\[u_{k}=(au_{k-1}+c)\\mbox{mod}\\,m\\] with integer \\(a\\), \\(m\\), \\(c\\), and \\(u_{k}\\) values. (Note that in some cases \\(c=0\\), in which case the periodicity can’t exceed \\(m-1\\) as the method is then set up so that we never get \\(u_{k}=0\\) as this causes the algorithm to break.) The seed is the initial state, \\(u_{0}\\) - i.e., the point in the sequence at which we start. By setting the seed you guarantee reproducibility since given a starting value, the sequence is deterministic. In general \\(a\\), \\(c\\) and \\(m\\) are chosen to be ‘large’. The standard values of \\(m\\) are Mersenne primes, which have the form \\(2^{p}-1\\) (but these are not prime for all \\(p\\)). Here’s an example of a linear congruential sampler (with \\(c=0\\)):\n\nn = 100\na = 171\nm = 30269\n\nu = np.empty(n)\nu[0] = 7306\n\nfor i in range(1, n):\n    u[i] = (a * u[i-1]) % m\n\nu = u / m\nuFromNP = np.random.uniform(size = n)\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(range(1, n+1), u)\nplt.title(\"manual\")\nplt.xlabel(\"Index\"); plt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 2)\nplt.plot(range(1, n+1), uFromNP)\nplt.title(\"numpy\")\nplt.xlabel(\"Index\"); plt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 3)\nplt.hist(u, bins=25)\n\n(array([6., 3., 6., 4., 1., 4., 6., 3., 4., 9., 4., 5., 1., 1., 2., 7., 1.,\n       2., 5., 2., 6., 5., 4., 4., 5.]), array([0.01833559, 0.05743434, 0.09653309, 0.13563183, 0.17473058,\n       0.21382933, 0.25292808, 0.29202683, 0.33112557, 0.37022432,\n       0.40932307, 0.44842182, 0.48752057, 0.52661931, 0.56571806,\n       0.60481681, 0.64391556, 0.68301431, 0.72211305, 0.7612118 ,\n       0.80031055, 0.8394093 , 0.87850804, 0.91760679, 0.95670554,\n       0.99580429]), &lt;BarContainer object of 25 artists&gt;)\n\nplt.xlabel(\"Value\"); plt.ylabel(\"Frequency\")\n\nplt.subplot(2, 2, 4)\nplt.hist(uFromNP, bins=25)\n\n(array([3., 5., 7., 2., 4., 3., 7., 5., 3., 5., 5., 5., 1., 4., 4., 4., 5.,\n       3., 8., 0., 4., 2., 1., 2., 8.]), array([0.0049552 , 0.04468369, 0.08441218, 0.12414066, 0.16386915,\n       0.20359764, 0.24332612, 0.28305461, 0.3227831 , 0.36251159,\n       0.40224007, 0.44196856, 0.48169705, 0.52142553, 0.56115402,\n       0.60088251, 0.640611  , 0.68033948, 0.72006797, 0.75979646,\n       0.79952494, 0.83925343, 0.87898192, 0.91871041, 0.95843889,\n       0.99816738]), &lt;BarContainer object of 25 artists&gt;)\n\nplt.xlabel(\"Value\"); plt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nA wide variety of different RNG have been proposed. Many have turned out to have substantial defects based on tests designed to assess if the behavior of the RNG mimics true randomness. Some of the behavior we want to ensure is uniformity of each individual random deviate, independence of sequences of deviates, and multivariate uniformity of subsequences. One test of a RNG that many RNGs don’t perform well on is to assess the properties of \\(k\\)-tuples - subsequences of length \\(k\\), which should be independently distributed in the \\(k\\)-dimensional unit hypercube. Unfortunately, linear congruential methods produce values that lie on a simple lattice in \\(k\\)-space, i.e., the points are not selected from \\(q^{k}\\) uniformly spaced points, where \\(q\\) is the the number of unique values. Instead, points often lie on parallel lines in the hypercube.\nCombining generators can yield better generators. The Wichmann-Hill is an option in R and is a combination of three linear congruential generators with \\(a=\\{171,172,170\\}\\), \\(m=\\{30269,30307,30323\\}\\), and \\(u_{i}=(x_{i}/30269+y_{i}/30307+z_{i}/30323)\\mbox{mod}\\,1\\) where \\(x\\), \\(y\\), and \\(z\\) are generated from the three individual generators. Let’s mimic the Wichmann-Hill manually:\n\nRNGkind(\"Wichmann-Hill\")\nset.seed(1)\nsaveSeed &lt;- .Random.seed\nuFromR &lt;- runif(10)\na &lt;- c(171, 172, 170)\nm &lt;- c(30269, 30307, 30323)\nxyz &lt;- matrix(NA, nr = 10, nc = 3)\nxyz[1, ] &lt;- (a * saveSeed[2:4]) %% m\nfor( i in 2:10)\n    xyz[i, ] &lt;- (a * xyz[i-1, ]) %% m\nfor(i in 1:10)\n    print(c(uFromR[i],sum(xyz[i, ]/m)%%1))\n\n[1] 0.1297134 0.1297134\n[1] 0.9822407 0.9822407\n[1] 0.8267184 0.8267184\n[1] 0.242355 0.242355\n[1] 0.8568853 0.8568853\n[1] 0.8408788 0.8408788\n[1] 0.3421633 0.3421633\n[1] 0.7062672 0.7062672\n[1] 0.6212432 0.6212432\n[1] 0.6537663 0.6537663\n\n## we should be able to recover the current value of the seed\nxyz[10, ]\n\n[1] 24279 14851 10966\n\n.Random.seed[2:4]\n\n[1] 24279 14851 10966\n\n\n\n\nPCG generators\nSomewhat recently O’Neal (2014) proposed a new approach to using the linear congruential generator in a way that gives much better performance than the basic versions of such generators described above. This approach is now the default random number generator in numpy (see numpy.random.default_rng()), called the PCG-64 generator. ‘PCG’ stands for permutation congruential generator and encompasses a family of such generators.\nThe idea of the PCG approach goes like this:\n\nLinear congruential generators (LCG) are simple and fast, but for small values of \\(m\\) don’t perform all that well statistically, in particular having values on a lattice as discussed above.\nUsing a large value of \\(m\\) can actually give good statistical performance.\nApplying a technique called permutation functions to the state of the LCG in order to produce the output at each step (the random value returned to the user) can improve the statistical performance even further.\n\nInstead of using relatively small values of \\(m\\) seen above, in the PCG approach one uses \\(m=2^k\\), for ‘large enough’ \\(k\\), usually 64 or 128. It turns out that if \\(m=2^k\\) then the period of the \\(b\\)th bit of the state is \\(2^b\\) where \\(b=1\\) is the right-most bit. Small periods are of course bad for RNG, so the bits with small period cause the LCG to not perform well. Thankfully, one simple fix is simply to discard some number of the right-most bits (this is one form of bit shift). Note that if one does this, the output of the RNG is based on a subset of the bits, which means that the number of unique values that can be generated is smaller than the period. This is not a problem given we start with a state with a large number of bits (64 or 128 as mentioned above).\nO’Neal then goes further; instead of simply discarding bits, she proposes to either shift bits by a random amount or rotate bits by a random amount, where the random amount is determined by a small number of the initial bits. This improves the statistical performance of the generator. The choice of how to do this gives the various members of the PCG family of generators. The details are fairly complicated (the PCG paper is 50-odd pages) and not important for our purposes here.\n\n\nMersenne Twister\nA commonly used generator (including in both R and Python) is the Mersenne Twister. It’s the default in R and “sort of” the default in numpy (see next section for what I mean by “sort of”).\nThe Mersenne Twister has some theoretical support, has performed reasonably on standard tests of pseudorandom numbers and has been used without evidence of serious failure. (But note that O’Neal criticizes it in her technical report.) Plus it’s fast (because bitwise operations are fast). The particular Mersenne twister used has a periodicity of \\(2^{19937}-1\\approx10^{6000}\\). Practically speaking this means that if we generated one random uniform per nanosecond for 10 billion years, then we would generate \\(10^{25}\\) numbers, well short of the period. So we don’t need to worry about the periodicity! The state (sometimes also called the seed) for the Mersenne twister is a set of 624 32-bit integers plus a position in the set, where the position is .Random.seed[2] in R and (I think) np.random.get_state()[2] in Python.\nThe Mersenne twister is in the class of generalized feedback shift registers (GFSR). The basic idea of a GFSR is to come up with a deterministic generator of bits (i.e., a way to generate sequences of 0s and 1s), \\(B_{i}\\), \\(i=1,2,3,\\ldots\\). The pseudo-random numbers are then determined as sequential subsequences of length \\(L\\) from \\(\\{B_{i}\\}\\), considered as a base-2 number and dividing by \\(2^{L}\\) to get a number in \\((0,1)\\). In general the sequence of bits is generated by taking \\(B_{i}\\) to be the exclusive or [i.e., 0+0 = 0; 0 + 1 = 1; 1 + 0 = 1; 1 + 1 = 0] summation of two previous bits further back in the sequence where the lengths of the lags are carefully chosen.\nnumpy provides access to the Mersenne Twister via the MT19937 generator; more on this below. It looks like PCG-64 only became available as of numpy version 1.17.\n\n\nThe period versus the number of unique values generated\nThe output of the PCG-64 is 64 bits while for the Mersenne Twister the output is 32 bits. The result is that the generators generate fewer unique values than their periods. This means you could get duplicated values in long runs, but this does not violate the comment about the periodicity of PCG-64 and Mersenne-Twister being longer than \\(2^{64}\\) and \\(2^{32}\\). Why not? Because the two values after the two duplicated numbers will not be duplicates of each other – as noted previously, there is a distinction between the output presented to the user and the state of the RNG algorithm.\n\n\nThe seed and the state\nSetting the seed picks a position in the periodic sequence of the RNG, i.e., in the state of the RNG. The state can be a single number or something much more complicated. As mentioned above, the state for the Mersenne Twister is a set of 624 32-bit integers plus a position in the set. For the PCG-64 in numpy, the state is two numbers – the actual state and the increment (c above). This means that when the user passes a single number as the seed, there needs to be a procedure that deterministically sets the state based on that single number seed. The details of this are not usually well-documented or viewable by the user.\nIdeally, nearby seeds generally should not correspond to getting sequences from the RNG stream that are closer to each other than far away seeds. According to Gentle (CS, p. 327) the input to set.seed() in R should be an integer, \\(i\\in\\{0,\\ldots,1023\\}\\) , and each of these 1024 values produces positions in the RNG sequence that are “far away” from each other. I don’t see any mention of this in the R documentation for set.seed() and furthermore, you can pass integers larger than 1023 to set.seed(), so I’m not sure how much to trust Gentle’s claim. More on generating parallel streams of random numbers below.\nWhen one invokes a RNG without a seed, RNG implementations generally have a method for choosing a seed (often based on the system clock). The numpy documentation says that it “mixes sources of entropy in a reproducible way” to do this.\nGenerators should give you the same sequence of random numbers, starting at a given seed, whether you ask for a bunch of numbers at once, or sequentially ask for individual numbers.\n\nAdditional notes\nThere have been some attempts to generate truly random numbers based on physical randomness. One that is based on quantum physics is http://www.idquantique.com/true-random-number-generator/quantis-usb-pcie-pci.html. Another approach is based on lava lamps!"
  },
  {
    "objectID": "units/unit9-sim.html#rng-in-python",
    "href": "units/unit9-sim.html#rng-in-python",
    "title": "Simulation",
    "section": "RNG in Python",
    "text": "RNG in Python\n\nChoosing a generator\nIn numpy, the default_rng RNG is PCG-64. It has a period of \\(2^{128}\\) and supports advancing an arbitrary number of steps, as well as \\(2^{127}\\) streams (both useful for generating random numbers when parallelizing). The state of the PCG-64 RNG is represented by two 128-bit unsigned integers, one the actual state and one the value of \\(c\\) (the increment).\nHowever, while the default is PCG-64, simply using the functions available via np.random to generate random numbers seems to actually use the Mersenne Twister, so the meaning of default is tricky.\nI think that this text from help(np.random) explains what is going on:\n   Legacy\n   ------\n    \n   For backwards compatibility with previous versions of numpy before 1.17, the\n   various aliases to the global `RandomState` methods are left alone and do not\n   use the new `Generator` API.\nWe can change to a specific RNG using syntax (the Generator API) like this:\n\nrng = np.random.Generator(np.random.MT19937(seed = 1))  # Mersenne Twister\nrng = np.random.Generator(np.random.PCG64(seed = 1))    # PCG-64\n\nbut below note that there is a simpler way to change to the PCG-64.\nThen to use that generator when doing operations that generate random numbers, we need to use methods accessed via the Generator object (rng here):\n\nrng.random.normal(size = 3)     # Now generate based on chosen generator.\n## np.random.normal(size = 3)   # This will NOT use the chosen generator.\n\nIn R, the default RNG is the Mersenne twister (?RNGkind).\n\n\nUsing the Mersenne Twister\nIf we simply start using numpy or scipy to generate random numbers, we’ll be using the Mersenne Twister. I believe this is what the documentation mentioned above means by “aliases to the global RandomState methods”.\nWe get replicability by setting the seed to a specific value at the beginning of our simulation. We can then set the seed to that same value when we want to replicate the simulation.\n\nnp.random.seed(1)\nnp.random.normal(size = 5)\n\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n\nnp.random.seed(1)\nnp.random.normal(size = 5)\n\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n\n\nWe can also save the state of the RNG and pick up where we left off. So this code will pick where you had left off, ignoring what happened in between saving to saved_state and resetting.\n\nnp.random.seed(1)\nnp.random.normal(size = 5)\n\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n\nsaved_state = np.random.get_state()\nnp.random.normal(size = 5)\n\narray([-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038])\n\n\nNow we’ll do some arbitrary work with random numbers, and see that if we use the saved state we can pick up where we left off above.\n\ntmp = np.random.choice(np.arange(1, 51), size=2000, replace=True) # arbitrary work\n\n## Restore the state.\nnp.random.set_state(saved_state)\nnp.random.normal(size = 5)\n\narray([-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038])\n\n\nIf we look at saved_state, we can confirm it actually corresponds to the Mersenne Twister.\n\n\nUsing PCG64\nTo use the PCG-64, we need to explicitly create and make use of the Generator object (rng here), which is the new numpy approach to handling RNG.\nWe set the seed when setting up the generator via np.random.default_rng(seed) (or np.random.Generator(np.random.PCG64(seed = 1))).\n\nrng = np.random.default_rng(seed = 1)\nrng.normal(size = 5)\n\narray([ 0.34558419,  0.82161814,  0.33043708, -1.30315723,  0.90535587])\n\nrng = np.random.default_rng(seed = 1)\nrng.normal(size = 5)\n\narray([ 0.34558419,  0.82161814,  0.33043708, -1.30315723,  0.90535587])\n\nsaved_state = rng.bit_generator.state\nrng.normal(size = 5)\n\narray([ 0.44637457, -0.53695324,  0.5811181 ,  0.3645724 ,  0.2941325 ])\n\ntmp = rng.choice(np.arange(1, 51), size=2000, replace=True)\nrng.bit_generator.state = saved_state\nrng.normal(size = 5)\n\narray([ 0.44637457, -0.53695324,  0.5811181 ,  0.3645724 ,  0.2941325 ])\n\nsaved_state\n\n{'bit_generator': 'PCG64', 'state': {'state': 216676376075457487203159048251690499413, 'inc': 194290289479364712180083596243593368443}, 'has_uint32': 0, 'uinteger': 0}\n\nsaved_state['state']['state']   # actual state\n\n216676376075457487203159048251690499413\n\nsaved_state['state']['inc']     # increment ('c')\n\n194290289479364712180083596243593368443\n\n\nsaved_state contains the actual state and the value of c, the increment.\nQuestion: how many bits does saved_state['state']['state'] correspond to?"
  },
  {
    "objectID": "units/unit9-sim.html#rng-in-parallel",
    "href": "units/unit9-sim.html#rng-in-parallel",
    "title": "Simulation",
    "section": "RNG in parallel",
    "text": "RNG in parallel\nWe can generally rely on the RNG in Python and R to give reasonable set of pseudo-random values. One time when we want to think harder is when doing work with RNG in parallel on multiple processors. The worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using np.random.seed(1) on every process. Numpy now provides some nice functionality for parallel RNG, with more details given in the SCF parallelization tutorial."
  },
  {
    "objectID": "units/unit9-sim.html#multivariate-distributions",
    "href": "units/unit9-sim.html#multivariate-distributions",
    "title": "Simulation",
    "section": "Multivariate distributions",
    "text": "Multivariate distributions\nThe mvtnorm package supplies code for working with the density and CDF of multivariate normal and t distributions.\nTo generate a multivariate normal, in Unit 10, we’ll see the standard method based on the Cholesky decomposition:\n\nL = np.linalg.cholesky(covMat) # L is lower-triangular\nx = L @ np.random.normal(size = covMat.shape[0])\n\nSide note: for a singular covariance matrix we can use the Cholesky with pivoting, setting as many rows to zero as the rank deficiency. Then when we generate the multivariate normals, they respect the constraints implicit in the rank deficiency. However, you’ll need to reorder the resulting vector because of the reordering involved in the pivoted Cholesky."
  },
  {
    "objectID": "units/unit9-sim.html#inverse-cdf",
    "href": "units/unit9-sim.html#inverse-cdf",
    "title": "Simulation",
    "section": "Inverse CDF",
    "text": "Inverse CDF\nMost of you know the inverse CDF method. To generate \\(X\\sim F\\) where \\(F\\) is a CDF and is an invertible function, first generate \\(Z\\sim\\mathcal{U}(0,1)\\), then \\(x=F^{-1}(z)\\). For discrete CDFs, one can work with a discretized version. For multivariate distributions, one can work with a univariate marginal and then a sequence of univariate conditionals: \\(f(x_{1})f(x_{2}|x_{1})\\cdots f(x_{k}|x_{k-1},\\ldots,x_{1})\\), when the distribution allows this analytic decomposition."
  },
  {
    "objectID": "units/unit9-sim.html#rejection-sampling",
    "href": "units/unit9-sim.html#rejection-sampling",
    "title": "Simulation",
    "section": "Rejection sampling",
    "text": "Rejection sampling\nThe basic idea of rejection sampling (RS) relies on the introduction of an auxiliary variable, \\(u\\). Suppose \\(X\\sim F\\). Then we can write \\(f(x)=\\int_{0}^{f(x)}du\\). Thus \\(f\\) is the marginal density of \\(X\\) in the joint density, \\((X,U)\\sim\\mathcal{U}\\{(x,u):0&lt;u&lt;f(x)\\}\\). Now we’d like to use this in a way that relies only on evaluating \\(f(x)\\) without having to draw from \\(f\\).\nTo implement this we draw from a larger set and then only keep draws for which \\(u&lt;f(x)\\). We choose a density, \\(g\\), that is easy to draw from and that can majorize \\(f\\), which means there exists a constant \\(c\\) s.t. , \\(cg(x)\\geq f(x)\\) \\(\\forall x\\). In other words we have that \\(cg(x)\\) is an upper envelope for \\(f(x)\\). The algorithm is\n\ngenerate \\(x\\sim g\\)\ngenerate \\(u\\sim\\mathcal{U}(0,1)\\)\nif \\(u\\leq f(x)/cg(x)\\) then use \\(x\\); otherwise go back to step 1\n\nThe intuition here is graphical: we generate from under a curve that is always above \\(f(x)\\) and accept only when \\(u\\) puts us under \\(f(x)\\) relative to the majorizing density. A key here is that the majorizing density have fatter tails than the density of interest, so that the constant \\(c\\) can exist. So we could use a \\(t\\) to generate from a normal but not the reverse. We’d like \\(c\\) to be small to reduce the number of rejections because it turns out that \\(\\frac{1}{c}=\\frac{\\int f(x)dx}{\\int cg(x)dx}\\) is the acceptance probability. This approach works in principle for multivariate densities but as the dimension increases, the proportion of rejections grows, because more of the volume under \\(cg(x)\\) is above \\(f(x)\\).\nIf \\(f\\) is costly to evaluate, we can sometimes reduce calculation using a lower bound on \\(f\\). In this case we accept if \\(u\\leq f_{\\mbox{low}}(y)/cg_{Y}(y)\\). If it is not, then we need to evaluate the ratio in the usual rejection sampling algorithm. This is called squeezing.\nOne example of RS is to sample from a truncated normal. Of course we can just sample from the normal and then reject, but this can be inefficient, particularly if the truncation is far in the tail (a case in which inverse CDF suffers from numerical difficulties). Suppose the truncation point is greater than zero. Working with the standardized version of the normal, you can use an translated exponential with lower end point equal to the truncation point as the majorizing density (Robert 1995; Statistics and Computing). For truncation less than zero, just make the values negative."
  },
  {
    "objectID": "units/unit9-sim.html#adaptive-rejection-sampling-optional",
    "href": "units/unit9-sim.html#adaptive-rejection-sampling-optional",
    "title": "Simulation",
    "section": "Adaptive rejection sampling (optional)",
    "text": "Adaptive rejection sampling (optional)\nThe difficulty of RS is finding a good enveloping function. Adaptive rejection sampling refines the envelope as the draws occur, in the case of a continuous, differentiable, log-concave density. The basic idea considers the log of the density and involves using tangents or secants to define an upper envelope and secants to define a lower envelope for a set of points in the support of the distribution. The result is that we have piecewise exponentials (since we are exponentiating from straight lines on the log scale) as the bounds. We can sample from the upper envelope based on sampling from a discrete distribution and then the appropriate exponential. The lower envelope is used for squeezing. We add points to the set that defines the envelopes whenever we accept a point that requires us to evaluate \\(f(x)\\) (the points that are accepted based on squeezing are not added to the set)."
  },
  {
    "objectID": "units/unit9-sim.html#importance-sampling",
    "href": "units/unit9-sim.html#importance-sampling",
    "title": "Simulation",
    "section": "Importance sampling",
    "text": "Importance sampling\nImportance sampling (IS) allows us to estimate expected values. It’s an extension of the simple Monte Carlo sampling we saw at the beginning of the unit, with some commonalities with rejection sampling.\n\\[\\phi=E_{f}(h(Y))=\\int h(y)\\frac{f(y)}{g(y)}g(y)dy\\] so \\(\\hat{\\phi}=\\frac{1}{m}\\sum_{i}h(y_{i})\\frac{f(y_{i})}{g(y_{i})}\\) for \\(y_{i}\\) drawn from \\(g(y)\\), where \\(w_{i}=f(y_{i})/g(y_{i})\\) act as weights. (Often in Bayesian contexts, we know \\(f(y)\\) only up to a normalizing constant. In this case we need to use \\(w_{i}^{*}=w_{i}/\\sum_{j}w_{j}\\).\nHere we don’t require the majorizing property, just that the densities have common support, but things can be badly behaved if we sample from a density with lighter tails than the density of interest. So in general we want \\(g\\) to have heavier tails. More specifically for a low variance estimator of \\(\\phi\\), we would want that \\(f(y_{i})/g(y_{i})\\) is large only when \\(h(y_{i})\\) is very small, to avoid having overly influential points.\nThis suggests we can reduce variance in an IS context by oversampling \\(y\\) for which \\(h(y)\\) is large and undersampling when it is small, since \\(\\mbox{Var}(\\hat{\\phi})=\\frac{1}{m}\\mbox{Var}(h(Y)\\frac{f(Y)}{g(Y)})\\). An example is that if \\(h\\) is an indicator function that is 1 only for rare events, we should oversample rare events and then the IS estimator corrects for the oversampling.\nWhat if we actually want a sample from \\(f\\) as opposed to estimating the expected value above? We can draw \\(y\\) from the unweighted sample, \\(\\{y_{i}\\}\\), with weights \\(\\{w_{i}\\}\\). This is called sampling importance resampling (SIR)."
  },
  {
    "objectID": "units/unit9-sim.html#ratio-of-uniforms-optional",
    "href": "units/unit9-sim.html#ratio-of-uniforms-optional",
    "title": "Simulation",
    "section": "Ratio of uniforms (optional)",
    "text": "Ratio of uniforms (optional)\nIf \\(U\\) and \\(V\\) are uniform in \\(C=\\{(u,v):\\,0\\leq u\\leq\\sqrt{f(v/u)}\\) then \\(X=V/U\\) has density proportion to \\(f\\). The basic algorithm is to choose a rectangle that encloses \\(C\\) and sample until we find \\(u\\leq f(v/u)\\). Then we use \\(x=v/u\\) as our RV. The larger region enclosing \\(C\\) is the majorizing region and a simple approach (if \\(f(x)\\)and \\(x^{2}f(x)\\) are bounded in \\(C\\)) is to choose the rectangle, \\(0\\leq u\\leq\\sup_{x}\\sqrt{f(x)}\\), \\(\\inf_{x}x\\sqrt{f(x)}\\leq v\\leq\\sup_{x}x\\sqrt{f(x)}\\).\nOne can also consider truncating the rectangular region, depending on the features of \\(f\\).\nMonahan recommends the ratio of uniforms, particularly a version for discrete distributions (p. 323 of the 2nd edition)."
  },
  {
    "objectID": "units/unit3-bash.html",
    "href": "units/unit3-bash.html",
    "title": "The bash shell and UNIX commands",
    "section": "",
    "text": "PDF\nReference:"
  },
  {
    "objectID": "units/unit3-bash.html#first-challenge",
    "href": "units/unit3-bash.html#first-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.1 First challenge",
    "text": "4.1 First challenge\nConsider the file cpds.csv. How would you write a shell command that returns “There are 8 occurrences of the word ‘Belgium’ in this file.”, where ‘8’ should instead be the correct number of times the word occurs.\nExtra: make your code into a function that can operate on any file indicated by the user and any word of interest."
  },
  {
    "objectID": "units/unit3-bash.html#second-challenge",
    "href": "units/unit3-bash.html#second-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.2 Second challenge",
    "text": "4.2 Second challenge\nConsider the data in the RTADataSub.csv file. This is a subset of data giving freeway travel times for segments of a freeway in an Australian city. The data are from a kaggle.com competition. We want to try to understand the kinds of data in each field of the file. The following would be particularly useful if the data were in many files or the data were many gigabytes in size.\n\nFirst, take the fourth column. Figure out the unique values in that column.\nNext, automate the process of determining if any of the values are non-numeric so that you don’t have to scan through all of the unique values looking for non-numbers. You’ll need to look for the following regular expression pattern [^0-9], which is interpreted as NOT any of the numbers 0 through 9.\n\nExtra: do it for all the fields, except the first one. Have your code print out the result in a human-readable way understandable by someone who didn’t write the code. For simplicity, you can assume you know the number of fields."
  },
  {
    "objectID": "units/unit3-bash.html#third-challenge",
    "href": "units/unit3-bash.html#third-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.3 Third challenge",
    "text": "4.3 Third challenge\n\nFor Belgium, determine the minimum unemployment value (field #6) in cpds.csv in a programmatic way.\nHave what is printed out to the screen look like “Belgium 6.2”.\nNow store the unique values of the countries in a variable, first stripping out the quotation marks.\nFigure out how to automate step 1 to do the calculation for all the countries and print to the screen.\nHow would you instead store the results in a new file?"
  },
  {
    "objectID": "units/unit3-bash.html#fourth-challenge",
    "href": "units/unit3-bash.html#fourth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.4 Fourth challenge",
    "text": "4.4 Fourth challenge\nLet’s return to the RTADataSub.csv file and the issue of missing values.\n\nCreate a new file without any rows that have an ‘x’ (which indicate a missing value).\nTurn the code into a function that also prints out the number of rows that are being removed and that sends its output to stdout so that it can be used with piping.\nNow modify your function so that the user could provide the missing value string and the input filename."
  },
  {
    "objectID": "units/unit3-bash.html#fifth-challenge",
    "href": "units/unit3-bash.html#fifth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.5 Fifth challenge",
    "text": "4.5 Fifth challenge\nConsider the coop.txt weather station file.\nFigure out how to use grep to tell you the starting position of the state field. Hints: search for a known state-country combination and figure out what flags you can use with grep to print out the “byte offset” for the matched state.\nUse that information to automate the first mission where we extracted the state field using cut. You’ll need to do a bit of arithmetic using shell commands."
  },
  {
    "objectID": "units/unit3-bash.html#sixth-challenge",
    "href": "units/unit3-bash.html#sixth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.6 Sixth challenge",
    "text": "4.6 Sixth challenge\nHere’s an advanced one - you’ll probably need to use sed, but the brief examples of text substitution in the using bash tutorial (or in the demos above) should be sufficient to solve the problem.\nConsider a CSV file that has rows that look like this:\n1,\"America, United States of\",45,96.1,\"continental, coastal\" \n2,\"France\",33,807.1,\"continental, coastal\"\nWhile Pandas would be able to handle this using read_csv(), using cut in UNIX won’t work because of the commas embedded within the fields. The challenge is to convert this file to one that we can use cut on, as follows.\nFigure out a way to make this into a new delimited file in which the delimiter is not a comma. At least one solution that will work for this particular two-line dataset does not require you to use regular expressions, just simple replacement of fixed patterns."
  },
  {
    "objectID": "units/unit3-bash.html#versions-of-regular-expressions",
    "href": "units/unit3-bash.html#versions-of-regular-expressions",
    "title": "The bash shell and UNIX commands",
    "section": "Versions of regular expressions",
    "text": "Versions of regular expressions\nOne thing that can cause headaches is differences in version of regular expression syntax used. As discussed in man grep, extended regular expressions are standard, with basic regular expressions providing less functionality and Perl regular expressions additional functionality.\nThe bash shell tutorial provides a full documentation of the extended regular expressions syntax, which we’ll focus on here. This syntax should be sufficient for most usage and should be usable in Python and R, but if you notice something funny going on, it might be due to differences between the regular expressions versions.\n\nIn bash, grep -E (or egrep) enables use of the extended regular expressions.\nIn Python, the re package provides syntax “similar to” Perl.\nIn R, stringr provides ICU regular expressions (see help(regex)), which are based on Perl regular expressions.\n\nMore details about Perl regular expressions can be found in the regex Wikipedia page."
  },
  {
    "objectID": "units/unit3-bash.html#general-principles-for-working-with-regex",
    "href": "units/unit3-bash.html#general-principles-for-working-with-regex",
    "title": "The bash shell and UNIX commands",
    "section": "General principles for working with regex",
    "text": "General principles for working with regex\nThe syntax is very concise, so it’s helpful to break down individual regular expressions into the component parts to understand them. As Murrell notes, since regex are their own language, it’s a good idea to build up a regex in pieces as a way of avoiding errors just as we would with any computer code. re.findall in Python and str_detect in R’s stringr, as well as regex101.com are particularly useful in seeing what was matched to help in understanding and learning regular expression syntax and debugging your regex. As with many kinds of coding, I find that debugging my regex is usually what takes most of my time."
  },
  {
    "objectID": "units/unit3-bash.html#challenge-problem",
    "href": "units/unit3-bash.html#challenge-problem",
    "title": "The bash shell and UNIX commands",
    "section": "Challenge problem",
    "text": "Challenge problem\nChallenge: Let’s think about what regex syntax we would need to detect any number, integer- or real-valued. Let’s start from a test-driven development perspective of writing out test cases including: - various cases we want to detect, - various tricky cases that are not numbers and we don’t want to detect, and - “corner cases” – tricky (perhaps unexpected) cases that might trip us up."
  },
  {
    "objectID": "units/unit2-dataTech.html",
    "href": "units/unit2-dataTech.html",
    "title": "Data technologies, formats, and structures",
    "section": "",
    "text": "PDF\nReferences (see syllabus for links):\n(Optional) Videos:\nThere are four videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to:\nNote that the videos were prepared for a version of the course that used R, so there are some differences from the content in the current version of the unit that reflect translating between R and Python. I’m not sure how helpful they’ll be, but they are available."
  },
  {
    "objectID": "units/unit2-dataTech.html#text-and-binary-files",
    "href": "units/unit2-dataTech.html#text-and-binary-files",
    "title": "Data technologies, formats, and structures",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or files in other encodings such as UTF-8, both covered in Section 5. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nSome text file formats, such as JSON or HTML, are not easily interpretable/manipulable on a line-by-line basis (unlike, e.g., CSV), so they are not as amenable to processing using shell commands.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, Python pickle files, R data (e.g., .Rda) files, , and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 8."
  },
  {
    "objectID": "units/unit2-dataTech.html#common-file-types",
    "href": "units/unit2-dataTech.html#common-file-types",
    "title": "Data technologies, formats, and structures",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types, some of which are text formats and some of which are binary formats.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters might be preferable. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with from the command line, but read_table() in Pandas handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX. Windows uses a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) whereas UNIX uses onlyl a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see \\^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may be in a text file with one piece of information per row, but without meaningful columns/fields.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The lxml and json packages are useful for reading and writing from these formats. More in Section 4.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The requests and BeautifulSoup packages are useful for reading HTML.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The netCDF4 package in Python nicely handles working with netCDF files.\nData may already be in a database or in the data storage format of another statistical package (Stata, SAS, SPSS, etc.). The Pandas package in Python has capabilities for importing Stata (read_stata), SPSS (read_spss), and SAS (read_sas) files, among others.\nFor Excel, there are capabilities to read an Excel file (see the read_excel function in Pandas), but you can also just go into Excel and export as a CSV file or the like and then read that into Python. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nPython can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to Python. More in the big data unit and in the large datasets tutorial mentioned above."
  },
  {
    "objectID": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "href": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "title": "Data technologies, formats, and structures",
    "section": "CSV vs. specialized formats such as Parquet",
    "text": "CSV vs. specialized formats such as Parquet\nCSV is a common format (particularly in some disciplines/contexts) and has the advantages of being simple to understand, human readable, and readily manipulable by line-based processing tools such as shell commands. However, it has various disadvantages:\n\nstorage is by row, which will often mix values of different types;\nextra space is taken up by explicitly storing commas and newlines; and\none must search through the document to find a given row or value – e.g., to find the 10th row, we must search for the 9th newline and then read until the 10th newline.\n\nA popular file format that has some advantages over plain text formats such as CSV is Parquet. The storage is by column (actually in chunks of columns). This works well with how datasets are often structured in that a given field/variable will generally have values of all the same type and there may be many repeated values, so there are opportunities for efficient storage including compression. Storage by column also allows retrieval only of the columns that a user needs. As a result data stored in the Parquet format often takes up much less space than stored as CSV and can be queried much faster. Also note that data stored in Parquet will often be stored as multiple files.\nHere’s a brief exploration using a data file not in the class repository.\n\nimport time\n\n## Read from CSV\nt0 = time.time()\ndata_from_csv = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nprint(time.time() - t0)\n\n0.8456258773803711\n\n\n\n## Write out Parquet-formatted data\ndata_from_csv.to_parquet(os.path.join('..', 'data', 'airline.parquet'))\n\n## Read from Parquet\nt0 = time.time()\ndata_from_parquet = pd.read_parquet(os.path.join(\n                                    '..', 'data', 'airline.parquet'))\nprint(time.time() - t0)\n\n0.10250258445739746\n\n\nThe CSV file is 51 MB while the Parquet file is 8 MB.\n\nimport subprocess\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.csv\")])\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.parquet\")])\n\n-rw-r--r-- 1 paciorek scfstaff 51480244 Aug 29  2022 ../data/airline.csv\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.csv'], returncode=0)\n\n\n-rw-r--r-- 1 paciorek scfstaff 8153160 Aug 31 08:01 ../data/airline.parquet\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.parquet'], returncode=0)"
  },
  {
    "objectID": "units/unit2-dataTech.html#core-python-functions",
    "href": "units/unit2-dataTech.html#core-python-functions",
    "title": "Data technologies, formats, and structures",
    "section": "Core Python functions",
    "text": "Core Python functions\nThe read_table and read_csv functions in the Pandas package are commonly used for reading in data. They read in delimited files (CSV specifically in the latter case). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read_fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how Pandas determines the types of the fields that are read in. While Pandas will try to determine the types automatically, it can be safer (and faster) to tell Pandas what the types are, using the dtype argument to read_table().\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read_table. Note that sep='' can use regular expressions (which would be helpful if you want to separate on any amount of white space, as one example).\n\ndat = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                    sep = ',', header = None)\ndat.dtypes.head()   # 'object' is string or mixed type\ndat.loc[0,1]     \ntype(dat.loc[0,1]) # string!\n## Whoops, there is an 'x', presumably indicating missingness:\ndat.loc[:,1].unique()\n\n/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/IPython/core/formatters.py:342: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n\n\n\n\n\n'2336'\n\n\nstr\n\n\narray(['2336', '2124', '1830', '1833', '1600', '1578', '1187', '1005',\n       '918', '865', '871', '860', '883', '897', '898', '893', '913',\n       '870', '962', '880', '875', '884', '894', '836', '848', '885',\n       '851', '900', '861', '866', '867', '829', '853', '920', '877',\n       '908', '855', '845', '859', '856', '825', '828', '854', '847',\n       '840', '873', '822', '818', '838', '815', '813', '816', '849',\n       '802', '805', '792', '823', '808', '798', '800', '842', '809',\n       '807', '826', '810', '801', '794', '771', '796', '790', '787',\n       '775', '751', '783', '811', '768', '779', '795', '770', '821',\n       '830', '767', '772', '791', '781', '773', '777', '814', '778',\n       '782', '837', '759', '846', '797', '835', '832', '793', '803',\n       '834', '785', '831', '820', '812', '824', '728', '760', '762',\n       '753', '758', '764', '741', '709', '735', '749', '752', '761',\n       '750', '776', '766', '789', '763', '864', '858', '869', '886',\n       '844', '863', '916', '890', '872', '907', '926', '935', '933',\n       '906', '905', '912', '972', '996', '1009', '961', '952', '981',\n       '917', '1011', '1071', '1920', '3245', '3805', '3926', '3284',\n       '2700', '2347', '2078', '2935', '3040', '1860', '1437', '1512',\n       '1720', '1493', '1026', '928', '874', '833', '850', nan, 'x'],\n      dtype=object)\n\n\n\n## Let's treat 'x' as a missing value indicator.\ndat2 = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                     sep = ',', header = None, na_values = 'x')\ndat2.dtypes.head()\ndat2.loc[:,1].unique()\n\n/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/IPython/core/formatters.py:342: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n\n\n\n\n\narray([2336., 2124., 1830., 1833., 1600., 1578., 1187., 1005.,  918.,\n        865.,  871.,  860.,  883.,  897.,  898.,  893.,  913.,  870.,\n        962.,  880.,  875.,  884.,  894.,  836.,  848.,  885.,  851.,\n        900.,  861.,  866.,  867.,  829.,  853.,  920.,  877.,  908.,\n        855.,  845.,  859.,  856.,  825.,  828.,  854.,  847.,  840.,\n        873.,  822.,  818.,  838.,  815.,  813.,  816.,  849.,  802.,\n        805.,  792.,  823.,  808.,  798.,  800.,  842.,  809.,  807.,\n        826.,  810.,  801.,  794.,  771.,  796.,  790.,  787.,  775.,\n        751.,  783.,  811.,  768.,  779.,  795.,  770.,  821.,  830.,\n        767.,  772.,  791.,  781.,  773.,  777.,  814.,  778.,  782.,\n        837.,  759.,  846.,  797.,  835.,  832.,  793.,  803.,  834.,\n        785.,  831.,  820.,  812.,  824.,  728.,  760.,  762.,  753.,\n        758.,  764.,  741.,  709.,  735.,  749.,  752.,  761.,  750.,\n        776.,  766.,  789.,  763.,  864.,  858.,  869.,  886.,  844.,\n        863.,  916.,  890.,  872.,  907.,  926.,  935.,  933.,  906.,\n        905.,  912.,  972.,  996., 1009.,  961.,  952.,  981.,  917.,\n       1011., 1071., 1920., 3245., 3805., 3926., 3284., 2700., 2347.,\n       2078., 2935., 3040., 1860., 1437., 1512., 1720., 1493., 1026.,\n        928.,  874.,  833.,  850.,   nan])\n\n\nUsing dtype is a good way to control how data are read in.\n\ndat = pd.read_table(os.path.join('..', 'data', 'hivSequ.csv'),\n                  sep = ',', header = 0,\n                  dtype = {\n                  'PatientID': int,\n                  'Resp': int,\n                  'PR Seq': str,\n                  'RT Seq': str,\n                  'VL-t0': float,\n                  'CD4-t0': int})\ndat.dtypes\ndat.loc[0,'PR Seq']\n\n/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/IPython/core/formatters.py:342: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n\n\n\n\n\n'CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAGGGGGGCAACTAAAGGAAGCYCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGACATGGAGTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTTATCAAAGTAARACAGTATGATCAGRTACCCATAGAAATCTATGGACATAAAGCTGTAGGTACAGTATTAATAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACTTTAAATTTY'\n\n\nNote that you can avoid reading in one or more columns by using the usecols argument. Also, specifying the dtype argument explicitly should make for faster file reading.\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into Python to catch such issues in advance. Using the UNIX command less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities (as we will see in Unit 3) is often useful before trying to read something into a program.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. We can read each line as a separate string, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     lines = file.readlines()\n\nid = [line[3:11] for line in lines]\nyear = [int(line[17:21]) for line in lines]\nmonth = [int(line[21:23]) for line in lines]\nnvalues = [int(line[27:30]) for line in lines]\nyear[0:5]\n\n[2010, 2010, 2010, 2010, 2010]\n\n\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using pandas.read_fwf() would be a good strategy."
  },
  {
    "objectID": "units/unit2-dataTech.html#connections-and-streaming",
    "href": "units/unit2-dataTech.html#connections-and-streaming",
    "title": "Data technologies, formats, and structures",
    "section": "Connections and streaming",
    "text": "Connections and streaming\nPython allows you to read in not just from a file but from a more general construct called a connection. This can include reading in text from the output of running a shell command and from unzipping a file on the fly.\nHere are some examples of connections:\n\nimport gzip\nwith gzip.open('dat.csv.gz', 'r') as file:\n     lines = file.readlines()\n\nimport zipfile\nwith zipfile.ZipFile('dat.zip', 'r') as archive:\n     with archive.open('data.txt', 'r') as file:\n          lines = file.readlines()\n\nimport subprocess\ncommand = \"ls -al\"\noutput = subprocess.check_output(command, shell = True)\n# `output` is a sequence of bytes.\nwith io.BytesIO(output) as stream:  # Create a file-like object.\n    content = stream.readlines()\n\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")\n\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. This is referred to as online processing, streaming, or chunking, and can be done using Pandas (among other tools).\n\nfile_path = os.path.join('..', 'data', 'RTADataSub.csv')\nchunksize = 50 # Obviously this would be much larger in any real application.\n\nwith pd.read_csv(file_path, chunksize = chunksize) as reader:\n     for chunk in reader:\n         # manipulate the lines and store the key stuff\n         print(f'Read {len(chunk)} rows.')\n\nMore details on sequential (on-line) processing of large files can be found in the tutorial on large datasets mentioned in the reference list above.\nOne cool trick that can come in handy is to ‘read’ from a string as if it were a text file. Here’s an example:\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     text = file.read()\n\nstringIOtext = io.StringIO(text)\ndf = pd.read_fwf(stringIOtext, header = None, widths = [3,8,4,2,4,2])\n\nWe can create connections for writing output too. Just make sure to open the connection first."
  },
  {
    "objectID": "units/unit2-dataTech.html#file-paths",
    "href": "units/unit2-dataTech.html#file-paths",
    "title": "Data technologies, formats, and structures",
    "section": "File paths",
    "text": "File paths\nA few notes on file paths, related to ideas of reproducibility.\n\nIn general, you don’t want to hard-code absolute paths into your code files because those absolute paths won’t be available on the machines of anyone you share the code with. Instead, use paths relative to the directory the code file is in, or relative to a baseline directory for the project, e.g.:\n\n\ndat = pd.read_csv('../data/cpds.csv')\n\nUsing UNIX style directory separators will work in Windows, Mac or Linux, but using Windows style separators is not portable across operating systems.\n\n## good: will work on Windows\ndat = pd.read_csv('../data/cpds.csv')\n## bad: won't work on Mac or Linux\ndat = pd.read_csv('..\\data\\cpds.csv')  \n\nEven better, use os.path.join so that paths are constructed specifically for the operating system the user is using:\n\n\n## good: operating-system independent\ndat = pd.read_csv(os.path.join('..', 'data', 'cpds.csv'))"
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "href": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "title": "Data technologies, formats, and structures",
    "section": "Reading data quickly: Arrow and Polars",
    "text": "Reading data quickly: Arrow and Polars\nApache Arrow provides efficient data structures for working with data in memory, usable in Python via the PyArrow package. Data are stored by column, with values in a column stored sequentially and in such a way that one can access a specific value without reading the other values in the column (O(1) lookup). Arrow is designed to read data from various file formats, including Parquet, native Arrow format, and text files. In general Arrow will only read data from disk as needed, avoiding keeping the entire dataset in memory.\nOther options for avoiding reading all your data into memory include the Dask package and using numpy.load with the mmap_mode argument.\npolars is designed to be a faster alternative to Pandas for working with data in-memory.\n\nimport polars\nimport time\nt0 = time.time()\ndat = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nt1 = time.time()\ndat2 = polars.read_csv(os.path.join('..', 'data', 'airline.csv'), null_values = ['NA'])\nt2 = time.time()\nprint(f\"Timing for Pandas: {t1-t0}.\")\nprint(f\"Timing for Polars: {t2-t1}.\")\n\nTiming for Pandas: 0.8093833923339844.\nTiming for Polars: 0.12352108955383301."
  },
  {
    "objectID": "units/unit2-dataTech.html#writing-output-to-files",
    "href": "units/unit2-dataTech.html#writing-output-to-files",
    "title": "Data technologies, formats, and structures",
    "section": "Writing output to files",
    "text": "Writing output to files\nFunctions for text output are generally analogous to those for input.\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'w') as file:\n     file.writelines(lines)\n\nWe can also use file.write() to write individual strings.\nIn Pandas, we can use DataFrame.to_csv and DataFrame.to_parquet.\nWe can use the json.dump function to output appropriate data objects (e.g., dictionaries or possibly lists) as JSON. One use of JSON as output from Python would be to ‘serialize’ the information in an Python object such that it could be read into another program.\nAnd of course you can always save to a Pickle data file (a binary file format) using pickle.dump() and pickle.load() from the pickle package. Happily this is platform-independent so can be used to transfer Python objects between different OS."
  },
  {
    "objectID": "units/unit2-dataTech.html#formatting-output",
    "href": "units/unit2-dataTech.html#formatting-output",
    "title": "Data technologies, formats, and structures",
    "section": "Formatting output",
    "text": "Formatting output\nWe can use string formatting to control how output is printed to the screen.\nThe mini-language involved in the format specification can get fairly involved, but a few basic pieces of syntax can do most of what one generally needs to do.\nWe can format numbers to chosen number of digits and decimal places and handle alignment, using the format method of the string class.\nFor example:\n\n'{:&gt;10}'.format(3.5)    # right-aligned, using 10 characters\n'{:.10f}'.format(1/3)   # force 10 decimal places\n'{:15.10f}'.format(1/3) # force 15 characters, with 10 decimal places\nformat(1/3, '15.10f') # alternative using a function\n\n'       3.5'\n\n\n'0.3333333333'\n\n\n'   0.3333333333'\n\n\n'   0.3333333333'\n\n\nWe can also “interpolate” variables into strings.\n\n\"The number pi is {}.\".format(np.pi)\n\"The number pi is {:.5f}.\".format(np.pi)\n\"The number pi is {:.12f}.\".format(np.pi)\n\n'The number pi is 3.141592653589793.'\n\n\n'The number pi is 3.14159.'\n\n\n'The number pi is 3.141592653590.'\n\n\n\nval1 = 1.5\nval2 = 2.5\n# As of Python 3.6, put the variable names in directly.\nprint(f\"Let's add {val1} and {val2}.\")  \nnum1 = 1/3\nprint(\"Let's add the %s numbers %.5f and %15.7f.\"\n       %('floating point', num1 ,32+1/7))\n\nLet's add 1.5 and 2.5.\nLet's add the floating point numbers 0.33333 and      32.1428571.\n\n\nOr to insert into a file:\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'a') as file:\n     file.write(\"Let's add the %s numbers %.5f and %15.7f.\"\n                %('floating point', num1 ,32+1/7))\n\nround is another option, but it’s often better to directly control the printing format."
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-html",
    "href": "units/unit2-dataTech.html#reading-html",
    "title": "Data technologies, formats, and structures",
    "section": "Reading HTML",
    "text": "Reading HTML\nHTML (Hypertext Markup Language) is the standard markup language used for displaying content in a web browser. In simple webpages (ignoring the more complicated pages that involve Javascript), what you see in your browser is simply a rendering (by the browser) of a text file containing HTML.\nHowever, instead of rendering the HTML in a browser, we might want to use code to extract information from the HTML.\nLet’s see a brief example of reading in HTML tables.\nNote that before doing any coding, it can be helpful to look at the raw HTML source code for a given page. We can explore the underlying HTML source in advance of writing our code by looking at the page source directly in the browser (e.g., in Firefox under the 3-lines (hamburger) “open menu” symbol, see Web Developer (or More Tools) -&gt; Page Source and in Chrome View -&gt; Developer -&gt; View Source), or by downloading the webpage and looking at it in an editor, although in some cases (such as the nytimes.com case), what we might see is a lot of JavaScript.\nOne lesson here is not to write a lot of your own code to do something that someone else has probably already written a package for. We’ll use the BeautifulSoup4 package.\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\nURL = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nresponse = requests.get(URL)\nhtml = response.content\n\n# Create a BeautifulSoup object to parse the HTML\nsoup = bs(html, 'html.parser')\n\nhtml_tables = soup.find_all('table')\n\npd_tables = [pd.read_html(str(tbl))[0] for tbl in html_tables]\n\n[x.shape[0] for x in pd_tables]\n\npd_tables[0].head()\n\n[242, 13, 1]\n\n\n/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/IPython/core/formatters.py:342: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nCountry / Dependency\nPopulation\n% of world\nDate\nSource (official or from the United Nations)\nUnnamed: 6\n\n\n\n\n0\n–\nWorld\n8057107000\n100%\n31 Aug 2023\nUN projection[3]\nNaN\n\n\n1\n1\nChina\n1411750000\nNaN\n31 Dec 2022\nOfficial estimate[4]\n[b]\n\n\n2\n2\nIndia\n1392329000\nNaN\n1 Jul 2023\nOfficial projection[5]\n[c]\n\n\n3\n3\nUnited States\n335340000\nNaN\n31 Aug 2023\nNational population clock[7]\n[d]\n\n\n4\n4\nIndonesia\n277749853\nNaN\n31 Dec 2022\nOfficial estimate[8]\nNaN\n\n\n\n\n\n\n\nBeautiful Soup works by reading in the HTML as text and then parsing it to build up a tree containing the HTML elements. Then one can search by HTML tag or attribute for information you want using find_all.\nAs another example, it’s often useful to be able to extract the hyperlinks in an HTML document.\n\nURL = \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\nresponse = requests.get(URL)\nsoup = bs(response.content, 'html.parser')\n\n## Approach 1: search for HTML 'a' tags.\na_elements = soup.find_all('a')\nlinks1 = [x.get('href') for x in a_elements]\n## Approach 2: search for 'a' elements with 'href' attribute\nhref_elements = soup.find_all('a', href = True)\nlinks2 = [x.get('href') for x in href_elements]\n## In either case, then use `get` to retrieve the `href` attribute value.\n\nlinks2[0:9]\n# help(bs.find_all)\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']\n\n\nThe kwargs keyword arguments to find and find_all allow one to search for elements with particular characteristics, such as having a particular attribute (seen above) or having an attribute have a particular value (e.g., picking out an element with a particular id).\nHere’s another example of extracting specific components of information from a webpage (results not shown, since headlines will vary from day to day). We’ll use get_text to retrieve the element’s value.\n\nURL = \"https://www.nytimes.com\"\nresponseNYT = requests.get(URL)\nsoupNYT = bs(responseNYT.content, 'html.parser')\nh2_elements = soupNYT.find_all(\"h2\")\nheadlines2 = [x.get_text() for x in h2_elements]\nh3_elements = soupNYT.find_all(\"h3\")\nheadlines3 = [x.get_text() for x in h3_elements]\n\nMore generally, we may want to read an HTML document, parse it into its components (i.e., the HTML elements), and navigate through the tree structure of the HTML.\nWe can use CSS selectors with the select method for more powerful extraction capabilities. Going back to the climate data, let’s extract all the th elements nested within tr elements:\n\nsoup.select(\"tr th\")\n\n[&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;]\n\n\nOr we could extract the a elements whose parents are th elements:\n\nsoup.select(\"th &gt; a\")\n\n[&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;,\n &lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;,\n &lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;,\n &lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;]\n\n\nNext let’s use the XPath language to specify elements rather than CSS selectors. XPath can also be used for navigating through XML documents.\n\nimport lxml.html\n\n# Convert the BeautifulSoup object to a lxml object\nlxml_doc = lxml.html.fromstring(str(soup))\n\n# Use XPath to select elements\na_elements = lxml_doc.xpath('//a[@href]')\nlinks = [x.get('href') for x in a_elements]\nlinks[0:9]\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']"
  },
  {
    "objectID": "units/unit2-dataTech.html#xml",
    "href": "units/unit2-dataTech.html#xml",
    "title": "Data technologies, formats, and structures",
    "section": "XML",
    "text": "XML\nXML is a markup language used to store data in self-describing (no metadata needed) format, often with a hierarchical structure. It consists of sets of elements (also known as nodes because they generally occur in a hierarchical structure and therefore have parents, children, etc.) with tags that identify/name the elements, with some similarity to HTML. Some examples of the use of XML include serving as the underlying format for Microsoft Office and Google Docs documents and for the KML language used for spatial information in Google Earth.\nHere’s a brief example. The book with id attribute bk101 is an element; the author of the book is also an element that is a child element of the book. The id attribute allows us to uniquely identify the element.\n    &lt;?xml version=\"1.0\"?&gt;\n    &lt;catalog&gt;\n       &lt;book id=\"bk101\"&gt;\n          &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n          &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n          &lt;genre&gt;Computer&lt;/genre&gt;\n          &lt;price&gt;44.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n          &lt;description&gt;An in-depth look at creating applications with XML.&lt;/description&gt;\n       &lt;/book&gt;\n       &lt;book id=\"bk102\"&gt;\n          &lt;author&gt;Ralls, Kim&lt;/author&gt;\n          &lt;title&gt;Midnight Rain&lt;/title&gt;\n          &lt;genre&gt;Fantasy&lt;/genre&gt;\n          &lt;price&gt;5.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-12-16&lt;/publish_date&gt;\n         &lt;description&gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.&lt;/description&gt;\n       &lt;/book&gt;\n    &lt;/catalog&gt;\nWe can read XML documents into Python using various packages, including lxml and then manipulate the resulting structured data object. Here’s an example of working with lending data from the Kiva lending non-profit. You can see the XML format in a browser at http://api.kivaws.org/v1/loans/newest.xml.\nXML documents have a tree structure with information at nodes. As above with HTML, one can use the XPath language for navigating the tree and finding and extracting information from the node(s) of interest.\nHere is some example code for extracting loan info from the Kiva data. We’ll first show the ‘brute force’ approach of working with the data as a list and then the better approach of using XPath.\n\nimport xmltodict\n\nURL = \"https://api.kivaws.org/v1/loans/newest.xml\"\nresponse = requests.get(URL)\ndata = xmltodict.parse(response.content)\ndata.keys()\ndata['response'].keys()\ndata['response']['loans'].keys()\nlen(data['response']['loans']['loan'])\ndata['response']['loans']['loan'][2]\ndata['response']['loans']['loan'][2]['activity']\n\ndict_keys(['response'])\n\n\ndict_keys(['paging', 'loans'])\n\n\ndict_keys(['@type', 'loan'])\n\n\n20\n\n\n{'id': '2635728',\n 'name': 'Gulmira',\n 'description': {'languages': {'@type': 'list', 'language': ['ru', 'en']}},\n 'status': 'fundraising',\n 'funded_amount': '0',\n 'basket_amount': '0',\n 'image': {'id': '5248685', 'template_id': '1'},\n 'activity': 'Education provider',\n 'sector': 'Education',\n 'use': 'to purchase a printer.',\n 'location': {'country_code': 'KG',\n  'country': 'Kyrgyzstan',\n  'town': 'Kochkor district, Naryn region',\n  'geo': {'level': 'town', 'pairs': '42.099103 75.52767', 'type': 'point'}},\n 'partner_id': '171',\n 'posted_date': '2023-08-31T14:50:10Z',\n 'planned_expiration_date': '2023-10-05T14:50:10Z',\n 'loan_amount': '375',\n 'borrower_count': '1',\n 'lender_count': '0',\n 'bonus_credit_eligibility': '1',\n 'tags': None}\n\n\n'Education provider'\n\n\n\nfrom lxml import etree\ndoc = etree.fromstring(response.content)\n\nloans = doc.xpath(\"//loan\")\n[loan.xpath(\"activity/text()\") for loan in loans]\n\n## suppose we only want the country locations of the loans (using XPath)\n[loan.xpath(\"location/country/text()\") for loan in loans]\n## or extract the geographic coordinates\n[loan.xpath(\"location/geo/pairs/text()\") for loan in loans]\n\n[['Personal Housing Expenses'],\n ['Education provider'],\n ['Education provider'],\n ['Agriculture'],\n ['Education provider'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Agriculture'],\n ['Agriculture'],\n ['Clothing'],\n ['Crafts'],\n ['Food Production/Sales'],\n ['Agriculture'],\n ['Agriculture'],\n ['Agriculture'],\n ['Food']]\n\n\n[['Nicaragua'],\n ['Kyrgyzstan'],\n ['Kyrgyzstan'],\n ['Kyrgyzstan'],\n ['Kyrgyzstan'],\n ['Kenya'],\n ['Kenya'],\n ['Kenya'],\n ['Kenya'],\n ['Kenya'],\n ['Kenya'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa'],\n ['Samoa']]\n\n\n[['11.934407 -85.956001'],\n ['42.099103 75.52767'],\n ['42.099103 75.52767'],\n ['41.824168 71.097317'],\n ['42.874621 74.569762'],\n ['-0.333333 37.65'],\n ['-0.333333 37.65'],\n ['43.010557 -74.379162'],\n ['-0.538838 37.459641'],\n ['0.566667 34.566667'],\n ['-0.583333 35.183333'],\n ['-13.816667 -171.916667'],\n ['-13.583333 -172.333333'],\n ['-13.816667 -171.783333'],\n ['-14.045728 -171.414159'],\n ['-13.833333 -171.816667'],\n ['-14.045728 -171.414159'],\n ['-13.883333 -171.55'],\n ['-13.866667 -171.566667'],\n ['-13.816667 -171.783333']]"
  },
  {
    "objectID": "units/unit2-dataTech.html#json",
    "href": "units/unit2-dataTech.html#json",
    "title": "Data technologies, formats, and structures",
    "section": "JSON",
    "text": "JSON\nJSON files are structured as “attribute-value” pairs (aka “key-value” pairs), often with a hierarchical structure. Here’s a brief example:\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\nA set of key-value pairs is a named array and is placed inside braces (squiggly brackets). Note the nestedness of arrays within arrays (e.g., address within the overarching person array and the use of square brackets for unnamed arrays (i.e., vectors of information), as well as the use of different types: character strings, numbers, null, and (not shown) boolean/logical values. JSON and XML can be used in similar ways, but JSON is less verbose than XML.\nWe can read JSON into Python using the json package. Let’s play again with the Kiva data. The same data that we had worked with in XML format is also available in JSON format: https://api.kivaws.org/v1/loans/newest.json.\n\nURL = \"https://api.kivaws.org/v1/loans/newest.json\"\nresponse = requests.get(URL)\n\nimport json\ndata = json.loads(response.text)\ntype(data)\ndata.keys()\n\ntype(data['loans'])\ndata['loans'][0].keys()\n\ndata['loans'][0]['location']['country']\n[loan['location']['country']  for loan in data['loans']]\n\ndict\n\n\ndict_keys(['paging', 'loans'])\n\n\nlist\n\n\ndict_keys(['id', 'name', 'description', 'status', 'funded_amount', 'basket_amount', 'image', 'activity', 'sector', 'use', 'location', 'partner_id', 'posted_date', 'planned_expiration_date', 'loan_amount', 'borrower_count', 'lender_count', 'bonus_credit_eligibility', 'tags'])\n\n\n'Nicaragua'\n\n\n['Nicaragua',\n 'Kyrgyzstan',\n 'Kyrgyzstan',\n 'Kyrgyzstan',\n 'Kyrgyzstan',\n 'Kenya',\n 'Kenya',\n 'Kenya',\n 'Kenya',\n 'Kenya',\n 'Kenya',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa',\n 'Samoa']\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing values, infinity, etc."
  },
  {
    "objectID": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "href": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "title": "Data technologies, formats, and structures",
    "section": "Webscraping and web APIs",
    "text": "Webscraping and web APIs\nHere we’ll see some examples of making requests over the Web to get data. We’ll use APIs to systematically query a website for information. Ideally, but not always, the API will be documented. In many cases that simply amounts to making an HTTP GET request, which is done by constructing a URL.\nThe requests package is useful for a wide variety of such functionality. Note that much of the functionality I describe below is also possible within the shell using either wget or curl.\n\nWebscraping ethics and best practices\nWebscraping is the process of extracting data from the web, either directly from a website or using a web API (application programming interface).\n\nShould you webscrape? In general, if we can avoid webscraping (particularly if there is not an API) and instead directly download a data file from a website, that is greatly preferred.\nMay you webscrape? Before you set up any automated downloading of materials/data from the web you should make sure that what you are about to do is consistent with the rules provided by the website.\n\nSome places to look for information on what the website allows are:\n\nlegal pages such as Terms of Service or Terms and Conditions on the website.\ncheck the robots.txt file (e.g., https://scholar.google.com/robots.txt) to see what a web crawler is allowed to do, and whether the site requires a particular delay between requests to the sites\npotentially contact the site owner if you plan to scrape a large amount of data\n\nHere are some links with useful information:\n\nBlog post on webscraping ethics\nSome information on how to understand a robots.txt file\n\nTips for when you make automated requests:\n\nWhen debugging code that processes the result of such a request, just run the request once, save (i.e., cache) the result, and then work on the processing code applied to the result. Don’t make the same request over and over again.\nIn many cases you will want to include a time delay between your automated requests to a site, including if you are not actually crawling a site but just want to automate a small number of queries.\n\n\n\nWhat is HTTP?\nHTTP (hypertext transfer protocol) is a system for communicating information from a server (i.e., the website of interest) to a client (e.g., your laptop). The client sends a request and the server sends a response.\nWhen you go to a website in a browser, your browser makes an HTTP GET request to the website. Similarly, when we did some downloading of html from webpages above, we used an HTTP GET request.\nAnytime the URL you enter includes parameter information after a question mark (www.somewebsite.com?param1=arg1&param2=arg2), you are using an API.\nThe response to an HTTP request will include a status code, which can be interpreted based on this information.\nThe response will generally contain content in the form of text (e.g., HTML, XML, JSON) or raw bytes.\n\n\nAPIs: REST- and SOAP-based web services\nIdeally a web service documents their API (Applications Programming Interface) that serves data or allows other interactions. REST and SOAP are popular API standards/styles. Both REST and SOAP use HTTP requests; we’ll focus on REST as it is more common and simpler. When using REST, we access resources, which might be a Facebook account or a database of stock quotes. The API will (hopefully) document what information it expects from the user and will return the result in a standard format (often a particular file format rather than producing a webpage).\nOften the format of the request is a URL (aka an endpoint) plus a query string, passed as a GET request. Let’s search for plumbers near Berkeley, and we’ll see the GET request, in the form:\nhttps://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1\n\nthe query string begins with ?\nthere are one or more Parameter=Argument pairs\npairs are separated by &\n+ is used in place of each space\n\nLet’s see an example of accessing economic data from the World Bank, using the documentation for their API. Following the API call structure, we can download (for example), data on various countries. The documentation indicates that our REST-based query can use either a URL structure or an argument-based structure.\n\n## Queries based on the documentation\napi_url = \"http://api.worldbank.org/V2/incomeLevel/LIC/country\"\napi_args = \"http://api.worldbank.org/V2/country?incomeLevel=LIC\"\n\n## Generalizing a bit\nurl = \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json\"\nresponse = requests.get(url)\n\ndata = json.loads(response.content)\n\n## Be careful of data truncation/pagination\nif False:\n    url = \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json&per_page=1000\"\n    response = requests.get(url)\n    data = json.loads(response.content)\n\n## Programmatic control\nbaseURL = \"http://api.worldbank.org/V2/country\"\ngroup = 'MIC'\nformat = 'json'\nargs = {'incomeLevel': group, 'format': format, 'per_page': 1000}\nurl = baseURL + '?' + '&'.join(['='.join(\n                               [key, str(args[key])]) for key in args])\nresponse = requests.get(url)\ndata = json.loads(response.content)\n   \ntype(data)\nlen(data[1])\ntype(data[1][5])\ndata[1][5]\n\nlist\n\n\n108\n\n\ndict\n\n\n{'id': 'BEN',\n 'iso2Code': 'BJ',\n 'name': 'Benin',\n 'region': {'id': 'SSF', 'iso2code': 'ZG', 'value': 'Sub-Saharan Africa '},\n 'adminregion': {'id': 'SSA',\n  'iso2code': 'ZF',\n  'value': 'Sub-Saharan Africa (excluding high income)'},\n 'incomeLevel': {'id': 'LMC',\n  'iso2code': 'XN',\n  'value': 'Lower middle income'},\n 'lendingType': {'id': 'IDX', 'iso2code': 'XI', 'value': 'IDA'},\n 'capitalCity': 'Porto-Novo',\n 'longitude': '2.6323',\n 'latitude': '6.4779'}\n\n\nAPIs can change and disappear. A few years ago, the example above involved the World Bank’s Climate Data API, which I can no longer find!\nAs another example, here we can see the US Treasury Department API, which allows us to construct queries for federal financial data.\nThe Nolan and Temple Lang book provides a number of examples of different ways of authenticating with web services that control access to the service.\nFinally, some web services allow us to pass information to the service in addition to just getting data or information. E.g., you can programmatically interact with your Facebook, Dropbox, and Google Drive accounts using REST based on HTTP POST, PUT, and DELETE requests. Authentication is of course important in these contexts and some times you would first authenticate with your login and password and receive a “token”. This token would then be used in subsequent interactions in the same session.\nI created your github.berkeley.edu accounts from Python by interacting with the GitHub API using requests.\n\n\nHTTP requests by deconstructing an (undocumented) API\nIn some cases an API may not be documented or we might be lazy and not use the documentation. Instead we might deconstruct the queries a browser makes and then mimic that behavior, in some cases having to parse HTML output to get at data. Note that if the webpage changes even a little bit, our carefully constructed query syntax may fail.\nLet’s look at some UN data (agricultural crop data). By going to\nhttp://data.un.org/Explorer.aspx?d=FAO, and clicking on “Crops”, we’ll see a bunch of agricultural products with “View data” links. Click on “apricots” as an example and you’ll see a “Download” button that allows you to download a CSV of the data. Let’s select a range of years and then try to download “by hand”. Sometimes we can right-click on the link that will download the data and directly see the URL that is being accessed and then one can deconstruct it so that you can create URLs programmatically to download the data you want.\nIn this case, we can’t see the full URL that is being used because there’s some Javascript involved. Therefore, rather than looking at the URL associated with a link we need to view the actual HTTP request sent by our browser to the server. We can do this using features of the browser (e.g., in Firefox see Web Developer -&gt; Network and in Chrome View -&gt; Developer -&gt; Developer tools and choose the Network tab) (or right-click on the webpage and select Inspect and then Network). Based on this we can see that an HTTP GET request is being used with a URL such as:\nhttp://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc.\nWe’e now able to easily download the data using that URL, which we can fairly easily construct using string processing in bash, Python, or R, such as this (here I just paste it together directly, but using more structured syntax such as I used for the World Bank example would be better):\n\nimport zipfile\n\n## example URL:\n## http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode = 526\nbaseURL = \"http://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs = ','.join([str(yr) for yr in range(2012,2018)])\nfilter = f\"?DataFilter=itemCode:{itemCode};year:{yrs}\"\nargs1 = \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 = \"s=countryName:asc,elementCode:asc,year:desc\"\nurl = baseURL + filter + args1 + args2\n## If the website provided a CSV, this would be easier, but it zips the file.\nresponse = requests.get(url)\n\nwith io.BytesIO(response.content) as stream:  # create a file-like object\n     with zipfile.ZipFile(stream, 'r') as archive:   # treat the object as a zip file\n          with archive.open(archive.filelist[0].filename, 'r') as file:  # get a pointer to the embedded file\n              dat = pd.read_csv(file)\n\ndat.head()\n\n/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/IPython/core/formatters.py:342: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n\n\n\n\n\n\n\n\n\nCountry or Area\nElement Code\nElement\nYear\nUnit\nValue\nValue Footnotes\n\n\n\n\n0\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2017.0\nindex\n202.19\nFc\n\n\n1\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2016.0\nindex\n27.45\nFc\n\n\n2\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2015.0\nindex\n134.50\nFc\n\n\n3\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2014.0\nindex\n138.05\nFc\n\n\n4\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2013.0\nindex\n138.05\nFc\n\n\n\n\n\n\n\nSo, what have we achieved?\n\nWe have a reproducible workflow we can share with others (perhaps ourself in the future).\nWe can automate the process of downloading many such files.\n\n\n\nMore details on HTTP requests\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically. Here what is returned is a zip file, which is represented in Python as a sequence of “raw” bytes.\n\ndata = {\"DataFilter\": f\"itemCode:{itemCode};year:{yrs}\",\n       \"DataMartID\": \"FAO\", \n       \"Format\": \"csv\", \n       \"c\": \"2,3,4,5,6,7\",\n       \"s\": \"countryName:asc,elementCode:asc,year:desc\"\n       }    \n\nresponse = requests.get(baseURL, params = data)\n\nwith io.BytesIO(response.content) as stream:  \n     with zipfile.ZipFile(stream, 'r') as archive:\n          with archive.open(archive.filelist[0].filename, 'r') as file:  \n              dat = pd.read_csv(file)\n\nIn some cases we may need to send a lot of information as part of the URL in a GET request. If it gets to be too long (e.g,, more than 2048 characters) many web servers will reject the request. Instead we may need to use an HTTP POST request (POST requests are often used for submitting web forms). A typical request would have syntax like this search (using requests):\n\nurl = 'http://www.wormbase.org/db/searches/advanced/dumper'\n\ndata = {      \"specipes\":\"briggsae\",\n              \"list\": \"\",\n              \"flank3\": \"0\",\n              \"flank5\": \"0\",\n              \"feature\": \"Gene Models\",\n              \"dump\": \"Plain TEXT\",\n              \"orientation\": \"Relative to feature\",\n              \"relative\": \"Chromsome\",\n              \"DNA\":\"flanking sequences only\",\n              \".cgifields\" :  \"feature, orientation, DNA, dump, relative\"\n}                                  \n\nresponse = requests.post(url, data = data)\nif response.status_code == 200:\n    print(\"POST request successful\")\nelse:\n    print(f\"POST request failed with status code: {response.status_code}\")\n\nUnfortunately that specific search doesn’t work because the server URL and/or API seem to have changed. But it gives you an idea of what the format would look like.\nrequests can handle other kinds of HTTP requests such as PUT and DELETE. Finally, some websites use cookies to keep track of users, and you may need to download a cookie in the first interaction with the HTTP server and then send that cookie with later interactions. More details are available in the Nolan and Temple Lang book.\n\n\nPackaged access to an API\nFor popular websites/data sources, a developer may have packaged up the API calls in a user-friendly fashion for use from Python, R, or other software. For example there are Python (twitter) and R (twitteR) packages for interfacing with Twitter via its API.\nHere’s some example code for Python. This looks up the US senators’ Twitter names and then downloads a portion of each of their timelines, i.e., the time series of their tweets. Note that Twitter has limits on how much one can download at once.\n\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\",\n                             slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n\n\n\nAccessing dynamic pages\nSome websites dynamically change in reaction to the user behavior. In these cases you need a tool that can mimic the behavior of a human interacting with a site. Some options are:\n\nselenium is a popular tool for doing this, and there is a Python package of the same name.\nUsing scrapy plus splash is another approach."
  },
  {
    "objectID": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "href": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "title": "Data technologies, formats, and structures",
    "section": "Standard data structures in Python and R",
    "text": "Standard data structures in Python and R\n\nIn Python and R, one often ends up working with dataframes, lists, and arrays/vectors/matrices/tensors.\nIn Python we commonly work with data structures that are part of additional packages, in particular numpy arrays and pandas dataframes.\nDictionaries in Python allow for easy use of key-value pairs where one can access values based on their key/label. In R one can do something similar with named vectors or named lists or (more efficiently) by using environments.\nIn R, if we are not working with rectangular datasets or standard numerical objects, we often end up using lists or enhanced versions of lists, sometimes with deeply nested structures.\n\nIn Unit 7, we’ll talk about distributed data structures that allow one to easily work with data distributed across multiple computers."
  },
  {
    "objectID": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "href": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "title": "Data technologies, formats, and structures",
    "section": "Other kinds of data structures",
    "text": "Other kinds of data structures\nYou may have heard of various other kinds of data structures, such as linked lists, trees, graphs, queues, and stacks. One of the key aspects that differentiate such data structures is how one navigates through the elements.\nSets are collections of elements that don’t have any duplicates (like a mathematical set).\nWith a linked list, with each element (or node) has a value and a pointer (reference) to the location of the next element. (With a doubly-linked list, there is also a pointer back to the previous element.) One big advantage of this is that one can insert an element by simply modifying the pointers involved at the site of the insertion, without copying any of the other elements in the list. A big disadvantage is that to get to an element you have to navigate through the list.\n\n\n\nLinked list (courtesy of computersciencewiki.org)\n\n\nBoth trees and graphs are collections of nodes (vertices) and links (edges). A tree involves a set of nodes and links to child nodes (also possibly containing information linking the child nodes to their parent nodes). With a graph, the links might not be directional, and there can be cycles.\n\n\n\nTree (courtesy of computersciencewiki.org)\n\n\n\n\n\nGraph (courtesy of computersciencewiki.org)\n\n\nA stack is a collection of elements that behave like a stack of lunch trays. You can only access the top element directly(“last in, first out”), so the operations are that you can push a new element onto the stack or pop the top element off the stack. In fact, nested function calls behave as stacks, and the memory used in the process of evaluating the function calls is called the ‘stack’.\nA queue is like the line at a grocery store, behaving as “first in, first out”.\nOne can use such data structures either directly or via add-on packages in Python and R, though I don’t think they’re all that commonly used in R. This is probably because statistical/data science/machine learning workflows often involve either ‘rectangular’ data (i.e., dataframe-style data) and/or mathematical computations with arrays. That said, trees and graphs are widely used.\nSome related concepts that we’ll discuss further in Unit 5 include:\n\ntypes: this refers to how a given piece of information is stored and what operations can be done with the information.\n\n‘primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., booleans, integers, numeric (real-valued), character, pointer (address, reference).\n\npointers: references to other locations (addresses) in memory. One often uses pointers to avoid unnecessary copying of data.\nhashes: hashing involves fast lookup of the value associated with a key (a label), using a hash function, which allows one to convert the key to an address. This avoids having to find the value associated with a specific key by looking through all the keys until the key of interest is found (an O(n) operation)."
  },
  {
    "objectID": "units/unit4-goodPractices.html",
    "href": "units/unit4-goodPractices.html",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "",
    "text": "PDF\nSources:\nThis unit covers good coding/software development practices, debugging (and practices for avoiding bugs), and doing reproducible research. As in later units of the course, the material is generally not specific to Python, but some details and the examples are in Python."
  },
  {
    "objectID": "units/unit4-goodPractices.html#editors",
    "href": "units/unit4-goodPractices.html#editors",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Editors",
    "text": "Editors\nUse an editor that supports the language you are using (e.g., Atom, Emacs/Aquamacs, Sublime, vim, VSCode, TextMate, WinEdt, or the built-in editor in RStudio [you can use Python from within RStudio]). Some advantages of this can include:\n\nhelpful color coding of different types of syntax,\nautomatic indentation and spacing,\nparenthesis matching,\nline numbering (good for finding bugs), and\ncode can often be run (or compiled) and debugged from within the editor.\n\nSee the problem set submission how-to document for more information about editors that interact nicely with Quarto documents."
  },
  {
    "objectID": "units/unit4-goodPractices.html#coding-syntax",
    "href": "units/unit4-goodPractices.html#coding-syntax",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Coding syntax",
    "text": "Coding syntax\nThe PEP 8 style guide is your go-to reference for Python style. I’ve highlighted some details here as well as included some general suggestions of my own.\n\nHeader information: put metainfo on the code into the first few lines of the file as comments. Include who, when, what, how the code fits within a larger program (if appropriate), possibly the versions of Python and key packages that you used.\nWrite docstrings for public modules, classes, functions, and methods. For non-public items, a comment after the def line is sufficient to describe the purpose of the item.\nIndentation: Python is strict about indentation of course, which helps to enforce clear indentation more than in other languages. This helps you and others to read and understand the code and can help in detecting errors in your code because it can expose lack of symmetry.\n\nuse 4 spaces per indentation level (avoid tabs if possible).\n\nWhitespace: use it in a variety of places. Some places where it is good to have it are\n\naround operators (assignment and arithmetic);\nbetween function arguments;\nbetween list/tuple elements; and\nbetween matrix/array indices.\n\nUse blank lines to separate blocks of code with comments to say what the block does.\nUse whitespaces or parentheses for clarity even if not needed for order of operations. For example, a/y*x will work but is not easy to read and you can easily induce a bug if you forget the order of ops. Instead, use a/y * x.\nAvoid code lines longer than 79 characters and comment/docstring lines longer than 72 characters.\nComments: add lots of comments (but don’t belabor the obvious, such as x = x + 1  # increment x).\n\nRemember that in a few months, you may not follow your own code any better than a stranger.\nSome key things to document: (1) summarizing a block of code, (2) explaining a very complicated piece of code - recall our complicated regular expressions, and (3) explaining arbitrary constant values.\nComments should generally be complete sentences.\n\nYou can use parentheses to group operations such that they can be split up into lines and easily commented, e.g.,\n\nnewdf = (\n        pd.read_csv('file.csv')\n        .rename(columns = {'STATE': 'us_state'})  # adjust column names\n        .dropna()                                 # remove some rows\n        )\n\nFor software development, break code into separate files (2000-3000 lines per file) with meaningful file names and related functions grouped within a file.\nBeing consistent about the naming style for objects and functions is hard, but try to be consistent. PEP8 suggests:\n\nClass names should be UpperCamelCase.\nFunction, method, and variable names should be snake_case, e.g., number_of_its or n_its.\nNon-public methods and variables should have a leading underscore.\n\nTry to have the names be informative without being overly long.\nDon’t overwrite names of objects/functions that already exist in Python. E.g., don’t use len. That said, the namespace system helps with the unavoidable cases where there are name conflicts.\nUse active names for functions (e.g., calc_loglik, calc_log_lik rather than loglik or loglik_calc). The idea is that a function in a programming language is like a verb in regular language (a function does something), so use a verb to name it.\nLearn from others’ code\n\nThis semester, someone will be reading your code - the GSI and and me when we look at your assignments. So to help us in understanding your code and develop good habits, put these ideas into practice in your assignments.\nWhile not Python examples, the files goodCode.R and badCode.R in the units directory of the class repository provide examples of code written such that it does and does not conform to the general ideas listed above (leaving aside the different syntax of Python and R)."
  },
  {
    "objectID": "units/unit4-goodPractices.html#coding-style",
    "href": "units/unit4-goodPractices.html#coding-style",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Coding style",
    "text": "Coding style\nThis is particularly focused on software development, but some of the ideas are useful for data analysis as well.\n\nBreak down tasks into core units\nWrite reusable code for core functionality and keep a single copy of the code (using version control or at least with a reasonable backup strategy) so you only need to make changes to a piece of code in one place\nSmaller functions are easier to debug, easier to understand, and can be combined in a modular fashion (like the UNIX utilities)\nWrite functions that take data as an argument and not lines of code that operate on specific data objects. Why? Functions allow us to reuse blocks of code easily for later use and for recreating an analysis (reproducible research). It’s more transparent than sourcing a file of code because the inputs and outputs are specified formally, so you don’t have to read through the code to figure out what it does.\nFunctions should:\n\nbe modular (having a single task);\nhave meaningful name; and\nhave a doc string describing their purpose, inputs and outputs.\n\nWrite tests for each function (i.e., unit tests)\nDon’t hard code numbers - use variables (e.g., number of iterations, parameter values in simulations), even if you don’t expect to change the value, as this makes the code more readable. For example, the speed of light is a constant in a scientific sense, but best to make it a variable in code: speed_of_light = 3e8\nUse lists or tuples to keep disparate parts of related data together\nPractice defensive programming (see also the discussion below on assertions)\n\ncheck function inputs and warn users if the code will do something they might not expect or makes particular choices;\ncheck inputs to if:\n\nNote that in Python, an expression used as the condition of an if will be equivalent to True unless it is one of False, 0, None, or an empty list/tuple/string.\n\nprovide reasonable default arguments;\ndocument the range of valid inputs;\ncheck that the output produced is valid; and\nstop execution based on checks and give an informative error message.\n\nTry to avoid system-dependent code that only runs on a specific version of an OS or specific OS\nLearn from others’ code\nConsider rewriting your code once you know all the settings and conditions; often analyses and projects meander as we do our work and the initial plan for the code no longer makes sense and the code is no longer designed specifically for the job being done."
  },
  {
    "objectID": "units/unit4-goodPractices.html#assertions-exceptions-and-testing",
    "href": "units/unit4-goodPractices.html#assertions-exceptions-and-testing",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Assertions, exceptions and testing",
    "text": "Assertions, exceptions and testing\nAssertions, exceptions and testing are critically important for writing robust code that is less likely to contain bugs.\n\nExceptions\nYou’ve probably already seen exceptions in action whenever you’ve done something in Python that causes an error to occur and an error message to be printed. Syntax errors are different from exceptions in that exceptions occur when the syntax is correct but the execution of the code results in some sort of error.\nExceptions can be a valuable tool for making your code handle different modes of failure (missing file, URL unreachable, permission denied, invalid inputs, etc.). You use them when you are writing code that is supposed to perform a task (e.g., a function that does something with an input file) to indicate that the task failed and the reason for that failure (e.g., the file was not there in the first place). In such case, you want your code to raise an exception and make the error message informative as possible, typically by handling the exception thrown by another function you call and augmenting the message. Another possibility is that your code detects a situation where you need to throw an exception (e.g., an invalid input to a function).\nThe other side of the coin happens when you want to write a piece of code that handles failure in a specific way, instead of simply giving up. For example, if you are writing a script that reads and downloads hundreds of URLs, you don’t want your program to stop when any of them fails to respond (or do you?). You might want to continue with the rest of the URLs, and write out the failed URLs in a secondary output file.\n\nUsing try-except to continue execution\nIf you want some code to continue running even when it encounters an error, you can use try-except. This would often be done in code where you were running some workflow rather than in functions that you write for general purpose use (e.g., code in a package you are writing).\nSuppose we have a loop and we want to run all the iterations even if the code for some iterations fail. We can embed the code that might fail in a try block and then in the except block, run code that will handle the situation when the error occurs.\n\nfor i in range(n):\n    try:\n        &lt;some code that might fail&gt;\n        result[i] = &lt;actual result&gt;\n    except:\n        &lt;what to do if the code fails&gt;\n        result[i] = None   \n\n\n\nStrategies for invoking and handling errors\nHere we’ll address situations that might arise when you are developing code for general purpose use (e.g., writing functions in a package) and need that code to invoke an error under certain circumstances or deal gracefully with an error occurring in some code that you are calling.\nA basic situation is when you want to detect a situation where you need to invoke an error (i.e., throw an exception).\nWith raise you can invoke an exception. Suppose we need an input to be a positive number. We’ll use Python’s built-in ValueError, one of the various exception types that Python provides and that you could use. You can also create your own exceptions by subclassing one of Python’s existing exception classes. (We haven’t yet discussed classes and object-oriented programming, so don’t worry if you’re not sure about what that means.)\n\ndef myfun(val):\n    if val &lt;= 0:\n        raise ValueError(\"`val` should be positive\")\n\nmyfun(-3)\n\nValueError: `val` should be positive\n\n\nNext let’s consider cases where your function runs some code that might return an error.\nWe’d often want to catch the error using try-except. In some cases we would want to notify the user and then continue (perhaps falling back to a different way to do things or returning None from our function) while in others we might want to provide a more informative error message than if we had just let the error occur, but still have the exception be raised.\nFirst let’s see the case of continuing execution.\n\nimport os\n\ndef myfun(filename):\n    try:\n        with open(filename, \"r\") as file:\n            text = file.read()\n    except Exception as err:\n        print(f\"{err}\\nCheck that the file `{filename}` can be found \"\\\n              f\"in the current path: `{os.getcwd()}`.\")\n        return None\n\n    return(text.lower())\n\n\nmyfun('missing_file.txt')\n\n[Errno 2] No such file or directory: 'missing_file.txt'\nCheck that the file `missing_file.txt` can be found in the current path: `/accounts/vis/paciorek/teaching/243fall23/stat243-fall-2023/units`.\n\n\nFinally let’s see how we can intercept an error but then “re-raise” the error rather than continuing execution.\n\nimport requests\n\ndef myfun(url):\n    try:\n        requests.get(url)\n    except Exception as err:\n        print(f\"There was a problem accessing {url}. \"\\\n              f\"Perhaps it doesn't exist or the URL has a typo?\")\n        raise\n\nmyfun(\"http://missingurl.com\")\n\nThere was a problem accessing http://missingurl.com. Perhaps it doesn't exist or the URL has a typo?\n\n\nConnectionError: HTTPConnectionPool(host='missingurl.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f348db44590&gt;: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\n\n\n\n\nAssertions\nAssertions are a quick way to raise a specific type of Exception (AssertionError). Assertions are useful for performing quick checks in your code that the state of the program is as you expect. They’re primarily intended for use during the development process to provide “sanity checks” that specific conditions are true, and there are ways to disable them when you are running your code for production purposes (to improve performance). A common use is for verifying preconditions and postconditions (especially preconditions). One would generally only expect such conditions not to be true if there is a bug in the code. Here’s an example of using the assert statement in Python, with a clear assertion message telling the developer what the problem is.\n\nnumber = -42\nassert number &gt; 0, f\"number greater than 0 expected, got: {number}\"\n## Produces this error:\n## Traceback (most recent call last):\n##   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n## AssertionError: number greater than 0 expected, got: -42\n\nVarious operators/functions are commonly used in assertions, including\n\nassert x in y\nassert x not in y\nassert x is y\nassert x is not y\nassert isinstance(x, &lt;some_type&gt;)\nassert all(x)\nassert any(x)\n\n\n\nTesting\nTesting is informally what you do after you write some code and want to check that it actually works. But when you are developing important code (e.g. functions that are going to be used by others) you typically want to encode your tests in code. There are many reasons to do that, including making sure that if someone else changes your code later on without fully understanding what it was supposed to do, the test suite should immediately indicate that.\nSome people even advocate for writing a preliminary test suite before writing the code itself(!) as it can be a good way to organize work and track progress, as well as act as a secondary form of documentation for clarity. This can include tests that your code provides correct and useful errors when something goes wrong (so that means that a test might be to see if problematic input correctly produces an error). Unit tests are intended to test the behavior of small pieces (units) of code, generally individual functions. Unit tests naturally work well with the ideas above of writing small, modular functions. I recommend the pytest package, which is designed to make it easier to write sets of good tests.\nIn lab, we’ll go over assertions, exceptions, and testing in detail."
  },
  {
    "objectID": "units/unit4-goodPractices.html#version-control",
    "href": "units/unit4-goodPractices.html#version-control",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Version control",
    "text": "Version control\n\nUse it! Even for projects that only you are working on. It’s the closest thing you’ll get to having a time machine!\nUse an issues tracker (e.g., the GitHub issues tracker is quite nice), or at least a simple to-do file, noting changes you’d like to make in the future.\nIn addition to good commit messages, it’s a good idea to keep good running notes documenting your projects.\n\nWe’ve already seen Git some and will see it in a lot more detail later in the semester, so I don’t have more to say here."
  },
  {
    "objectID": "units/unit4-goodPractices.html#basic-debugging-strategies",
    "href": "units/unit4-goodPractices.html#basic-debugging-strategies",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Basic debugging strategies",
    "text": "Basic debugging strategies\nHere we’ll discuss some basic strategies for finding and fixing bugs. Other useful locations for tips on debugging include:\n\nEfficient Debugging by Goldspink\nDebugging for Beginners by Brody\n\nRead and think about the error message (the traceback), starting from the bottom of the traceback. Sometimes it’s inscrutable, but often it just needs a bit of deciphering. Looking up a given error message by simply doing a web search with the exact message in double quotes can be a good strategy, or you could look specifically on Stack Overflow.\nBelow we’ll see how one can view the stack trace. Usually when an error occurs, it occurs in a function call that is nested in a series of function calls. This series of calls is the call stack and the traceback or stack trace shows that series of calls that led to the error. To debug, you’ll often need to focus on the function being executed at the time the error occurred (which will be at the top of the call stack but the bottom of the traceback) and the arguments passed into that function. However, if the error occurs in a function you didn’t write, the problem will often be with the arguments that your code provided at the last point in the call stack at which code that you wrote was run. Check the arguments that your code passed into that first function that is not a function of yours.\nWhen running code that produces multiple errors, fix errors from the top down - fix the first error that is reported, because later errors are often caused by the initial error. It’s common to have a string of many errors, which looks daunting, caused by a single initial error.\nIs the bug reproducible - does it always happen in the same way at at the same point? It can help to restart Python and see if the bug persists - this can sometimes help in figuring out if there is a scoping issue and we are using a global variable that we did not mean to.\nIf you can’t figure out where the error occurs based on the error messages, a basic strategy is to build up code in pieces (or tear it back in pieces to a simpler version). This allows you to isolate where the error is occurring. You might use a binary search strategy. Figure out which half of the code the error occurs in. Then split the ‘bad’ half in half and figure out which half the error occurs in. Repeat until you’ve isolated the problem.\nIf you’ve written your code modularly with lots of functions, you can test individual functions. Often the error will be in what gets passed into and out of each function.\nAt the beginning of time (the 1970s?), the standard debugging strategy was to insert print statements in one’s code to see the value of a variable and thereby decipher what could be going wrong. We have better tools nowadays. But sometimes we still need to fall back to inserting print statements.\nPython is a scripting language, so you can usually run your code line by line to figure out what is happening. This can be a decent approach, particularly for simple code. However, when you are trying to find errors that occur within a series of many nested function calls or when the errors involve variable scoping (how Python looks for variables that are not local to a function), or in other complicated situations, using formal debugging tools can be much more effective. Finally, if the error occurs inside of functions provided by Python, rather than ones you write, it can be hard to run the code in those functions line by line."
  },
  {
    "objectID": "units/unit4-goodPractices.html#using-pdb",
    "href": "units/unit4-goodPractices.html#using-pdb",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Using pdb",
    "text": "Using pdb\nWe can activate the debugger in various ways:\n\nby inserting breakpoint() (or equivalently import pdb; pdb.set_trace()) inside a function or module at a location of interest (and then running the function or module)\nby using pdb.pm() after an error (i.e., an exception) has occurred to invoke the browser at the point the error occurred\nby running a function under debugger control with pdb.run()\nby starting python with python -m pdb file.py and adding breakpoints\n\n\nUsing breakpoint\nLet’s define a function that will run a stratified analysis, in this case fitting a regression to each of the strata (groups/clusters) in some data. Our function is in stratified_with_break.py, and it contains breakpoint at the point where we want to invoke the debugger.\nNow I can call the function and will be put into debugging mode just before the next line is called:\n\nimport run_with_break as run\nrun.fit(run.data, run.n_cats)\n\nWhen I run this, I see this:\n&gt;&gt;&gt; run.fit(data, n_cats)\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_with_break.py(10)fit()\n-&gt; sub = data[data['cats'] == i]\n(Pdb) \nThis indicates I am debugging at line 10 of run_with_break.py, which is the line that creates sub, but I haven’t yet created sub.\nI can type n to run that line and go to the next one:\n(Pdb) n\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_with_break.py(11)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\nat which point the debugger is about to execute line 11, which fits the regression.\nI can type c to continue until the next breakpoint:\n(Pdb) c\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_with_break.py(10)fit()\n-&gt; sub = data[data['cats'] == i]\nNow if I print i, I see that it has incremented to 1.\n(Pdb) p i\n1\nWe could keep hitting n or c until hitting the stratum where an error occurs, but that would be tedious.\nLet’s hit q to quit out of the debugger.\n(Pdb) q\n&gt;&gt;&gt;\nNext let’s see how we can enter debugging mode only at point an error occurs.\n\n\nPost-mortem debugging\nWe’ll use a version of the module without the breakpoint() command.\n\nimport pdb\nimport run_no_break as run \n\nrun.fit(run.data, run.n_cats)\npdb.pm()\n\nThat puts us into debugging mode at the point the error occurred:\n&gt; /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py(86)_wrapreduction()\n-&gt; return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n(Pdb)\nwhich turns out to be in some internal Python function that calls a reduce function, which is where the error occurs (presumably the debugger doesn’t enter this function because it calls compiled code):\n(Pdb) l\n 81                 if dtype is not None:\n 82                     return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n 83                 else:\n 84                     return reduction(axis=axis, out=out, **passkwargs)\n 85     \n 86  -&gt;     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n 87     \n 88     \n 89     def _take_dispatcher(a, indices, axis=None, out=None, mode=None):\n 90         return (a, out)\n 91     \nWe can enter u multiple times (it’s only shown once below) to go up in the stack of function calls until we recognize code that we wrote:\n(Pdb) u\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break.py(10)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\nNow let’s use p to print variable values to understand the problem:\n(Pdb)  p i\n29\n(Pdb) p sub\nEmpty DataFrame\nColumns: [y, x, cats]\nIndex: []\nAh, so in the 29th stratum there are no data!\nIn addition using the IPython magic %debug will put you into the debugger in in post-mortem mode when an error occurs.\n\n\npdb commands\nHere’s a list of useful pdb commands (some of which we saw above) that you can use once you’ve entered debugging mode.\n\nh or help: shows all the commands\nl or list: show the code around where the debugger is currently operating\nc or continue: continue running the code until the next breakpoint\np or print: print a variable\nn or next: run the current line and go to the next line in the current function\ns or step: jump (step) into the function called in the current line (if it’s a Python function)\nr or run: exit out of the current function (e.g., if you accidentally stepped into a function) (but note this stops at breakpoints)\nunt or until: run until the next line (or unt &lt;number&gt; to run until reaching line number ); this is useful for letting a loop run until completion\nb or break: set a breakpoint\ntbreak: one-time breakpoint\nwhere: shows call stack\nu (or up) and d (or down): move up and down the call stack\nq quit out of the debugger\n&lt;return&gt;: runs the previous pdb command again\n\n\n\nInvoking pdb on a function or block of code\nWe can use pdb.run() to run a function under the debugger. We need to make sure to use s as the first pdb command in order to actually step into the function. From there, we can debug as normal as if we had set a breakpoint at the start of the function.\n\nimport run_with_break as run\nimport pdb\npdb.run(\"run.fit(run.data, run.n_cats)\")\n(Pdb) s\n\n\n\nInvoking pdb on a module\nWe can also invoke pdb when we start Python, executing a file (module). Here we’ve added fit(data, n_cats) at the end of run_no_break2.py so that we can have that run under the debugger.\n#| eval: false\npython -m pdb run_no_break2.py\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break2.py(1)&lt;module&gt;()\n-&gt; import numpy as np\n(Pdb) \nLet’s set a breakpoint at the same place we did with breakpoint() but using a line number (this avoids having to actually modify our code):\n(Pdb) b 9\nBreakpoint 1 at /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break.py:9\n\n(Pdb) c\n&gt; /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break2.py(9)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\n\nSo we’ve broken at the same point where we manually added breakpoint() in run_with_break.py.\nOr we could have set a breakpoint at the start of the function:\n(Pdb) disable 1\nDisabled breakpoint 1 at /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break2.py:9\n(Pdb) b fit\nBreakpoint 1 at /accounts/vis/paciorek/teaching/243fall22/stat243-fall-2023/units/run_no_break.py:6"
  },
  {
    "objectID": "units/unit4-goodPractices.html#some-common-causes-of-bugs",
    "href": "units/unit4-goodPractices.html#some-common-causes-of-bugs",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Some common causes of bugs",
    "text": "Some common causes of bugs\nSome of these are Python-specific, while others are common to a variety of languages.\n\nParenthesis mis-matches\n== vs. =\nComparing real numbers exactly using == is dangerous because numbers on a computer are only represented to limited numerical precision. For example,\n::: {.cell execution_count=12} {.python .cell-code}  1/3 == 4*(4/12-3/12)\n::: {.cell-output .cell-output-display execution_count=12} False ::: :::\nYou expect a single value but execution of the code gives an array\nSilent type conversion when you don’t want it, or lack of coercion where you’re expecting it\nUsing the wrong function or variable name\nGiving unnamed arguments to a function in the wrong order\nForgetting to define a variable in the environment of a function and having Python, via lexical scoping, get that variable as a global variable from one of the enclosing scope. At best the types are not compatible and you get an error; at worst, you use a garbage value and the bug is hard to trace. In some cases your code may work fine when you develop the code (if the variable exists in the enclosing environment), but then may not work when you restart Python if the variable no longer exists or is different.\nPython (usually helpfully) drops matrix and array dimensions that are extraneous. This can sometimes confuse later code that expects an object of a certain dimension. More on this below."
  },
  {
    "objectID": "units/unit4-goodPractices.html#tips-for-avoiding-bugs-and-catching-errors",
    "href": "units/unit4-goodPractices.html#tips-for-avoiding-bugs-and-catching-errors",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Tips for avoiding bugs and catching errors",
    "text": "Tips for avoiding bugs and catching errors\n\nPractice defensive programming\nWhen writing functions, and software more generally, you’ll want to warn the user or stop execution when there is an error and exit gracefully, giving the user some idea of what happened. Here are some things to consider:\n\ncheck function inputs and warn users if the code will do something they might not expect or makes particular choices;\ncheck inputs to if and the ranges in for loops;\nprovide reasonable default arguments;\ndocument the range of valid inputs;\ncheck that the output produced is valid; and\nstop execution based on assertions, try or raise with an informative error message.\n\nHere’s an example of building a robust square root function:\n\nimport warnings\n\ndef mysqrt(x):\n    assert not isinstance(x, str), f\"what is the square root of '{x}'?\"\n    if isinstance(x, int) or isinstance(x, float):\n        if x &lt; 0:\n            warnings.warn(\"Input value is negative.\", UserWarning)\n            return float('nan')   # avoid complex number result\n        else:\n            return x**0.5\n    else:\n        raise ValueError(f\"Cannot take the square root of {x}\")\n\n\nmysqrt(3.1)\nmysqrt(-3)\ntry:\n    mysqrt('hat')\nexcept Exception as error:\n    print(error)\n\nwhat is the square root of 'hat'?\n\n\n/tmp/ipykernel_3887481/1815678236.py:7: UserWarning:\n\nInput value is negative.\n\n\n\n\n\nCatch run-time errors with try/except statements\nAlso, sometimes a function you call will fail, but you want to continue execution. For example, consider the stratified analysis show previously in which you take subsets of your data based on some categorical variable and fit a statistical model for each value of the categorical variable. If some of the subsets have no or very few observations, the statistical model fitting might fail. To do this, you might be using a for loop or apply. You want your code to continue and fit the model for the rest of the cases even if one (or more) of the cases cannot be fit. You can wrap the function call that may fail within the try statement and then your code won’t stop, even when an error occurs. Here’s a toy example.\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport statsmodels.api\n\nnp.random.seed(2)\nn_cats = 30\nn = 80\ny = np.random.normal(size=n)\nx = np.random.normal(size=n)\ncats = [np.random.randint(0, n_cats-1) for _ in range(n)]\ndata = pd.DataFrame({'y': y, 'x': x, 'cats': cats})\n\nparams = np.full((n_cats, 2), np.nan)\nfor i in range(n_cats):\n    sub = data[data['cats'] == i]\n    try:\n        model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\n        fit = model.fit()\n        params[i, :] = fit.params.values\n    except Exception as error:\n        print(f\"Regression cannot be fit for stratum {i}.\")\n\nprint(params)\n\nRegression cannot be fit for stratum 7.\nRegression cannot be fit for stratum 20.\nRegression cannot be fit for stratum 24.\nRegression cannot be fit for stratum 29.\n[[ 5.52897442e-01  2.61511154e-01]\n [ 5.72564369e-01  4.37210543e-02]\n [-9.91086764e-01  2.84116572e-01]\n [-6.50606465e-01  4.26310060e-01]\n [-2.59058826e+00 -2.59058826e+00]\n [ 8.59455139e-01 -4.64514288e+00]\n [ 3.82737032e-06  3.82737032e-06]\n [            nan             nan]\n [-5.55478634e-01 -1.17864561e-01]\n [-9.11601460e-02 -5.91519525e-01]\n [-7.30270153e-01 -1.99976841e-01]\n [-1.14495705e-01 -3.06421213e-02]\n [ 4.01648095e-01  9.30890661e-01]\n [ 7.88388728e-01 -1.45835443e+00]\n [ 4.08462508e+01  6.89262864e+01]\n [ 2.95467536e-01  8.80528901e-01]\n [ 1.04592517e+00  4.55379445e+00]\n [ 6.99549010e-01 -5.17503241e-01]\n [-1.75642254e+00 -8.07798224e-01]\n [-4.49033150e-02  3.53455362e-01]\n [            nan             nan]\n [ 2.63097970e-01  2.63097970e-01]\n [ 1.13328314e+00 -1.39985074e-01]\n [ 1.17996663e+00  3.68770563e-01]\n [            nan             nan]\n [-3.85101497e-03 -3.85101497e-03]\n [-8.04536124e-01 -5.19470059e-01]\n [-5.19200779e-01 -1.39952387e-01]\n [-9.16593858e-01 -2.67613324e-01]\n [            nan             nan]]\n\n\nThe stratum with id 7 had no observations, so that call to do the regression failed, but the loop continued because we ‘caught’ the error with try. In this example, we could have checked the sample size for the subset before doing the regression, but in other contexts, we may not have an easy way to check in advance whether the function call will fail.\n\n\nMaintain dimensionality\nPython (usually helpfully) drops array dimensions that are extraneous. This can sometimes confuse later code that expects an object of a certain dimension. Here’s a work-around:\n\nimport numpy as np\nmat = np.array([[1, 2], [3, 4]])\nnp.sum(mat, axis=0)         # This sums columns, as desired\n\nrow_subset = 1\nmat2 = mat[row_subset, :]\nnp.sum(mat2, axis=0)        # This sums the elements, not the columns.\n\nif len(mat2.shape) != 2:    # Fix dimensionality.\n    mat2 = mat2.reshape(1, -1)\n\n\nnp.sum(mat2, axis=0)   \n\narray([3, 4])\n\n\nIn this simple case it’s obvious that a dimension will be dropped, but in more complicated settings, this can easily occur for some inputs without the coder realizing that it may happen. Not dropping dimensions is much easier than putting checks in to see if dimensions have been dropped and having the code behave differently depending on the dimensionality.\n\n\nFind and avoid global variables\nIn general, using global variables (variables that are not created or passed into a function) results in code that is not robust. Results will change if you or a user modifies that global variable, usually without realizing/remembering that a function depends on it.\nOne ad hoc strategy is to remove objects you don’t need from Python’s global scope, to avoid accidentally using values from an old object via Python’s scoping rules. You can also run your function in a fresh session to see if it’s unable to find variables.\n\ndel x   # Mimic having a fresh sesson (knowing in this case `x` is global).\n\ndef f(z):\n    y = 3\n    print(x + y + z)\n\ntry:\n    f(2)\nexcept Exception as error:\n    print(error)\n\nname 'x' is not defined\n\n\n\n\nMiscellaneous tips\n\nUse core Python functionality and algorithms already coded. Figure out if a functionality already exists in (or can be adapted from) an Python package (or potentially in a C/Fortran library/package): code that is part of standard mathematical/numerical packages will probably be more efficient and bug-free than anything you would write.\nCode in a modular fashion, making good use of functions, so that you don’t need to debug the same code multiple times. Smaller functions are easier to debug, easier to understand, and can be combined in a modular fashion (like the UNIX utilities).\nWrite code for clarity and accuracy first; then worry about efficiency. Write an initial version of the code in the simplest way, without trying to be efficient (e.g., you might use for loops even if you’re coding in Python); then make a second version that employs efficiency tricks and check that both produce the same output.\nPlan out your code in advance, including all special cases/possibilities.\nWrite tests for your code early in the process.\nBuild up code in pieces, testing along the way. Make big changes in small steps, sequentially checking to see if the code has broken on test case(s).\nBe careful that the conditions of if statements and the sequences of for loops are robust when they involve evaluating R code.\nDon’t hard code numbers - use variables (e.g., number of iterations, parameter values in simulations), even if you don’t expect to change the value, as this makes the code more readable and reduces bugs when you use the same number multiple times; e.g. speed_of_light = 3e8 or n_its = 1000.\n\nIn a future Lab, we’ll go over debugging in detail."
  },
  {
    "objectID": "units/unit4-goodPractices.html#some-basic-strategies",
    "href": "units/unit4-goodPractices.html#some-basic-strategies",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Some basic strategies",
    "text": "Some basic strategies\n\nHave a directory for each project with subdirectories with meaningful and standardized names: e.g., code, data, paper. The Journal of the American Statistical Association (JASA) has a template GitHub repository with some suggestions.\nHave a file of code for pre-processing, one or more for analysis, and one for figure/table preparation.\n\nThe pre-processing may involve time-consuming steps. Save the output of the pre-processing as a file that can be read in to the analysis script.\nYou may want to name your files something like this, so there is an obvious ordering: “1-prep.py”, “2-analysis.py”, “3-figs.py”.\nHave the code file for the figures produce the exact manuscript/report figures, operating on a file (e.g., a pickle file) that contains all the objects necessary to run the figure-producing code; the code producing the pickle file should be in your analysis code file (or somewhere else sensible).\nAlternatively, use Quarto or Jupyter notebooks for your document preparation.\n\nKeep a document describing your running analysis with dates in a text file (i.e., a lab book).\nNote where data were obtained (and when, which can be helpful when publishing) and pre-processing steps in the lab book. Have data version numbers with a file describing the changes and dates (or in lab book). If possible, have all changes to data represented as code that processes the data relative to a fixed baseline dataset.\nNote what code files do what in the lab book.\nKeep track of the details of the system and software you are running your code under, e.g., operating system version, software (e.g., Python,\n\nversions, Python or R package versions, etc.\n\n\npip list and conda list will show you version numbers for installed packages."
  },
  {
    "objectID": "units/unit4-goodPractices.html#formal-tools",
    "href": "units/unit4-goodPractices.html#formal-tools",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Formal tools",
    "text": "Formal tools\n\nIn some cases you may be able to carry out your complete workflow in a Quarto document or in a Jupyter notebook.\nYou might consider workflow/pipeline management software such as Drake or other tools discussed in the CRAN Reproducible Research Task View. Alternatively, one can use the make tool, which is generally used for compiling code, as a tool for reproducible research: if interested, see the tutorial on Using make for workflows or this Journal of Statistical Software article for more details.\nYou might organize your workflow as a Python or R package as described (for the R case) in this article.\nPackage management:\n\nPython: You can manage the versions of Python packages (and dependent packages) used in your project using Conda environments (or virtualenvs).\nR: You can manage the versions of R packages (and dependent packages) used in your project using package management packages such as renv and packrat. Unfortunately, the useful checkpoint package relies on snapshots of CRAN that are not available after January 2023.\n\nIf your project uses multiple pieces of software (e.g., not just Python or R), you can set up a reproducible environment using containers, of which Docker containers are the best known. These provide something that is like a lightweight virtual machine in which you can install exactly the software (and versions) you want and then share with others. Docker container images are a key building block of various tools such as GitHub Actions and the Binder project. Alternatively Conda is a general package manager that can install lots of non-Python packages and can also be used in many circumstances."
  },
  {
    "objectID": "units/unit7-bigData.html",
    "href": "units/unit7-bigData.html",
    "title": "Big data and databases",
    "section": "",
    "text": "PDF\nReferences:\nI’ve also pulled material from a variety of other sources, some mentioned in context below.\nNote that for a lot of the demo code I ran the code separately from rendering this document because of the time involved in working with large datasets.\nWe’ll focus on Dask and databases/SQL in this Unit. The material on using Spark is provided for reference, but you’re not responsible for that material. If you’re interested in working with big datasets in R or with tools other than Dask in Python, there is some material in the tutorial on working with large datasets."
  },
  {
    "objectID": "units/unit7-bigData.html#an-editorial-on-big-data",
    "href": "units/unit7-bigData.html#an-editorial-on-big-data",
    "title": "Big data and databases",
    "section": "An editorial on ‘big data’",
    "text": "An editorial on ‘big data’\n‘Big data’ was trendy these days, though I guess it’s not quite the buzzword/buzzphrase that it was a few years ago, given the AI/ML revolution, but of course that revolution is largely based on having massive datasets available online.\nPersonally, I think some of the hype around giant datasets is justified and some is hype. Large datasets allow us to address questions that we can’t with smaller datasets, and they allow us to consider more sophisticated (e.g., nonlinear) relationships than we might with a small dataset. But they do not directly help with the problem of correlation not being causation. Having medical data on every American still doesn’t tell me if higher salt intake causes hypertension. Internet transaction data does not tell me if one website feature causes increased viewership or sales. One either needs to carry out a designed experiment or think carefully about how to infer causation from observational data. Nor does big data help with the problem that an ad hoc ‘sample’ is not a statistical sample and does not provide the ability to directly infer properties of a population. Consider the immense difficulties we’ve seen in answering questions about Covid despite large amounts of data, because it is incomplete/non-representative. A well-chosen smaller dataset may be much more informative than a much larger, more ad hoc dataset. However, having big datasets might allow you to select from the dataset in a way that helps get at causation or in a way that allows you to construct a population-representative sample. Finally, having a big dataset also allows you to do a large number of statistical analyses and tests, so multiple testing is a big issue. With enough analyses, something will look interesting just by chance in the noise of the data, even if there is no underlying reality to it.\nDifferent people define the ‘big’ in big data differently. One definition involves the actual size of the data, and in some cases the speed with which it is collected. Our efforts here will focus on dataset sizes that are large for traditional statistical work but would probably not be thought of as large in some contexts such as Google or the US National Security Agency (NSA). Another definition of ‘big data’ has more to do with how pervasive data and empirical analyses backed by data are in society and not necessarily how large the actual dataset size is."
  },
  {
    "objectID": "units/unit7-bigData.html#logistics-and-data-size",
    "href": "units/unit7-bigData.html#logistics-and-data-size",
    "title": "Big data and databases",
    "section": "Logistics and data size",
    "text": "Logistics and data size\nOne of the main drawbacks with Python (and R) in working with big data is that all objects are stored in memory, so you can’t directly work with datasets that are more than 1-20 Gb or so, depending on the memory on your machine.\nThe techniques and tools discussed in this Unit (apart from the section on MapReduce/Spark) are designed for datasets in the range of gigabytes to tens of gigabytes, though they may scale to larger if you have a machine with a lot of memory or simply have enough disk space and are willing to wait. If you have 10s of gigabytes of data, you’ll be better off if your machine has 10s of GBs of memory, as discussed in this Unit.\nIf you’re scaling to 100s of GBs, terabytes or petabytes, tools such as carefully-administered databases, cloud-based tools such as provided by AWS and Google Cloud Platform, and Spark or other such tools are probably your best bet.\nNote: in handling big data files, it’s best to have the data on the local disk of the machine you are using to reduce traffic and delays from moving data over the network."
  },
  {
    "objectID": "units/unit7-bigData.html#what-we-already-know-about-handling-big-data",
    "href": "units/unit7-bigData.html#what-we-already-know-about-handling-big-data",
    "title": "Big data and databases",
    "section": "What we already know about handling big data!",
    "text": "What we already know about handling big data!\nUNIX operations are generally very fast, so if you can manipulate your data via UNIX commands and piping, that will allow you to do a lot. We’ve already seen UNIX commands for extracting columns. And various commands such as grep, head, tail, etc. allow you to pick out rows based on certain criteria. As some of you have done in problem sets, one can use awk to extract rows. So basic shell scripting may allow you to reduce your data to a more manageable size.\nThe tool GNU parallel allows you to parallelize operations from the command line and is commonly used in working on Linux clusters.\nAnd don’t forget simple things. If you have a dataset with 30 columns that takes up 10 Gb but you only need 5 of the columns, get rid of the rest and work with the smaller dataset. Or you might be able to get the same information from a random sample of your large dataset as you would from doing the analysis on the full dataset. Strategies like this will often allow you to stick with the tools you already know.\nAlso, remember that we can often store data more compactly in binary formats than in flat text (e.g., csv) files.\nFinally, for many applications, storing large datasets in a standard database will work well."
  },
  {
    "objectID": "units/unit7-bigData.html#overview",
    "href": "units/unit7-bigData.html#overview",
    "title": "Big data and databases",
    "section": "Overview",
    "text": "Overview\nA basic paradigm for working with big datasets is the MapReduce paradigm. The basic idea is to store the data in a distributed fashion across multiple nodes and try to do the computation in pieces on the data on each node. Results can also be stored in a distributed fashion.\nA key benefit of this is that if you can’t fit your dataset on disk on one machine you can on a cluster of machines. And your processing of the dataset can happen in parallel. This is the basic idea of MapReduce.\nThe basic steps of MapReduce are as follows:\n\nread individual data objects (e.g., records/lines from CSVs or individual data files)\nmap: create key-value pairs using the inputs (more formally, the map step takes a key-value pair and returns a new key-value pair)\nreduce: for each key, do an operation on the associated values and create a result - i.e., aggregate within the values assigned to each key\nwrite out the {key,result} pair\n\nA similar paradigm that is implemented in pandas and dplyr is the split-apply-combine strategy.\nA few additional comments. In our map function, we could exclude values or transform them in some way, including producing multiple records from a single record. And in our reduce function, we can do more complicated analysis. So one can actually do fairly sophisticated things within what may seem like a restrictive paradigm. But we are constrained such that in the map step, each record needs to be treated independently and in the reduce step each key needs to be treated independently. This allows for the parallelization.\nOne important note is that any operations that require moving a lot of data between the workers can take a long time. (This is sometimes called a shuffle.) This could happen if, for example, you computed the median value within each of many groups if the data for each group are spread across the workers. In contrast, if we compute the mean or sum, one can compute the partial sums on each worker and then just add up the partial sums.\nNote that as discussed in Unit 5 the concepts of map and reduce are core concepts in functional programming, and of course Python provides the map function.\nHadoop is an infrastructure for enabling MapReduce across a network of machines. The basic idea is to hide the complexity of distributing the calculations and collecting results. Hadoop includes a file system for distributed storage (HDFS), where each piece of information is stored redundantly (on multiple machines). Calculations can then be done in a parallel fashion, often on data in place on each machine thereby limiting the amount of communication that has to be done over the network. Hadoop also monitors completion of tasks and if a node fails, it will redo the relevant tasks on another node. Hadoop is based on Java. Given the popularity of Spark, I’m not sure how much usage these approaches currently see. Setting up a Hadoop cluster can be tricky. Hopefully if you’re in a position to need to use Hadoop, it will be set up for you and you will be interacting with it as a user/data analyst.\nOk, so what is Spark? You can think of Spark as in-memory Hadoop. Spark allows one to treat the memory across multiple nodes as a big pool of memory. Therefore, Spark should be faster than Hadoop when the data will fit in the collective memory of multiple nodes. In cases where it does not, Spark will make use of the HDFS (and generally, Spark will be reading the data initially from HDFS.) While Spark is more user-friendly than Hadoop, there are also some things that can make it hard to use. Setting up a Spark cluster also involves a bit of work, Spark can be hard to configure for optimal performance, and Spark calculations have a tendency to fail (often involving memory issues) in ways that are hard for users to debug."
  },
  {
    "objectID": "units/unit7-bigData.html#using-dask-for-big-data-processing",
    "href": "units/unit7-bigData.html#using-dask-for-big-data-processing",
    "title": "Big data and databases",
    "section": "Using Dask for big data processing",
    "text": "Using Dask for big data processing\nUnit 6 on parallelization gives an overview of using Dask for flexible parallelization on different kinds of computational resources (in particular, parallelizing across multiple cores on one machine versus parallelizing across multiple cores across multiple machines/nodes).\nHere we’ll see the use of Dask to work with distributed datasets. Dask can process datasets (potentially very large ones) by parallelizing operations across subsets of the data using multiple cores on one or more machines.\nLike Spark, Dask automatically reads data from files in parallel and operates on chunks (also called partitions or shards) of the full dataset in parallel. There are two big advantages of this:\n\nYou can do calculations (including reading from disk) in parallel because each worker will work on a piece of the data.\nWhen the data is split across machines, you can use the memory of multiple machines to handle much larger datasets than would be possible in memory on one machine. That said, Dask processes the data in chunks, so one often doesn’t need a lot of memory, even just on one machine.\n\nWhile reading from disk in parallel is a good goal, if all the data are on one hard drive, there are limitations on the speed of reading the data from disk because of having multiple processes all trying to access the disk at once. Supercomputing systems will generally have parallel file systems that support truly parallel reading (and writing, i.e., parallel I/O). Hadoop/Spark deal with this by distributing across multiple disks, generally one disk per machine/node.\nBecause computations are done in external compiled code (e.g., via numpy) it’s effective to use the threads scheduler when operating on one node to avoid having to copy and move the data.\n\nDask dataframes (pandas)\nDask dataframes are Pandas-like dataframes where each dataframe is split into groups of rows, stored as smaller Pandas dataframes.\nOne can do a lot of the kinds of computations that you would do on a Pandas dataframe on a Dask dataframe, but many operations are not possible. See here.\nBy default dataframes are handled by the threads scheduler. (Recall we discussed Dask’s various schedulers in Unit 6.)\nHere’s an example of reading from a dataset of flight delays (about 11 GB data). You can get the data here.\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.dataframe as ddf\npath = '/scratch/users/paciorek/243/AirlineData/csvs/'\nair = ddf.read_csv(path + '*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1', # (unexpected) latin1 value(s) in TailNum field in 2001\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object', 'DepDelay': 'float64'})\n# specify some dtypes so Pandas doesn't complain about column type heterogeneity\nair\n\nDask will reads the data in parallel from the various .csv.bz2 files (unzipping on the fly), but note the caveat in the previous section about the possibilities for truly parallel I/O.\nHowever, recall that Dask uses delayed evaluation. In this case, the reading is delayed until compute() is called. For that matter, the various other calculations (max, groupby, mean) shown below are only done after compute() is called.\n\nimport time\n\nt0 = time.time()\nair.DepDelay.max().compute()   # this takes a while\nprint(time.time() - t0)\n\nt0 = time.time()\nair.DepDelay.mean().compute()   # this takes a while\nprint(time.time() - t0)\n\nair.DepDelay.median().compute() \n\nWe’ll discuss in class why Dask won’t do the median. Consider the discussion about moving data in the earlier section on MapReduce.\nNext let’s see a full split-apply-combine (aka MapReduce) type of analysis.\n\nsub = air[(air.UniqueCarrier == 'UA') & (air.Origin == 'SFO')]\nbyDest = sub.groupby('Dest').DepDelay.mean()\nresults = byDest.compute()            # this takes a while too\nresults\n\nYou should see this:\n    Dest \n    ACV 26.200000 \n    BFL 1.000000 \n    BOI 12.855069 \n    BOS 9.316795 \n    CLE 4.000000\n    ...\nNote: calling compute twice is a bad idea as Dask will read in the data twice - more on this in a bit.\n\nWarning Think carefully about the size of the result from calling compute. The result will be returned as a standard Python object, not distributed across multiple workers (and possibly machines), and with the object entirely in memory. It’s easy to accidentally return an entire giant dataset.\n\n\n\nDask bags\nBags are like lists but there is no particular ordering, so it doesn’t make sense to ask for the i’th element.\nYou can think of operations on Dask bags as being like parallel map operations on lists in Python or R.\nBy default bags are handled via the processes scheduler.\nLet’s see some basic operations on a large dataset of Wikipedia log files. You can get a subset of the Wikipedia data here.\nHere we again read the data in (which Dask will do in parallel):\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \nimport dask.bag as db\n## This is the full data\n## path = '/scratch/users/paciorek/wikistats/dated_2017/'\n## For demo we'll just use a small subset\npath = '/scratch/users/paciorek/wikistats/dated_2017_small/dated/'\nwiki = db.read_text(path + 'part-0*gz')\n\nHere we’ll just count the number of records.\n\nimport time\nt0 = time.time()\nwiki.count().compute()\ntime.time() - t0   # 136 sec. for full data\n\nAnd here is a more realistic example of filtering (subsetting).\n\nimport re\ndef find(line, regex = 'Armenia'):\n    vals = line.split(' ')\n    if len(vals) &lt; 6:\n        return(False)\n    tmp = re.search(regex, vals[3])\n    if tmp is None:\n        return(False)\n    else:\n        return(True)\n    \n\nwiki.filter(find).count().compute()\narmenia = wiki.filter(find)\nsmp = armenia.take(100) ## grab a handful as proof of concept\nsmp[0:5]\n\nNote that it is quite inefficient to do the find() (and implicitly reading the data in) and then compute on top of that intermediate result in two separate calls to compute(). Rather, we should set up the code so that all the operations are set up before a single call to compute(). This is discussed in detail in the Dask/future tutorial.\nSince the data are just treated as raw strings, we might want to introduce structure by converting each line to a tuple and then converting to a data frame.\n\ndef make_tuple(line):\n    return(tuple(line.split(' ')))\n\ndtypes = {'date': 'object', 'time': 'object', 'language': 'object',\n'webpage': 'object', 'hits': 'float64', 'size': 'float64'}\n\n## Let's create a Dask dataframe. \n## This will take a while if done on full data.\ndf = armenia.map(make_tuple).to_dataframe(dtypes)\ntype(df)\n\n## Now let's actually do the computation, returning a Pandas df\nresult = df.compute()  \ntype(result)\nresult[0:5]\n\n\n\nDask arrays (numpy)\nDask arrays are numpy-like arrays where each array is split up by both rows and columns into smaller numpy arrays.\nOne can do a lot of the kinds of computations that you would do on a numpy array on a Dask array, but many operations are not possible. See here.\nBy default arrays are handled via the threads scheduler.\n\nNon-distributed arrays\nLet’s first see operations on a single node, using a single 13 GB two-dimensional array. Again, Dask uses lazy evaluation, so creation of the array doesn’t happen until an operation requiring output is done.\n\nimport dask\ndask.config.set(scheduler = 'threads', num_workers = 4) \nimport dask.array as da\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(10000, 10000))\n# square 10k x 10k chunks\nmycalc = da.mean(x, axis = 1)  # by row\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0  # 41 sec.\n\nFor a row-based operation, we would presumably only want to chunk things up by row, but this doesn’t seem to actually make a difference, presumably because the mean calculation can be done in pieces and only a small number of summary statistics moved between workers.\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\n# x = da.from_array(x, chunks=(2500, 40000))  # adjust chunk size of existing array\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(2500, 40000))\nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 42 sec.\n\nOf course, given the lazy evaluation, this timing comparison is not just timing the actual row mean calculations.\nBut this doesn’t really clarify the story…\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nimport numpy as np\nimport time\nt0 = time.time()\nx = np.random.normal(0, 1, size=(40000,40000))\ntime.time() - t0   # 110 sec.\n# for some reason the from_array and da.mean calculations are not done lazily here\nt0 = time.time()\ndx = da.from_array(x, chunks=(2500, 40000))\ntime.time() - t0   # 27 sec.\nt0 = time.time()\nmycalc = da.mean(x, axis = 1)  # what is this doing given .compute() also takes time?\ntime.time() - t0   # 28 sec.\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 21 sec.\n\nDask will avoid storing all the chunks in memory. (It appears to just generate them on the fly.) Here we have an 80 GB array but we never use more than a few GB of memory (based on top or free -h).\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nx = da.random.normal(0, 1, size=(100000,100000), chunks=(10000, 10000))\nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 205 sec.\nrs[0:5]\n\n\n\nDistributed arrays\nUsing arrays distributed across multiple machines should be straightforward based on using Dask distributed. However, one would want to be careful about creating arrays by distributing the data from a single Python process as that would involve copying between machines."
  },
  {
    "objectID": "units/unit7-bigData.html#overview-1",
    "href": "units/unit7-bigData.html#overview-1",
    "title": "Big data and databases",
    "section": "Overview",
    "text": "Overview\nBasically, standard SQL databases are relational databases that are a collection of rectangular format datasets (tables, also called relations), with each table similar to R or Pandas data frames, in that a table is made up of columns, which are called fields or attributes, each containing a single type (numeric, character, date, currency, enumerated (i.e., categorical), …) and rows or records containing the observations for one entity. Some of the tables in a given database will generally have fields in common so it makes sense to merge (i.e., join) information from multiple tables. E.g., you might have a database with a table of student information, a table of teacher information and a table of school information, and you might join student information with information about the teacher(s) who taught the students. Databases are set up to allow for fast querying and merging (called joins in database terminology).\n\nMemory and disk use\nFormally, databases are stored on disk, while Python and R store datasets in memory. This would suggest that databases will be slow to access their data but will be able to store more data than can be loaded into an Python or R session. However, databases can be quite fast due in part to disk caching by the operating system as well as careful implementation of good algorithms for database operations."
  },
  {
    "objectID": "units/unit7-bigData.html#interacting-with-a-database",
    "href": "units/unit7-bigData.html#interacting-with-a-database",
    "title": "Big data and databases",
    "section": "Interacting with a database",
    "text": "Interacting with a database\nYou can interact with databases in a variety of database systems (DBMS=database management system). Some popular systems are SQLite, DuckDB, MySQL, PostgreSQL, Oracle and Microsoft Access. We’ll concentrate on accessing data in a database rather than management of databases. SQL is the Structured Query Language and is a special-purpose high-level language for managing databases and making queries. Variations on SQL are used in many different DBMS.\nQueries are the way that the user gets information (often simply subsets of tables or information merged across tables). The result of an SQL query is in general another table, though in some cases it might have only one row and/or one column.\nMany DBMS have a client-server model. Clients connect to the server, with some authentication, and make requests (i.e., queries).\nThere are often multiple ways to interact with a DBMS, including directly using command line tools provided by the DBMS or via Python or R, among others.\nWe’ll concentrate on SQLite (because it is simple to use on a single machine). SQLite is quite nice in terms of being self-contained - there is no server-client model, just a single file on your hard drive that stores the database and to which you can connect to using the SQLite shell, R, Python, etc. However, it does not have some useful functionality that other DBMS have. For example, you can’t use ALTER TABLE to modify column types or drop columns.\nA good alternative to SQLite that I encourage you to consider is DuckDB. DuckDB stores data column-wise, which can lead to big speedups when doing queries operating on large portions of tables (so-called “online analytical processing” (OLAP)). Another nice feature of DuckDB is that it can interact with data on disk without always having to read all the data into memory. In fact, ideally we’d use it for this class, but I haven’t had time to create a DuckDB version of the StackOverflow database."
  },
  {
    "objectID": "units/unit7-bigData.html#database-schema-and-normalization",
    "href": "units/unit7-bigData.html#database-schema-and-normalization",
    "title": "Big data and databases",
    "section": "Database schema and normalization",
    "text": "Database schema and normalization\nTo truly leverage the conceptual and computational power of a database you’ll want to have your data in a normalized form, which means spreading your data across multiple tables in such a way that you don’t repeat information unnecessarily.\nThe schema is the metadata about the tables in the database and the fields (and their types) in those tables.\nLet’s consider this using an educational example. Suppose we have a school with multiple teachers teaching multiple classes and multiple students taking multiple classes. If we put this all in one table organized per student, the data might have the following fields:\n\nstudent ID\nstudent grade level\nstudent name\nclass 1\nclass 2\n…\nclass n\ngrade in class 1\ngrade in class 2\n…\ngrade in class n\nteacher ID 1\nteacher ID 2\n…\nteacher ID n\nteacher name 1\nteacher name 2\n…\nteacher name n\nteacher department 1\nteacher department 2\n…\nteacher department n\nteacher age 1\nteacher age 2\n…\nteacher age n\n\nThere are a lot of problems with this:\n\nA lot of information is repeated across rows (e.g., teacher age for students who have the same teacher) - this is a waste of space - it is hard/error-prone to update values in the database (e.g., after a teacher’s birthday), because a given value needs to be updated in multiple places\nThere are potentially a lot of empty cells (e.g., for a student who takes fewer than ‘n’ classes). This will generally result in a waste of space.\nIt’s hard to see the information that is not organized uniquely by row – i.e., it’s much easier to understand the information at the student level than the teacher level\nWe have to know in advance how big ‘n’ is. Then if a single student takes more than ‘n’ classes, the whole database needs to be restructured.\n\nIt would get even worse if there was a field related to teachers for which a given teacher could have multiple values (e.g., teachers could be in multiple departments). This would lead to even more redundancy - each student-class-teacher combination would be crossed with all of the departments for the teacher (so-called multivalued dependency in database theory).\nAn alternative organization of the data would be to have each row represent the enrollment of a student in a class.\n\nstudent ID\nstudent name\nclass\ngrade in class\nstudent grade level\nteacher ID\nteacher department\nteacher age\n\nThis has some advantages relative to our original organization in terms of not having empty cells, but it doesn’t solve the other three issues above.\nInstead, a natural way to order this database is with the following four tables.\n\nStudent\n\nID\nname\ngrade_level\n\nTeacher\n\nID\nname\ndepartment\nage\n\nClass\n\nID\ntopic\nclass_size\nteacher_ID\n\nClassAssignment\n\nstudent_ID\nclass_ID\ngrade\n\n\nThe ClassAssignment table has one row per student-class pair. Having a table like this handles “ragged” data where the number of observations per unit (in this case classes per student) varies. Using such tables is a common pattern when considering how to normalize a database. It’s also a core part of the idea of “tidy data” and data in long format, seen in the tidyr package.\nThen we do queries to pull information from multiple tables. We do the joins based on keys, which are the fields in each table that allow us to match rows from different tables.\n(That said, if all anticipated uses of a database will end up recombining the same set of tables, we may want to have a denormalized schema in which those tables are actually combined in the database. It is possible to be too pure about normalization! We can also create a virtual table, called a view, as discussed later.)\n\nKeys\nA key is a field or collection of fields that give(s) a unique value for every row/observation. A table in a database should then have a primary key that is the main unique identifier used by the DBMS. Foreign keys are columns in one table that give the value of the primary key in another table. When information from multiple tables is joined together, the matching of a row from one table to a row in another table is generally done by equating the primary key in one table with a foreign key in a different table.\nIn our educational example, the primary keys would presumably be: Student.ID, Teacher.ID, Class.ID, and for ClassAssignment a primary key made of two fields: {ClassAssignment.studentID, ClassAssignment.class_ID}.\nSome examples of foreign keys would be:\n\nstudent_ID as the foreign key in ClassAssignment for joining with Student on Student.ID\nteacher_ID as the foreign key in Class for joining with Teacher based on Teacher.ID\nclass_ID as the foreign key in ClassAssignment for joining with Class based on Class.ID\n\n\n\nQueries that join data across multiple tables\nSuppose we want a result that has the grades of all students in 9th grade. For this we need information from the Student table (to determine grade level) and information from the ClassAssignment table (to determine the class grade). More specifically we need a query that:\n\njoins Student with ClassAssignment based on matching rows in Student with rows in ClassAssignment where Student.ID is the same as ClassAssignment.student_ID and\nfilters the rows based on Student.grade_level:\n\n\nSELECT Student.ID, grade FROM Student, ClassAssignment WHERE \n  Student.ID = ClassAssignment.student_ID and Student.grade_level = 9;\n\nNote that the query is a join (specifically an inner join), which is like merge() (or dplyr::join) in R. We don’t specifically use the JOIN keyword, but one could do these queries explicitly using JOIN, as we’ll see later."
  },
  {
    "objectID": "units/unit7-bigData.html#stack-overflow-metadata-example",
    "href": "units/unit7-bigData.html#stack-overflow-metadata-example",
    "title": "Big data and databases",
    "section": "Stack Overflow metadata example",
    "text": "Stack Overflow metadata example\nI’ve obtained data from Stack Overflow, the popular website for asking coding questions, and placed it into a normalized database. The SQLite version has metadata (i.e., it lacks the actual text of the questions and answers) on all of the questions and answers posted in 2021.\nWe’ll explore SQL functionality using this example database.\nNow let’s consider the Stack Overflow data. Each question may have multiple answers and each question may have multiple (topic) tags.\nIf we tried to put this into a single table, the fields could look like this if we have one row per question:\n\nquestion ID\nID of user submitting question\nquestion title\ntag 1\ntag 2\n…\ntag n\nanswer 1 ID\nID of user submitting answer 1\nage of user submitting answer 1\nname of user submitting answer 1\nanswer 2 ID\nID of user submitting answer 2\nage of user submitting answer 2\nname of user submitting answer 2\n…\n\nor like this if we have one row per question-answer pair:\n\nquestion ID\nID of user submitting question\nquestion title\ntag 1\ntag 2\n…\ntag n\nanswer ID\nID of user submitting answer\nage of user submitting answer\nname of user submitting answer\n\nAs we’ve discussed neither of those schema is particularly desirable.\nChallenge: How would you devise a schema to normalize the data. I.e., what set of tables do you think we should create?\nYou can view one reasonable schema. The lines between tables indicate the relationship of foreign keys in one table to primary keys in another table. The schema in the actual database of Stack Overflow data we’ll use in the examples here is similar to but not identical to that.\nYou can download a copy of the SQLite version of the Stack Overflow 2021 database."
  },
  {
    "objectID": "units/unit7-bigData.html#accessing-databases-in-python",
    "href": "units/unit7-bigData.html#accessing-databases-in-python",
    "title": "Big data and databases",
    "section": "Accessing databases in Python",
    "text": "Accessing databases in Python\nPython provides a variety of front-end packages for manipulating databases from a variety of DBMS (SQLite, DuckDB, MySQL, PostgreSQL, among others). Basically, you start with a bit of code that links to the actual database, and then you can easily query the database using SQL syntax regardless of the back-end. The Python function calls that wrap around the SQL syntax will also look the same regardless of the back-end (basically execute(\"SOME SQL STATEMENT\")).\nWith SQLite, Python processes make calls against the stand-alone SQLite database (.db) file, so there are no SQLite-specific processes. With a client-server DBMS like PostgreSQL, Python processes call out to separate Postgres processes; these are started from the overall Postgres background process\nYou can access and navigate an SQLite database from Python as follows.\n\nimport sqlite3 as sq\ndir_path = '../data'  # Replace with the actual path\ndb_filename = 'stackoverflow-2021.db'\n## download from http://www.stat.berkeley.edu/share/paciorek/stackoverflow-2021.db\n\ncon = sq.connect(os.path.join(dir_path, db_filename))\ndb = con.cursor()\ndb.execute(\"select * from questions limit 5\")  # simple query \n\n&lt;sqlite3.Cursor object at 0x7ff8707acd40&gt;\n\ndb.fetchall() # retrieve results\n\n[(65534165.0, '2021-01-01 22:15:54', 0.0, 112.0, 2.0, 0.0, None, \"Can't update a value in sqlite3\", 13189393.0), (65535296.0, '2021-01-02 01:33:13', 2.0, 1109.0, 0.0, 0.0, None, 'Install and run ROS on Google Colab', 14924336.0), (65535910.0, '2021-01-02 04:01:34', -1.0, 110.0, 1.0, 8.0, 0.0, 'Operators on date/time fields', 651174.0), (65535916.0, '2021-01-02 04:03:20', 1.0, 35.0, 1.0, 0.0, None, 'Plotting values normalised', 14695007.0), (65536749.0, '2021-01-02 07:03:04', 0.0, 108.0, 1.0, 5.0, None, 'Export C# to word with template', 14899717.0)]\n\n\nAlternatively, we could use DuckDB. However, I don’t have a DuckDB version of the StackOverflow database, so one can’t actually run this code.\n\nimport duckdb as dd\ndir_path = '../data'  # Replace with the actual path\ndb_filename = 'stackoverflow-2021.duckdb'  # This doesn't exist.\n\ncon = dd.connect(os.path.join(dir_path, db_filename))\ndb = con.cursor()\ndb.execute(\"select * from questions limit 5\")  # simple query \ndb.fetchall() # retrieve results\n\nWe can (fairly) easily see the tables (this is easier from R):\n\ndef db_list_tables(db):\n    db.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = db.fetchall()\n    return [table[0] for table in tables]\n\ndb_list_tables(db)\n\n['questions', 'answers', 'questions_tags', 'users']\n\n\nTo see the fields in the table, if you’ve just queried the table, you can look at description:\n\n[item[0] for item in db.description]\n\n['name']\n\ndef get_fields():\n    return [item[0] for item in db.description]\n\nHere’s how to make a basic SQL query. One can either make the query and get the results in one go or make the query and separately fetch the results. Here we’ve selected the first five rows (and all columns, based on the * wildcard) and brought them into Python as list of tuples.\n\nresults = db.execute(\"select * from questions limit 5\").fetchall()  # simple query \ntype(results)\n\n&lt;class 'list'&gt;\n\ntype(results[0])\n\n&lt;class 'tuple'&gt;\n\nquery = db.execute(\"select * from questions\")  # simple query \nresults2 = query.fetchmany(5)\nresults == results2\n\nTrue\n\n\nTo disconnect from the database:\n\ndb.close()\n\nIt’s convenient to get a Pandas dataframe back as the result. To that we can execute queries like this:\n\nimport pandas as pd\nresults = pd.read_sql(\"select * from questions limit 5\", con)"
  },
  {
    "objectID": "units/unit7-bigData.html#basic-sql-for-choosing-rows-and-columns-from-a-table",
    "href": "units/unit7-bigData.html#basic-sql-for-choosing-rows-and-columns-from-a-table",
    "title": "Big data and databases",
    "section": "Basic SQL for choosing rows and columns from a table",
    "text": "Basic SQL for choosing rows and columns from a table\nSQL is a declarative language that tells the database system what results you want. The system then parses the SQL syntax and determines how to implement the query.\n\nNote: An imperative language is one where you provide the sequence of commands you want to be run, in order. A declarative language is one where you declare what result you want and rely on the system that interprets the commands to determine how to actually do it. Most of the languages we’re generally familiar with are imperative. (That said, even in languages like Python, function calls in many ways simply say what we want rather than exactly how the computer should carry out the granular operations.)\n\nHere are some examples using the Stack Overflow database of getting questions that have been viewed a lot (the viewcount field is large).\n\n## Get the questions (* indicates all fields) for which the viewcount field is large.\ndb.execute('select * from questions where viewcount &gt; 100000').fetchall()\n\n## Find the 10 largest viewcounts (and associated titles) in the questions table,\n## by sorting in descending order based on viewcount and returning the first 10.\n\n[(65547199.0, '2021-01-03 06:22:52', 124.0, 110832.0, 7.0, 2.0, 0.0, 'Using Bootstrap 5 with Vue 3', 11232893.0), (65549858.0, '2021-01-03 12:30:19', 52.0, 130479.0, 11.0, 0.0, 0.0, '\"ERESOLVE unable to resolve dependency tree\" when installing npm react-facebook-login', 12425004.0), (65630743.0, '2021-01-08 14:20:57', 77.0, 107140.0, 19.0, 4.0, 0.0, 'How to solve flutter web api cors error only with dart code?', 12373446.0), (65632698.0, '2021-01-08 16:22:59', 74.0, 101044.0, 9.0, 1.0, 0.0, 'How to open a link in a new Tab in NextJS?', 9578961.0), (65896334.0, '2021-01-26 05:33:33', 111.0, 141899.0, 12.0, 7.0, 0.0, 'Python Pip broken with sys.stderr.write(f\"ERROR: {exc}\")', 202576.0), (65908987.0, '2021-01-26 20:42:25', 238.0, 215399.0, 9.0, 1.0, 0.0, \"How can I open Visual Studio Code's 'settings.json' file?\", 793320.0), (65980952.0, '2021-01-31 15:36:21', 22.0, 141857.0, 10.0, 1.0, 0.0, 'Python: Could not install packages due to an OSError: [Errno 2] No such file or directory', 14489450.0), (66020820.0, '2021-02-03 03:27:19', 161.0, 174829.0, 3.0, 0.0, 0.0, 'npm: When to use `--force` and `--legacy-peer-deps`', 7824245.0), (66029781.0, '2021-02-03 14:41:21', 20.0, 107446.0, 6.0, 1.0, 0.0, 'Appcenter iOS install error \"this app cannot be installed because its integrity could not be verified\"', 462440.0), (66060487.0, '2021-02-05 09:11:48', 188.0, 241216.0, 19.0, 0.0, 0.0, 'ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject', 15150567.0), (66082397.0, '2021-02-06 21:58:06', 333.0, 315861.0, 17.0, 1.0, 0.0, 'TypeError: this.getOptions is not a function', 14337399.0), (66146088.0, '2021-02-10 22:32:45', 152.0, 175983.0, 17.0, 2.0, 0.0, 'Docker - failed to compute cache key: not found - runs fine in Visual Studio', 7419676.0), (66231282.0, '2021-02-16 19:54:53', 70.0, 116757.0, 10.0, 0.0, 0.0, 'How to add a GitHub personal access token to Visual Studio Code', 1186050.0), (66239691.0, '2021-02-17 10:03:55', 323.0, 261165.0, 6.0, 5.0, 0.0, \"What does npm install --legacy-peer-deps do exactly? When is it recommended / What's a potential use case?\", 15093141.0), (66252333.0, '2021-02-18 01:32:39', 21.0, 127736.0, 7.0, 0.0, 0.0, 'ERROR NullInjectorError: R3InjectorError(AppModule)', 14629851.0), (66366582.0, '2021-02-25 10:15:20', 90.0, 123630.0, 18.0, 5.0, 0.0, 'Github - unexpected disconnect while reading sideband packet', 11839478.0), (66597544.0, '2021-03-12 09:44:52', 29.0, 100293.0, 25.0, 4.0, 0.0, \"ENOENT: no such file or directory, lstat '/Users/Desktop/node_modules'\", 15124187.0), (66629862.0, '2021-03-14 21:43:50', 285.0, 117306.0, 9.0, 9.0, 0.0, \"Cannot determine the organization name for this 'dev.azure.com' remote url\", 11124332.0), (66662820.0, '2021-03-16 20:19:44', 104.0, 129848.0, 11.0, 0.0, 0.0, \"M1 docker preview and keycloak 'image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8)' Issue\", 4100000.0), (66666134.0, '2021-03-17 02:24:45', 159.0, 273201.0, 11.0, 2.0, 0.0, 'How to install homebrew on M1 mac', 15411878.0), (66801256.0, '2021-03-25 14:10:21', 143.0, 210144.0, 3.0, 8.0, 0.0, 'java.lang.IllegalAccessError: class lombok.javac.apt.LombokProcessor cannot access class com.sun.tools.javac.processing.JavacProcessingEnvironment', 12586904.0), (66835173.0, '2021-03-27 19:11:42', 158.0, 286886.0, 16.0, 0.0, 0.0, 'How to change background color of Elevated Button in Flutter from function?', 13628530.0), (66894200.0, '2021-03-31 19:33:26', 161.0, 263037.0, 13.0, 4.0, 0.0, 'Error message \"go: go.mod file not found in current directory or any parent directory; see \\'go help modules\\'\"', 4159198.0), (66964492.0, '2021-04-06 07:34:31', 40.0, 146006.0, 15.0, 2.0, 0.0, \"ImportError: cannot import name 'get_config' from 'tensorflow.python.eager.context'\", 5111234.0), (66980512.0, '2021-04-07 06:17:01', 739.0, 445775.0, 41.0, 0.0, 0.0, 'Android Studio Error \"Android Gradle plugin requires Java 11 to run. You are currently using Java 1.8\"', 11899911.0), (66989383.0, '2021-04-07 15:35:14', 54.0, 104476.0, 5.0, 0.0, 0.0, 'Could not resolve dependency: npm ERR! peer @angular/compiler@\"11.2.8\"', 12380096.0), (66992420.0, '2021-04-07 18:51:41', 86.0, 103193.0, 12.0, 1.0, 0.0, 'when I try to \"sync project with gradle files\" a warning pops up', 15576934.0), (67001968.0, '2021-04-08 10:19:41', 153.0, 141177.0, 17.0, 8.0, 0.0, 'How to disable maven blocking external HTTP repositories?', 5428154.0), (67045607.0, '2021-04-11 13:34:53', 204.0, 125367.0, 18.0, 1.0, 0.0, 'How to resolve \"Missing PendingIntent mutability flag\" lint warning in android api 30+?', 2652368.0), (67079327.0, '2021-04-13 17:02:54', 151.0, 278432.0, 13.0, 7.0, 0.0, 'How can I fix \"unsupported class file major version 60\" in IntelliJ IDEA?', 32914.0), (67191286.0, '2021-04-21 07:37:43', 142.0, 610026.0, 44.0, 8.0, 0.0, 'Crbug/1173575, non-JS module files deprecated. chromewebdata/(index)꞉5305:9:5551', 8732988.0), (67201708.0, '2021-04-21 18:39:28', 86.0, 105762.0, 2.0, 0.0, 0.0, 'Go update all modules', 1002260.0), (67246010.0, '2021-04-24 18:12:35', 41.0, 119121.0, 8.0, 0.0, 0.0, 'Error message \"The server selected protocol version TLS10 is not accepted by client preferences\"', 2153306.0), (67346232.0, '2021-05-01 12:23:44', 207.0, 276806.0, 60.0, 3.0, 0.0, 'Android Emulator issues in new versions - The emulator process has terminated', 13546747.0), (67352418.0, '2021-05-02 02:11:58', 74.0, 144925.0, 8.0, 1.0, 0.0, 'How to add SCSS styles to a React project?', 10836598.0), (67399785.0, '2021-05-05 10:45:12', 47.0, 151502.0, 12.0, 2.0, 0.0, 'How to solve npm install error “npm ERR! code 1”', 15841778.0), (67412084.0, '2021-05-06 05:00:27', 251.0, 286810.0, 14.0, 11.0, 0.0, 'Android Studio error: \"Manifest merger failed: Apps targeting Android 12\"', 15150212.0), (67440510.0, '2021-05-07 19:12:35', 10.0, 165243.0, 10.0, 2.0, 0.0, \"cv2.error: OpenCV(4.5.2) .error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\", 15866017.0), (67448034.0, '2021-05-08 13:21:36', 180.0, 214970.0, 34.0, 4.0, 0.0, '\"Module was compiled with an incompatible version of Kotlin. The binary version of its metadata is 1.5.1, expected version is 1.1.16\"', 14099703.0), (67501093.0, '2021-05-12 09:42:06', 47.0, 149690.0, 5.0, 4.0, 0.0, 'Passthrough is not supported, GL is disabled', 15519827.0), (67505347.0, '2021-05-12 14:11:11', 165.0, 105852.0, 12.0, 5.0, 0.0, 'Non-nullable property must contain a non-null value when exiting constructor. Consider declaring the property as nullable', 1977871.0), (67507452.0, '2021-05-12 16:23:28', 89.0, 135460.0, 16.0, 0.0, 0.0, 'No spring.config.import property has been defined', 15816596.0), (67698176.0, '2021-05-26 03:33:11', 134.0, 119615.0, 14.0, 0.0, 0.0, 'Error loading webview: Error: Could not register service workers: TypeError: Failed to register a ServiceWorker for scope', 7148467.0), (67699823.0, '2021-05-26 06:45:07', 204.0, 178718.0, 21.0, 3.0, 0.0, 'Module was compiled with an incompatible version of Kotlin. The binary version of its metadata is 1.5.1, expected version is 1.1.15', 11576007.0), (67782975.0, '2021-06-01 05:03:24', 69.0, 114886.0, 15.0, 0.0, 0.0, 'How to fix the \\'\\'module java.base does not \"opens java.io\" to unnamed module \\'\\' error in Android Studio?', 14620854.0), (67899129.0, '2021-06-09 07:03:22', 35.0, 111003.0, 6.0, 0.0, 0.0, 'Postfix and OpenJDK 11: \"No appropriate protocol (protocol is disabled or cipher suites are inappropriate)\"', 1465758.0), (67900692.0, '2021-06-09 08:49:35', 259.0, 121306.0, 25.0, 3.0, 0.0, 'Latest version of Xcode stuck on installation (12.5)', 8612435.0), (68166721.0, '2021-06-28 16:12:46', 34.0, 116051.0, 8.0, 1.0, 0.0, 'CUDA error: device-side assert triggered on Colab', 5080195.0), (68191392.0, '2021-06-30 08:45:37', 242.0, 121764.0, 25.0, 22.0, 0.0, 'Password authentication is temporarily disabled as part of a brownout. Please use a personal access token instead', 15507251.0), (68236007.0, '2021-07-03 11:50:25', 318.0, 303399.0, 20.0, 1.0, 0.0, 'I am getting error \"cmdline-tools component is missing\" after installing Flutter and Android Studio... I added the Android SDK. How can I solve them?', 11993020.0), (68260784.0, '2021-07-05 18:40:58', 117.0, 219591.0, 8.0, 0.0, 0.0, 'npm WARN old lockfile The package-lock.json file was created with an old version of npm', 12530530.0), (68387270.0, '2021-07-15 02:45:41', 464.0, 426798.0, 34.0, 8.0, 0.0, 'Android Studio error \"Installed Build Tools revision 31.0.0 is corrupted\"', 11957368.0), (68397062.0, '2021-07-15 15:55:59', 50.0, 108364.0, 8.0, 1.0, 0.0, 'Could not initialize class org.apache.maven.plugin.war.util.WebappStructureSerializer\\t-Maven Configuration Problem Any solution?', 16456713.0), (68486207.0, '2021-07-22 14:02:27', 54.0, 130114.0, 8.0, 0.0, 0.0, 'Import could not be resolved/could not be resolved from source Pylance in VS Code using Python 3.9.2 on Windows 10', 14132348.0), (68554294.0, '2021-07-28 04:09:31', 188.0, 154478.0, 32.0, 24.0, 0.0, 'android:exported needs to be explicitly specified for &lt;activity&gt;. Apps targeting Android 12 and higher are required to specify', 14280831.0), (68673221.0, '2021-08-05 20:37:54', 62.0, 103200.0, 4.0, 4.0, 0.0, \"WARNING: Running pip as the 'root' user\", 15037284.0), (68775869.0, '2021-08-13 16:49:34', 1286.0, 1236876.0, 47.0, 18.0, 0.0, 'Message \"Support for password authentication was removed. Please use a personal access token instead.\"', 15573670.0), (68836551.0, '2021-08-18 17:01:31', 53.0, 117984.0, 9.0, 1.0, 0.0, \"Keras AttributeError: 'Sequential' object has no attribute 'predict_classes'\", 10377186.0), (68857411.0, '2021-08-20 05:33:34', 41.0, 122460.0, 4.0, 3.0, 0.0, 'npm WARN deprecated tar@2.2.2: This version of tar is no longer supported, and will not receive security updates. Please upgrade asap', 14930713.0), (68958221.0, '2021-08-27 19:00:39', 80.0, 111679.0, 22.0, 3.0, 0.0, 'MongoParseError: options useCreateIndex, useFindAndModify are not supported', 12459536.0), (68959632.0, '2021-08-27 21:43:39', 35.0, 505992.0, 4.0, 2.0, 0.0, \"TypeError: Cannot read properties of undefined (reading 'id')\", 16261380.0), (69033022.0, '2021-09-02 15:18:46', 96.0, 114598.0, 30.0, 10.0, 0.0, 'Message \"error: resource android:attr/lStar not found\"', 16813382.0), (69034879.0, '2021-09-02 17:36:44', 218.0, 307961.0, 8.0, 1.0, 0.0, 'How can I resolve the error \"The minCompileSdk (31) specified in a dependency\\'s AAR metadata\" in native Java or Kotlin?', 8359705.0), (69041454.0, '2021-09-03 08:01:10', 91.0, 102261.0, 9.0, 11.0, 0.0, 'Error: require() of ES modules is not supported when importing node-fetch', 16821219.0), (69080597.0, '2021-09-06 21:56:50', 58.0, 458856.0, 22.0, 4.0, 0.0, \"× TypeError: Cannot read properties of undefined (reading 'map')\", 16846583.0), (69081410.0, '2021-09-07 00:51:50', 134.0, 296364.0, 3.0, 2.0, 0.0, 'Error [ERR_REQUIRE_ESM]: require() of ES Module not supported', 16847125.0), (69139074.0, '2021-09-11 00:12:49', 8.0, 109316.0, 3.0, 0.0, 0.0, \"ERROR TypeError: Cannot read properties of undefined (reading 'title')\", 16723200.0), (69163511.0, '2021-09-13 13:25:07', 224.0, 136939.0, 13.0, 0.0, 0.0, \"Build was configured to prefer settings repositories over project repositories but repository 'maven' was added by build file 'build.gradle'\", 12886431.0), (69390676.0, '2021-09-30 10:34:46', 121.0, 121388.0, 14.0, 2.0, 0.0, 'How to use appsettings.json in Asp.net core 6 Program.cs file', 10336618.0), (69394001.0, '2021-09-30 14:28:21', 34.0, 117452.0, 10.0, 1.0, 0.0, 'How to fix? \"kex_exchange_identification: read: Connection reset by peer\"', 8939187.0), (69394632.0, '2021-09-30 15:07:50', 249.0, 242414.0, 12.0, 1.0, 0.0, 'Webpack build failing with ERR_OSSL_EVP_UNSUPPORTED', 17044429.0), (69564817.0, '2021-10-14 03:41:23', 48.0, 100499.0, 4.0, 0.0, 0.0, \"TypeError: load() missing 1 required positional argument: 'Loader' in Google Colab\", 17147261.0), (69567381.0, '2021-10-14 08:22:53', 102.0, 132221.0, 16.0, 1.0, 0.0, 'Getting \"Cannot read property \\'pickAlgorithm\\' of null\" error in react native', 15269749.0), (69665222.0, '2021-10-21 16:00:47', 90.0, 165387.0, 13.0, 0.0, 0.0, 'Node.js 17.0.1 Gatsby error - \"digital envelope routines::unsupported ... ERR_OSSL_EVP_UNSUPPORTED\"', 7002673.0), (69692842.0, '2021-10-23 23:39:57', 843.0, 816368.0, 43.0, 10.0, 0.0, 'Error message \"error:0308010C:digital envelope routines::unsupported\"', 14994086.0), (69722872.0, '2021-10-26 12:10:04', 184.0, 130738.0, 10.0, 1.0, 0.0, 'ASP.NET Core 6 how to access Configuration during startup', 1977871.0), (69773547.0, '2021-10-29 18:49:01', 62.0, 108232.0, 14.0, 1.0, 0.0, 'Visual Studio 2019 Not Showing .NET 6 Framework', 1407658.0), (69832748.0, '2021-11-03 23:06:48', 98.0, 140728.0, 16.0, 0.0, 0.0, 'Error \"Error: A &lt;Route&gt; is only ever to be used as the child of &lt;Routes&gt; element\"', 13149387.0), (69843615.0, '2021-11-04 17:44:31', 72.0, 102942.0, 6.0, 0.0, 0.0, \"Switch' is not exported from 'react-router-dom'\", 8467488.0), (69854011.0, '2021-11-05 13:26:59', 137.0, 107635.0, 7.0, 1.0, 0.0, 'Matched leaf route at location \"/\" does not have an element', 16102215.0), (69864165.0, '2021-11-06 12:55:13', 160.0, 149243.0, 18.0, 0.0, 0.0, 'Error: [PrivateRoute] is not a &lt;Route&gt; component. All component children of &lt;Routes&gt; must be a &lt;Route&gt; or &lt;React.Fragment&gt;', 16830299.0), (69868956.0, '2021-11-07 00:34:29', 145.0, 221716.0, 9.0, 4.0, 0.0, 'How can I redirect in React Router v6?', 2000548.0), (69875125.0, '2021-11-07 17:53:49', 78.0, 105208.0, 4.0, 1.0, 0.0, 'find_element_by_* commands are deprecated in Selenium', 17351258.0), (69875520.0, '2021-11-07 18:45:12', 150.0, 180541.0, 13.0, 1.0, 0.0, 'Unable to negotiate with 40.74.28.9 port 22: no matching host key type found. Their offer: ssh-rsa', 7122272.0), (70000324.0, '2021-11-17 07:23:39', 120.0, 112536.0, 3.0, 0.0, 0.0, 'What is \"crypt key missing\" error in Pgadmin4 and how to resolve it?', 10279487.0), (70036953.0, '2021-11-19 15:09:03', 90.0, 103677.0, 14.0, 5.0, 0.0, \"Spring Boot 2.6.0 / Spring fox 3 - Failed to start bean 'documentationPluginsBootstrapper'\", 306436.0), (70281346.0, '2021-12-08 20:29:31', 150.0, 155000.0, 12.0, 5.0, 0.0, 'Node.js Sass version 7.0.0 is incompatible with ^4.0.0 || ^5.0.0 || ^6.0.0', 13765920.0), (70319606.0, '2021-12-11 22:44:15', 115.0, 110984.0, 5.0, 0.0, 0.0, \"ImportError: cannot import name 'url' from 'django.conf.urls' after upgrading to Django 4.0\", 113962.0), (70358643.0, '2021-12-15 04:58:02', 233.0, 107966.0, 6.0, 4.0, 0.0, '\"You are running create-react-app 4.0.3 which is behind the latest release (5.0.0)\"', 14426381.0), (70368760.0, '2021-12-15 18:37:56', 163.0, 164918.0, 21.0, 2.0, 0.0, 'React Uncaught ReferenceError: process is not defined', 14880787.0), (70538793.0, '2021-12-31 03:41:36', 40.0, 103218.0, 9.0, 1.0, 0.0, 'remote: Write access to repository not granted. fatal: unable to access', 10781286.0)]\n\ndb.execute(\n'select title, viewcount from questions order by viewcount desc limit 10').fetchall()\n\n[('Message \"Support for password authentication was removed. Please use a personal access token instead.\"', 1236876.0), ('Error message \"error:0308010C:digital envelope routines::unsupported\"', 816368.0), ('Crbug/1173575, non-JS module files deprecated. chromewebdata/(index)꞉5305:9:5551', 610026.0), (\"TypeError: Cannot read properties of undefined (reading 'id')\", 505992.0), (\"× TypeError: Cannot read properties of undefined (reading 'map')\", 458856.0), ('Android Studio Error \"Android Gradle plugin requires Java 11 to run. You are currently using Java 1.8\"', 445775.0), ('Android Studio error \"Installed Build Tools revision 31.0.0 is corrupted\"', 426798.0), ('TypeError: this.getOptions is not a function', 315861.0), ('How can I resolve the error \"The minCompileSdk (31) specified in a dependency\\'s AAR metadata\" in native Java or Kotlin?', 307961.0), ('I am getting error \"cmdline-tools component is missing\" after installing Flutter and Android Studio... I added the Android SDK. How can I solve them?', 303399.0)]\n\n\nLet’s lay out the various verbs in SQL. Here’s the form of a standard query (though the ORDER BY is often omitted and sorting is computationally expensive):\nSELECT &lt;column(s)&gt; FROM &lt;table&gt; WHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nSQL keywords are often written in ALL CAPITALS, although I won’t necessarily do that in this document.\nAnd here is a table of some important keywords:\n\n\n\n\n\n\n\nKeyword\nUsage\n\n\n\n\nSELECT\nselect columns\n\n\nFROM\nwhich table to operate on\n\n\nWHERE\nfilter (choose) rows satisfying certain conditions\n\n\nLIKE, IN, &lt;, &gt;, ==, etc.\nused as part of conditions\n\n\nORDER BY\nsort based on columns\n\n\n\nFor logical comparisons in a WHERE clause, some common syntax for setting conditions includes LIKE (for patterns), =, &gt;, &lt;, &gt;=, &lt;=, !=.\nSome other keywords are: DISTINCT, ON, JOIN, GROUP BY, AS, USING, UNION, INTERSECT, SIMILAR TO.\nQuestion: how would we find the oldest users in the database?"
  },
  {
    "objectID": "units/unit7-bigData.html#grouping-stratifying",
    "href": "units/unit7-bigData.html#grouping-stratifying",
    "title": "Big data and databases",
    "section": "Grouping / stratifying",
    "text": "Grouping / stratifying\nA common pattern of operation is to stratify the dataset, i.e., collect it into mutually exclusive and exhaustive subsets. One would then generally do some (reduction) operation on each subset (e.g., counting records, calculating the mean of a column, taking the max of a column). In SQL this is done with the GROUP BY keyword.\nThe basic syntax looks like this:\nSELECT &lt;reduction_operation&gt;(&lt;column(s)&gt;) FROM &lt;table&gt; GROUP BY &lt;column(s)&gt;\nHere’s a basic example where we count the occurrences of different tags. Note that we use as to define a name for the new column that is created based on the aggregation operation (count in this case).\n\ndb.execute(\"select tag, count(*) as n from questions_tags \\\n           group by tag \\\n           order by n desc limit 25\").fetchall()\n\n[('python', 255614), ('javascript', 182006), ('java', 89097), ('reactjs', 83180), ('html', 69401), ('c#', 67633), ('android', 55422), ('r', 51688), ('node.js', 50231), ('php', 48782), ('css', 48021), ('c++', 46267), ('pandas', 45862), ('sql', 43598), ('python-3.x', 42014), ('flutter', 39243), ('typescript', 33583), ('arrays', 29960), ('angular', 29783), ('django', 29228), ('mysql', 26562), ('dataframe', 25283), ('c', 24965), ('json', 24510), ('swift', 23008)]\n\n\nIn general GROUP BY statements will involve some aggregation operation on the subsets. Options include: COUNT, MIN, MAX, AVG, SUM. The number of results will be the same as the number of groups; in the example above there should be one result per tag.\nIf you filter after using GROUP BY, you need to use having instead of where.\nChallenge: Write a query that will count the number of answers for each question, returning the most answered questions."
  },
  {
    "objectID": "units/unit7-bigData.html#getting-unique-results-distinct",
    "href": "units/unit7-bigData.html#getting-unique-results-distinct",
    "title": "Big data and databases",
    "section": "Getting unique results (DISTINCT)",
    "text": "Getting unique results (DISTINCT)\nA useful SQL keyword is DISTINCT, which allows you to eliminate duplicate rows from any table (or remove duplicate values when one only has a single column or set of values).\n\n## Get the unique tags from the questions_tags table.\ntag_names = db.execute(\"select distinct tag from questions_tags\").fetchall()\ntag_names[0:5]\n## Count the number of unique tags.\n\n[('sorting',), ('visual-c++',), ('mfc',), ('cgridctrl',), ('css',)]\n\ndb.execute(\"select count(distinct tag) from questions_tags\").fetchall()\n\n[(42137,)]"
  },
  {
    "objectID": "units/unit7-bigData.html#simple-sql-joins",
    "href": "units/unit7-bigData.html#simple-sql-joins",
    "title": "Big data and databases",
    "section": "Simple SQL joins",
    "text": "Simple SQL joins\nOften to get the information we need, we’ll need data from multiple tables. To do this we’ll need to do a database join, telling the database what columns should be used to match the rows in the different tables.\nThe syntax generally looks like this (again the WHERE and ORDER BY are optional):\nSELECT &lt;column(s)&gt; FROM &lt;table1&gt; JOIN &lt;table2&gt; ON &lt;columns to match on&gt;\nWHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nLet’s see some joins using the different syntax on the Stack Overflow database. In particular let’s select only the questions with the tag ‘python’. By selecting * we are selecting all columns from both the questions and questions_tags tables.\n\nresult1 = db.execute(\"select * from questions join questions_tags \\\n        on questions.questionid = questions_tags.questionid \\\n        where tag = 'python'\").fetchall()\nget_fields()\n\n['questionid', 'creationdate', 'score', 'viewcount', 'answercount', 'commentcount', 'favoritecount', 'title', 'ownerid', 'questionid', 'tag']\n\n\nIt turns out you can do it without using the JOIN keyword.\n\nresult2 = db.execute(\"select * from questions, questions_tags \\\n        where questions.questionid = questions_tags.questionid and \\\n        tag = 'python'\").fetchall()\n\nresult1[0:5]\n\n[(65526804.0, '2021-01-01 01:54:10', 0.0, 2087.0, 3.0, 3.0, None, 'How to play an audio file starting at a specific time', 14718094.0, 65526804.0, 'python'), (65527402.0, '2021-01-01 05:14:22', 1.0, 56.0, 1.0, 0.0, None, 'Join dataframe columns in python', 1492229.0, 65527402.0, 'python'), (65529525.0, '2021-01-01 12:06:43', 1.0, 175.0, 1.0, 0.0, None, 'Issues with pygame.time.get_ticks()', 13720770.0, 65529525.0, 'python'), (65529971.0, '2021-01-01 13:14:40', 1.0, 39.0, 0.0, 1.0, None, 'How to check if Windows prompts a notification box using python?', 13845215.0, 65529971.0, 'python'), (65532644.0, '2021-01-01 18:46:52', -2.0, 49.0, 1.0, 1.0, None, 'How I divide this text file in a Dataframe?', 14122166.0, 65532644.0, 'python')]\n\nresult1 == result2\n\nTrue\n\n\nHere’s a three-way join (using both types of syntax) with some additional use of aliases to abbreviate table names. What does this query ask for?\n\nresult1 = db.execute(\"select * from \\\n        questions Q \\\n        join questions_tags T on Q.questionid = T.questionid \\\n        join users U on Q.ownerid = U.userid \\\n        where tag = 'python' and \\\n        viewcount &gt; 1000\").fetchall()\n\nresult2 = db.execute(\"select * from \\\n        questions Q, questions_tags T, users U where \\\n        Q.questionid = T.questionid and \\\n        Q.ownerid = U.userid and \\\n        tag = 'python' and \\\n        viewcount &gt; 1000\").fetchall()\n\nresult1 == result2\n\nTrue\n\n\nChallenge: Write a query that would return all the answers to questions with the Python tag.\nChallenge: Write a query that would return the users who have answered a question with the Python tag."
  },
  {
    "objectID": "units/unit7-bigData.html#temporary-tables-and-views",
    "href": "units/unit7-bigData.html#temporary-tables-and-views",
    "title": "Big data and databases",
    "section": "Temporary tables and views",
    "text": "Temporary tables and views\nYou can think of a view as a temporary table that is the result of a query and can be used in subsequent queries. In any given query you can use both views and tables. The advantage is that they provide modularity in our querying. For example, if a given operation (portion of a query) is needed repeatedly, one could abstract that as a view and then make use of that view.\nSuppose we always want the age and displayname of owners of questions to be readily available. Once we have the view we can query it like a regular table.\n\ndb.execute(\"create view questionsAugment as select \\\n                questionid, questions.creationdate, score, viewcount, \\\n                title, ownerid, age, displayname \\\n                from questions join users \\\n                on questions.ownerid = users.userid\")\n## you'll see the return value is '0'\n               \n\n&lt;sqlite3.Cursor object at 0x7f8ac84b8d40&gt;\n\ndb.execute(\"select * from questionsAugment where viewcount &gt; 1000 limit 5\").fetchall()\n\n[(65535296.0, '2021-01-02 01:33:13', 2.0, 1109.0, 'Install and run ROS on Google Colab', 14924336.0, None, 'Gustavo Lima'), (65526407.0, '2021-01-01 00:03:01', 1.0, 2646.0, 'How to remove Branding WHMCS Ver 8.1 \"Powered by WHMcomplete solutions\"', 14920717.0, None, 'Blunch Restaurant'), (65526447.0, '2021-01-01 00:10:40', 7.0, 25536.0, 'React Router v5.2 - Blocking route change with createBrowserHistory and history.block', 10841085.0, None, 'user51462'), (65526500.0, '2021-01-01 00:22:41', 3.0, 2870.0, 'intellisense vscode not showing parameters nor documentation when hovering above with mouse', 13660865.0, None, 'albert chen'), (65526515.0, '2021-01-01 00:27:26', 2.0, 1568.0, 'How to identify time and space complexity of recursive backtracking algorithms with step-by-step analysis', 6801755.0, None, 'BlueTriangles')]\n\n\nOne use of a view would be to create a mega table that stores all the information from multiple tables in the (unnormalized) form you might have if you simply had one data frame in Python or R."
  },
  {
    "objectID": "units/unit7-bigData.html#more-on-joins",
    "href": "units/unit7-bigData.html#more-on-joins",
    "title": "Big data and databases",
    "section": "More on joins",
    "text": "More on joins\nWe’ve seen a bunch of joins but haven’t discussed the full taxonomy of types of joins. There are various possibilities for how to do a join depending on whether there are rows in one table that do not match any rows in the other table.\nInner joins: In database terminology an inner join is when the result has a row for each match of a row in one table with the rows in the second table, where the matching is done on the columns you indicate. If a row in one table corresponds to more than one row in another table, you get all of the matching rows in the second table, with the information from the first table duplicated for each of the resulting rows. For example in the Stack Overflow data, an inner join of questions and answers would pair each question with each of the answers to that question. However, questions without any answers or (if this were possible) answers without a corresponding question would not be part of the result.\nOuter joins: Outer joins add additional rows from one table that do not match any rows from the other table as follows. A left outer join gives all the rows from the first table but only those from the second table that match a row in the first table. A right outer join is the converse, while a full outer join includes at least one copy of all rows from both tables. So a left outer join of the Stack Overflow questions and answers tables would, in addition to the matched questions and their answers, include a row for each question without any answers, as would a full outer join. In this case there should be no answers that do not correspond to question, so a right outer join should be the same as an inner join.\nCross joins: A cross join gives the Cartesian product of the two tables, namely the pairwise combination of every row from each table. I.e., take a row from the first table and pair it with each row from the second table, then repeat that for all rows from the first table. Since cross joins pair each row in one table with all the rows in another table, the resulting table can be quite large (the product of the number of rows in the two tables). In the Stack Overflow database, a cross join would pair each question with every answer in the database, regardless of whether the answer is an answer to that question.\nSimply listing two or more tables separated by commas as we saw earlier is the same as a cross join. Alternatively, listing two or more tables separated by commas, followed by conditions that equate rows in one table to rows in another is equivalent to an inner join.\nIn general, inner joins can be seen as a form of cross join followed by a condition that enforces matching between the rows of the table. More broadly, here are four equivalent joins that all perform the equivalent of an inner join:\n\n## explicit inner join:\nselect * from table1 join table2 on table1.id = table2.id \n## non-explicit join without JOIN\nselect * from table1, table2 where table1.id = table2.id \n## cross-join followed by matching\nselect * from table1 cross join table2 where table1.id = table2.id \n## explicit inner join with 'using'\nselect * from table1 join table2 using(id)\n\nChallenge: Create a view with one row for every question-tag pair, including questions without any tags.\nChallenge: Write a query that would return the displaynames of all of the users who have never posted a question. The NULL keyword will come in handy it’s like ‘NA’ in R. Hint: NULLs should be produced if you do an outer join."
  },
  {
    "objectID": "units/unit7-bigData.html#indexes",
    "href": "units/unit7-bigData.html#indexes",
    "title": "Big data and databases",
    "section": "Indexes",
    "text": "Indexes\nAn index is an ordering of rows based on one or more fields. DBMS use indexes to look up values quickly, either when filtering (if the index is involved in the WHERE condition) or when doing joins (if the index is involved in the JOIN condition). So in general you want your tables to have indexes.\nDBMS use indexing to provide sub-linear time lookup. Without indexes, a database needs to scan through every row sequentially, which is called linear time lookup if there are n rows, the lookup is O(n) in computational cost. With indexes, lookup may be logarithmic O(log(n)) (if using tree-based indexes) or constant time O(1) (if using hash-based indexes). A binary tree-based search is logarithmic; at each step through the tree you can eliminate half of the possibilities.\nHere’s how we create an index, with some time comparison for a simple query.\n\nt0 = time.time()\nresults = db.execute(\n  \"select * from questions where viewcount &gt; 10000\").fetchall()\nprint(time.time() - t0)  # 10 seconds\nt0 = time.time()\ndb.execute(\n  \"create index count_index on questions (viewcount)\")\nprint(time.time() - t0)  # 19 seconds\nt0 = time.time()\ndb.execute(\n  \"select * from questions where viewcount &gt; 10000\").fetchall()  \nprint(time.time() - t0)  # 3 seconds\n\nIn other contexts, an index can save huge amounts of time. So if you’re working with a database and speed is important, check to see if there are indexes. That said, as seen above it takes time to create the index, so you’d only want to create it if you were doing multiple queries that could take advantage of the index. See the databases tutorial for more discussion of how using indexes in a lookup is not always advantageous."
  },
  {
    "objectID": "units/unit7-bigData.html#set-operations-union-intersect-except",
    "href": "units/unit7-bigData.html#set-operations-union-intersect-except",
    "title": "Big data and databases",
    "section": "Set operations: union, intersect, except",
    "text": "Set operations: union, intersect, except\nYou can do set operations like union, intersection, and set difference using the UNION, INTERSECT, and EXCEPT keywords, respectively, on tables that have the same schema (same column names and types), though most often these would be used on single columns (i.e., single-column tables).\n\nNote: While one can often set up an equivalent query without using INTERSECT or UNION, set operations can be very handy. In the example below one could do it with a join, but the syntax is often more complicated.\n\nConsider the following example of using INTERSECT. What does it return?\n\nresult1 = db.execute(\"select displayname, userid from \\\n                     questions Q join users U on U.userid = Q.ownerid \\\n                     intersect \\\n                     select displayname, userid from \\\n                     answers A join users U on U.userid = A.ownerid\")\n\nChallenge: what if you wanted to find users who had neither asked nor answered a question?"
  },
  {
    "objectID": "units/unit7-bigData.html#subqueries",
    "href": "units/unit7-bigData.html#subqueries",
    "title": "Big data and databases",
    "section": "Subqueries",
    "text": "Subqueries\nA subquery is a full query that is embedded in a larger query. These can be quite handy in building up complicated queries. One could instead use temporary tables, but it often is easier to write all in one query (and that let’s the database’s query optimizer operate on the entire query).\n\nSubqueries in the FROM statement\nWe can use subqueries in the FROM statement to create a temporary table to use in a query. Here we’ll do it in the context of a join.\n\nChallenge: What does the following do?\n\n\ndb.execute(\"select * from questions join answers A \\\n           on questions.questionid = A.questionid \\\n           join \\\n           (select ownerid, count(*) as n_answered from answers \\\n           group by ownerid order by n_answered desc limit 1000) most_responsive \\\n           on A.ownerid = most_responsive.ownerid\")\n\nIt might be hard to just come up with that full query all at once. A good strategy is probably to think about creating a view that is the result of the inner query and then have the outer query use that. You can then piece together the complicated query in a modular way. For big databases, you are likely to want to submit this as a single query and not two queries so that the SQL optimizer can determine the best way to do the operations. But you want to start with code that you’re confident will give you the right answer!\nNote we could also have done that query using a subquery in the WHERE statement, as discussed in the next section.\n\n\nSubqueries in the WHERE statement\nInstead of a join, we can use subqueries as a way to combine information across tables, with the subquery involved in a WHERE statement. The subquery creates a set and we then can check for inclusion in (or exclusion from with not in) that set.\nFor example, suppose we want to know the average number of UpVotes for users who have posted a question with the tag “python”.\n\ndb.execute(\"select avg(upvotes) from users where userid in \\\n           (select distinct ownerid from \\\n           questions join questions_tags \\\n           on questions.questionid = questions_tags.questionid \\\n           where tag = 'python')\").fetchall()\n\n[(62.72529394895326,)]"
  },
  {
    "objectID": "units/unit7-bigData.html#creating-database-tables",
    "href": "units/unit7-bigData.html#creating-database-tables",
    "title": "Big data and databases",
    "section": "Creating database tables",
    "text": "Creating database tables\nOne can create tables from within the ‘sqlite’ command line interfaces (discussed in the tutorial), but often one would do this from Python or R. Here’s the syntax from Python, creating the table from a Pandas dataframe.\n\n## create data frame 'student_data' in some fashion\ncon = sq.connect(db_path)\nstudent_data.to_sql('student', con, if_exists='replace', index=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "See the links above for the key resources for the course."
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 243 Fall 2023",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible. All those enrolled or wishing to take the class should have filled it out by Friday August 25 at noon.\nUndergraduates can only take the course with my permission and once all graduate students have an opportunity to register. I expect there will be space, and will likely start admitting undergraduates early the week of August 28 or late the previous week.\nConcurrent enrollment students (e.g., exchange students from other universities/countries) can take the course with my permission and once all UC Berkeley students have had an opportunity to register. I expect there will be space, and will likely start admitting concurrent enrollment students early the week of August 28.\nPlease see the syllabus for the math and statistics background I expect, as well as the need to be familiar with Python or to get up to speed in Python during the first few weeks of the semester.\nThe first three weeks involve a lot of moving pieces, in part related to trying to get everyone up to speed with the bash shell and Python. Please use the schedule to keep on top of what you need to do."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Statistics 243 Fall 2023",
    "section": "Course content",
    "text": "Course content\n\nThis site: most non-recorded course material\n\nFor html versions of the Units, see the navigation bar above.\nFor PDF versions of the Units, clone the GitHub repository, or if you’re not yet familiar with Git, download a zip file.\n\nVarious SCF tutorials: These include the various tutorials referred to in the class materials (e.g., the UNIX and bash shell tutorials, the dynamic documents tutorial, the Git tutorial, the string processing tutorial, etc.).\nbCourses: links to class course captures and any pre-recorded material.\n\nIf you’re not yet familiar with Git, go to the upper right of this page and click on ‘Clone or download’ and then ‘Download ZIP’."
  },
  {
    "objectID": "labs/09/collab_with_git.html",
    "href": "labs/09/collab_with_git.html",
    "title": "Collaboration with Git",
    "section": "",
    "text": "Objectives & purpose\n\n\n\nToday we’re going to get some additional experience with Git and GitHub.\nKnowledge of this material is important to ensure that collaboration on your final projects goes smoothly.\nIf you weren’t able to attend in person, then you can email me so that I can connect you with another student that couldn’t make it. Then you two can set up a time to work through the lab (over Zoom is fine).\n\n\nIn our first section of the semester, we discussed how to use Git and GitHub. Now that you have a semester’s worth of experience using a remote repository, we’ll explore more advanced functionally of Git and GitHub today.\nIn particular, we’ll practice with a number features that become most useful when collaborating with others in a GitHub repository. There is additional information about using Git for collaboration and various other features in the appendix at the end for those of you that are curious.\nWe’ll work in pairs to practice collaboration with Git and how to fix issues that can arise when working in a shared Git repo.\nFirst, we’ll do some reading to get familiar with Git and GitHub’s main features that facilitate collaboration.\n\n\n\n\n\n\nExercise 0 (~10 minutes)\n\n\n\n\n\nBefore working on the rest of the pair exercises, we’ll read through the sections below. Much of the content is just examples of usage for different commands, so you can skim those examples and use them for reference when working on the exercises.\nIf you finish reading before your pair, you can take care of Exercises 1 and 2 in the Pair exercises section."
  },
  {
    "objectID": "labs/09/collab_with_git.html#manual-pages",
    "href": "labs/09/collab_with_git.html#manual-pages",
    "title": "Collaboration with Git",
    "section": "Manual pages",
    "text": "Manual pages\nGit’s built-in documentation is a good resource when in doubt about a particular command or to deepen your knowledge. There are many useful options to Git’s commands, so taking a skim through the manual pages for the commands you use most often is not a bad idea.\nTo see the docs for a command such as git log, you can do:\nman git-log\nor:\ngit help log\nAlternatively, you can access these manual pages on the web at https://git-scm.com/docs. From the git log manual, we can find information on a number of options that greatly increase the usefulness of the output. We’ll see some examples later on in the next section."
  },
  {
    "objectID": "labs/09/collab_with_git.html#git-pull",
    "href": "labs/09/collab_with_git.html#git-pull",
    "title": "Collaboration with Git",
    "section": "git pull",
    "text": "git pull\nYou already know how to use git push to update your remote repository on GitHub, but to this point in the course git pull may not have been super relevant.\nWhen it comes to collaboration, however, git pull is the very first command you should run when you sit down to work on a local copy of a shared remote repo.\nWhy? While you were doing other things, your collaborators could have worked on their local copies, made commits, and pushed to the shared remote repo on GitHub. So by using git pull, you will update your local copy with the changes that they made before you start making new changes.\nUsing git pull as a habit can help save some headaches down the line."
  },
  {
    "objectID": "labs/09/collab_with_git.html#git-log",
    "href": "labs/09/collab_with_git.html#git-log",
    "title": "Collaboration with Git",
    "section": "git log",
    "text": "git log\nAfter running git pull, if there were updates then you’ll want to get a quick sense of what changed. You already know how to view the commit history using:\ngit log\nIf you also want to see complete diffs at each step, use\ngit log -p\nHowever, the output using -p can at times be overwhelming. Often, a condensed overview of the files changed in each commit is useful to get a feel for the history:\ngit log --stat --summary\nIf you don’t care about the files but just want a compact glance at the full history, then the following command will give a pretty graph with information about other branches in the repo’s history:\ngit log --oneline --decorate --graph --all\nThere are many other ways to use git log, so take advantage of the docs if there is something in particular that you’d like to see."
  },
  {
    "objectID": "labs/09/collab_with_git.html#git-branch",
    "href": "labs/09/collab_with_git.html#git-branch",
    "title": "Collaboration with Git",
    "section": "git branch",
    "text": "git branch\nTo this point, I’ve been assuming that you and your collaborators are all working on the same branch (typically main).\nHowever, a single Git repository can maintain multiple branches of development. A workflow where each collaborator works on their own branch and then work together to merge their changes can be an effective way to collaborate and avoid too many headaches in the case of code conflicts.\nTo see what branches are available, use:\ngit branch\nYou will probably only see main at this point.\nTo create a new branch named “experimental”, use\ngit branch experimental\nIf you now run\ngit branch\nThe “experimental” branch is the one you just created, and the “main” branch is a default branch that was created for you automatically. The asterisk marks the branch you are currently on; type:\ngit checkout experimental\nto switch to the experimental branch. Now edit a file, commit the change, and switch back to the main branch:\ngit add file\ngit commit -m \"edited file\"\nAlternatively, one can “stash” the changes using git stash. This saves your changes for later, and then reverts the working tree to the last HEAD (whatever the last commit was). This allows you to keep working without the changes being applied to any files. You can apply those changes later using git stash pop, which applies the changes and removes them from your stash. Or, if you wish to apply the changes to multiple branches, you can use git stash apply, which applies the changes but leaves them in your stash.\nNow, you can switch back to the main branch.\ngit checkout main\nCheck that the change you made is no longer visible, since it was made on the experimental branch and you’re back on the main branch.\nYou can make a different change on the main branch and commit. At this point the two branches have diverged, with different changes made in each. To merge the changes made in experimental into main, run:\ngit merge experimental\nIf the changes don’t conflict, then the merge was successful and you can add, commit and push to the remote repository. If there are conflicts, markers will be left in the problematic files showing the conflict;\ngit diff\nwill show this. Once you’ve edited the files to resolve the conflicts,\ngit commit -a\nwill commit the result of the merge. Finally,\nAt this point you could delete the experimental branch with\ngit branch -d experimental\nThis command ensures that the changes in the experimental branch are already in the current branch.\nIf you want to remove a branch without pulling the changes into the main branch, the -D flag deletes it without checking any of the changes.\nThis only removes the branch from your local machine. To remove it from the remote repository, you use:\ngit push -d origin experimental"
  },
  {
    "objectID": "labs/09/collab_with_git.html#pull-requests",
    "href": "labs/09/collab_with_git.html#pull-requests",
    "title": "Collaboration with Git",
    "section": "Pull-requests",
    "text": "Pull-requests\nWhen working in a collaborative environment, instead of merging directly into the main, it is best to create a pull-request. This link has a good step-by-step explanation.\nPull requests tell repository maintainers the difference between the main repository and an individual’s branch. It will then allow maintainers to comment on the pull request and get bugs fixed before the branch is merged to the main.\nPull requests are common practice in the software development in industry."
  },
  {
    "objectID": "labs/09/collab_with_git.html#importing-a-project",
    "href": "labs/09/collab_with_git.html#importing-a-project",
    "title": "Collaboration with Git",
    "section": "Importing A Project",
    "text": "Importing A Project\nI do not recommend this process for initiating a new project. These steps are simple, until you get to creating the remote repository. Then, just like in the intro tutorial, you have you setup a new repository on Github and link it to the local one. It is easier to create the repo on Github, clone the empty repo locally, then put files in it as desired.\nAssume you have a tarball linReg.tar.gz with your initial work. You can place it under Git revision control as follows.\ntar xzf project.tar.gz\ncd project\ngit init\nGit will reply (something along the lines)\nInitialized empty Git repository in .git/\nYou’ve now initialized the working directory-you may notice a new directory created, named “.git”.\nNext, tell Git to take a snapshot of the contents of all files under the current directory (note the .), with git add:\ngit add .\nThis snapshot is now stored in a temporary staging area which Git calls the index. You can permanently store the contents of the index in the repository with git commit:\ngit commit -m \"add\"\nThis will prompt you for a commit message. You’ve now stored the first version of your project in Git."
  },
  {
    "objectID": "labs/09/collab_with_git.html#making-changes",
    "href": "labs/09/collab_with_git.html#making-changes",
    "title": "Collaboration with Git",
    "section": "Making Changes",
    "text": "Making Changes\nIf we make changes to files file1, file2 and file3 we can add them to be commited with git add as we have discussed before:\ngit add file1 file2 file3\nYou are now ready to commit. You can see what is about to be committed using git diff with the --cached option:\ngit diff --cached\n(Without –cached, git diff will show you any changes that you’ve made but not yet added to the index.)\nYou can also get a brief summary of the situation with git status:\ngit status\nAlternatively, instead of running git add before git commit, you can use:\ngit commit -a\nwhich will automatically notice any modified (but not new) files, add them to the index, and commit, all in one step."
  },
  {
    "objectID": "labs/09/collab_with_git.html#amending-file-to-commit",
    "href": "labs/09/collab_with_git.html#amending-file-to-commit",
    "title": "Collaboration with Git",
    "section": "Amending file to commit",
    "text": "Amending file to commit\nWhat if, in the files you just commited, there was a file you forgot? This situation is handled via git’s amend option in git commit. Say we have added and committed file1\ngit add file1\ngit commit -m \"adding file1\"\nBut we realize we also meant to commit file2. We can do that by ammending the original commit as follows:\ngit add file2\ngit commit --amend -m \"adding second file\"\nThis allows you to add more files to a commit and then update the message, while keeping your original message/commmited-files there."
  },
  {
    "objectID": "labs/09/collab_with_git.html#undoing-mistakes-checkout-reset-and-revert",
    "href": "labs/09/collab_with_git.html#undoing-mistakes-checkout-reset-and-revert",
    "title": "Collaboration with Git",
    "section": "Undoing mistakes: checkout, reset, and revert",
    "text": "Undoing mistakes: checkout, reset, and revert\n\n\n\n\n\n\nDanger zone\n\n\n\nSome of the commands below can get you into trouble if you aren’t 100% sure of what you’re doing. Use them with extreme caution.\n\n\n\nCheckout\ngit checkout can be used to look at a previous commit. It can also be used to move to a different branch, which we will look at in the next section. Here we can look at code from a previous commit with:\ngit checkout HEAD~1 # moves back 1 commit\ngit checkout HEAD~2 # moves back 2 commits\ngit checkout &lt;commit_hash&gt; # move back to a specific commit\nTo find commit IDs you can use git log or git reflog. You can also find commit IDs on GitHub.\nOnce you have looked at the commit you can go back to the most recent update using\ngit checkout main # or replacing main with whatever branch you are on\n\n\nRevert\ngit revert is used when you want to undo the changes made in a previous commit. It will undo a commit by creating a new commit. Consider using git revert HEAD~1, this will remove the changes that were added in the previous commit.\ngit revert HEAD~1 \ngit revert HEAD~2 \ngit revert &lt;commit_hash&gt; \n\n\nReset\nIf you added something that shouldn’t be commited or you want to reset your repo to what it looked like at a previous commit, then you need to use the git reset feature.\nman git-reset\n\n# e.g.\ngit reset --soft HEAD~1\ngit reset --hard HEAD~1 \nGit reset moves the tip of your working tree back to the specified revision (here, we go back one revision). The --soft flag means that the changes in the files are preserved, so all that was done was to undo the commit. If you use the --hard flag, then all changes are reverted to the specified time and later changes are lost forever."
  },
  {
    "objectID": "labs/lab2-testing.html",
    "href": "labs/lab2-testing.html",
    "title": "Lab 2: Assertions, Exceptions, and Tesing",
    "section": "",
    "text": "Today we will spend some time getting familiar with some of the programming tools that can help make your code more robust and resilient to errors and boundary conditions. Tools like unit tests, exceptions, and asserts (and next week we will spend some time on debugging misbehaving code).\nTesting is what you do when you finish implementing a piece of code and want to try it out to see if it works. Running your code manually and seeing if it works is a workable strategy for simple one-time scripts that do simple tasks, but there are situations (like writing a function that others will repeatedly use, or like running the same piece of code on hundreds of files or URLs) where it is prudent to test your code ahead of its actual use or deployment. Today we will use the pytest package to do that. We will also use some error handling techniques to make sure our code can handle malformed inputs."
  },
  {
    "objectID": "labs/lab2-testing.html#lab-exercise",
    "href": "labs/lab2-testing.html#lab-exercise",
    "title": "Lab 2: Assertions, Exceptions, and Tesing",
    "section": "Lab Exercise",
    "text": "Lab Exercise\n1- Imagine a function that:\n[option 1] takes in a string and return another string where the space separated words (or tokens) are the same as the input string but sorted according to a specific ordering. The ordering should be determined by the second argument to the function. The ordering could be specified as (1) lexicographic (alphabetic) according to the words (2) lexicographic according to the key produced by sorting the letters of the original word, or (3) numeric.\n[option 2] (in case you want to practice regex and keep stick with the theme of this week’s lecture) takes in a string and returns the first number in that string, returns None if there are no numeric values in the string.\n2- Write an interface for that function (a function name and arguments), but do not implement the function yet (you can have it return an None, or an empty string for now). We will do this in a good old fashioned .py file (not a notebook or a quarto file).\n3- Build a test suite using the pytest package to test that your function works as intended. Add at least 8 test cases with justification for each. Try to cover the main use cases, and as many potential corner cases or boundary conditions as possible.\n4- Now run the test suite. It should fail for all your tests (unless one of them was passing an empty string).\n5- Implement the function. You can do this at one go, or case by case. As you implement a case, you can rerun the test suite and see some of the tests relevant to that cases stopping to fail. When all the tests pass, you are done. This is called test-driven development.\n6- If some cases are still failing, that’s alright, we can use that failing code next week for demonstrating debugging functionality\n7- Make sure you have assertions to check for invalid input types and combinations.\n8- Make your function throw an exception for invalid input combinations.\n9- Now write a loop that calls your function on a variety of inputs including invalid inputs.\n10- In that loop, handle the thown exceptions and print something appropriate, but let the loop continue for later inputs."
  },
  {
    "objectID": "labs/lab5-codereview.html",
    "href": "labs/lab5-codereview.html",
    "title": "Lab 5: Code Reviews",
    "section": "",
    "text": "Pair up in teams of 2 (or 3 if necessary).\nOne member of the team will create a new repository on github and invite the other teammember(s) as collaborators on the repo.\nUsing the Github web UI, each team member will create a new file in the repo, and paste their code for presidential speaches into it.\nCommit you changes (from the github UI) as a pull request / separate branch.\nNow each team member will go to the pull request made by another team member and look at the code changes.\nRead eachother’s code carefully and leave some thoughtful comments.\nThe comments may be about implementation details, efficiency concerns, or style and readability\nRemember to be constructive and kind :)"
  },
  {
    "objectID": "labs/lab5-codereview.html#hands-on-steps-for-lab",
    "href": "labs/lab5-codereview.html#hands-on-steps-for-lab",
    "title": "Lab 5: Code Reviews",
    "section": "",
    "text": "Pair up in teams of 2 (or 3 if necessary).\nOne member of the team will create a new repository on github and invite the other teammember(s) as collaborators on the repo.\nUsing the Github web UI, each team member will create a new file in the repo, and paste their code for presidential speaches into it.\nCommit you changes (from the github UI) as a pull request / separate branch.\nNow each team member will go to the pull request made by another team member and look at the code changes.\nRead eachother’s code carefully and leave some thoughtful comments.\nThe comments may be about implementation details, efficiency concerns, or style and readability\nRemember to be constructive and kind :)"
  },
  {
    "objectID": "labs/lab1-submission.html",
    "href": "labs/lab1-submission.html",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "",
    "text": "By now you should already have access to the following 5 basic tools:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor of your choice\n\nToday we will use all these tools together to submit a solution for Problem Set 0 (not a real problem set) to make sure you know how to submit solutions to upcoming (real) problem sets.\nHere is a selection of some basic reference tutorials and documentation for unix, bash and unix commands, git & GitHub, quarto, python and VS Code\nSome books to learn more about Unix."
  },
  {
    "objectID": "labs/lab1-submission.html#submitting-problem-set-solutions-090123",
    "href": "labs/lab1-submission.html#submitting-problem-set-solutions-090123",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "",
    "text": "By now you should already have access to the following 5 basic tools:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor of your choice\n\nToday we will use all these tools together to submit a solution for Problem Set 0 (not a real problem set) to make sure you know how to submit solutions to upcoming (real) problem sets.\nHere is a selection of some basic reference tutorials and documentation for unix, bash and unix commands, git & GitHub, quarto, python and VS Code\nSome books to learn more about Unix."
  },
  {
    "objectID": "labs/lab1-submission.html#quick-intro-to-git-and-github",
    "href": "labs/lab1-submission.html#quick-intro-to-git-and-github",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Quick Intro to git and GitHub",
    "text": "Quick Intro to git and GitHub\n\nCreating a enw repository\nMaking changes\n\n\nEditing and saving files\nStaging changes\nCommitting changes locally\nPushing changes to remote repository\n\n\nUndoing changes:\n\n\nLocal changes\nLocal staged changes\nLocal commited changes\nPushed changes\n\n\nMerging divergent versions\nWorking with branches\nGUI options (sourcetree)\nGetting help\n\nDiscussion: - Why is git so damn complicated? - What do you need to remember when working with collaborators on the same repository?"
  },
  {
    "objectID": "labs/lab1-submission.html#hands-on-lab-instructions",
    "href": "labs/lab1-submission.html#hands-on-lab-instructions",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Hands-on Lab Instructions",
    "text": "Hands-on Lab Instructions\nProblem Set submission instructions:\n\nOpen the qmd file in any editor you like (e.g., Emacs, Sublime, ….). Use quarto preview FILE to show your rendered document live as you edit and save changes. You can put the preview window side by side with your editor, and the preview document should automatically render as you save your qmd file.\nUse VS Code with the following extensions: Python, Quarto, and Jupyter Notebooks. This allows you to execute and preview chunks (and whole document) inside VS Code. This is currently deeb’s favorite path due to how well it integrated with the Python debugger.\nUse RStudio (yes, RStudio), which can manage Python code and will display chunk output in the same way it does with R chunks. This path seems to work quite well and is recommended if you are already familiar with RStudio.\n\n\nSteps to perform today:\n\nClone your github repository to your development environment\nCreate a subdirectory in your github repository with the name ps0\nIn that subdirectory, create a quarto document (ps0.qmd) that has some simple code that creates a simple plot (you can follow this example/tutorial here)\nUse the quarto command line to render it into a pdf document (quarto render FILE –to pdf)\nCommit the changes to your repository (git add FILES; git commit -m MESSAGE; git push)\nAdd another section to your quarto document (use your imagination), then preview and commit the changes\nUse the quarto command line to render the updated document into a pdf document\nAdd the pdf document to the repository as well\nMake sure that you can log into gradescope and upload a pdf document\n[optional] Undo your last set of changes and regenerate the pdf file\n\nIf we finish early, We will also take today’s lab as an opportunity to get familiar with the basic use of all the 5 basic tools listed above.\nFor git and quarto, very basic knowledge should be sufficient for now, but for unix commands and python, the more you learn the more effective you will be at solving the problem sets (and at any computational task you take on after that). You will need to learn more advanced use of git and github towards the end of the semester when you start working with other team members on the same project.\n\n\nChunk options\nLike RMarkdown, quarto allows for several execution options to be set per document and per chunk. Spend some time getting familiar with the various options, and keep this link handy when you are working on the first few problem sets.\nDepending on what’s required in the problem sets, you may need to set eval to false (just print out code) or error to true (print errors and don’t halt rendering of the document). Some of the other options may be useful for controlling how the code gets printed."
  },
  {
    "objectID": "ps/ps6.html",
    "href": "ps/ps6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "This covers material in Units 8 and 9.\nIt’s due at 10 am (Pacific) on November 3, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that is is fine to hand-write solutions to the the non-coding questions, but make sure your writing is neat and insert any hand-written parts in order into your final submission."
  },
  {
    "objectID": "ps/ps6.html#comments",
    "href": "ps/ps6.html#comments",
    "title": "Problem Set 6",
    "section": "",
    "text": "This covers material in Units 8 and 9.\nIt’s due at 10 am (Pacific) on November 3, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that is is fine to hand-write solutions to the the non-coding questions, but make sure your writing is neat and insert any hand-written parts in order into your final submission."
  },
  {
    "objectID": "ps/ps6.html#problems",
    "href": "ps/ps6.html#problems",
    "title": "Problem Set 6",
    "section": "Problems",
    "text": "Problems\n\nIn class and in the Unit 8 notes, I mentioned that integers as large as \\(2^{53}\\) can be stored exactly in the double precision floating point representation. (Note that for this problem, you don’t need to write out e in base 2; you can use base 10).\n\nDemonstrate how the integers 1, 2, 3, …,\\(2^{53}-2\\), \\(2^{53}-1\\) can be stored exactly in the \\((-1)^{S}\\times1.d\\times2^{e-1023}\\) format where d is represented as 52 bits. I’m not expecting anything particularly formal - just write out for a few numbers and show the pattern.\nThen show that \\(2^{53}\\) and \\(2^{53}+2\\) can be represented exactly but \\(2^{53}+1\\) cannot, so the spacing of numbers of this magnitude is 2. Finally show that for numbers starting with \\(2^{54}\\) that the spacing between integers that can be represented exactly is 4. Then confirm that what you’ve shown is consistent with the result of executing \\(2.0^{53}-1\\), \\(2.0^{53}\\), and \\(2.0^{53}+1\\) in Python (you can use base Python floats or numpy).\nFinally, calculate the relative error in representing numbers of magnitude \\(2^{53}\\) in base 10. (This should, of course, look very familiar, and should be the same as the relative error for numbers of magnitude \\(2^{54}\\) or any other magnitude…)\n\nIf we want to estimate a derivative of a function on a computer (often because it is hard to calculate the derivative analytically), a standard way to approximate this is to compute: \\[f'(x)\\approx\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}\\] for some small \\(\\epsilon\\). Since the limit of the right-hand side of the expression as \\(\\epsilon\\to0\\) is exactly \\(f'(x)\\) by the definition of the derivative, we presumably want to use \\(\\epsilon\\) very small from the perspective of using a difference to approximate the derivative.\n\nConsidering the numerator, if we try to do this on a computer, in what ways (there are more than one) do the limitations of arithmetic on a computer affect our choice of \\(\\epsilon\\)? (Note that I am ignoring the denominator because that just scales the magnitude of the result and itself has 16 digits of accuracy.)\nWrite a Python function that calculates the approximation and explore how the error in the estimated derivative for some \\(x\\) varies as a function of \\(\\epsilon\\) for a (non-linear) function that you choose such that you can calculate the derivative analytically (so that you know the truth).\n\nConsider multiclass logistic regression, where you have quantities like this: \\[p_{j}=\\text{Prob}(y=j)=\\frac{\\exp(x\\beta_{j})}{\\sum_{k=1}^{K}\\exp(x\\beta_{k})}=\\frac{\\exp(z_{j})}{\\sum_{k=1}^{K}\\exp(z_{k})}\\] for \\(z_{k}=x\\beta_{k}\\). Here \\(p_j\\) is the probability that the observation, \\(y\\), is in class \\(j\\).\n\nWhat will happen if the \\(z\\) values are very large in magnitude (either positive or negative)?\nHow can we reexpress the equation so as to be able to do the calculation even when either of those situations occurs?\n\nLet’s consider importance sampling and explore the need to have the sampling density have heavier tails than the density of interest. Assume that we want to estimate \\(\\phi=EX\\) and \\(\\phi=E(X^{2})\\) with respect to a density, \\(f\\). We’ll make use of the Pareto distribution, which has the pdf \\(p(x)=\\frac{\\beta\\alpha^{\\beta}}{x^{\\beta+1}}\\) for \\(\\alpha&lt;x&lt;\\infty\\), \\(\\alpha&gt;0\\), \\(\\beta&gt;0\\). The mean is \\(\\frac{\\beta\\alpha}{\\beta-1}\\) for \\(\\beta&gt;1\\) and non-existent for \\(\\beta\\leq1\\) and the variance is \\(\\frac{\\beta\\alpha^{2}}{(\\beta-1)^{2}(\\beta-2)}\\) for \\(\\beta&gt;2\\) and non-existent otherwise.\n\nDoes the tail of the Pareto decay more quickly or more slowly than that of an exponential distribution?\nSuppose \\(f\\) is an exponential density with parameter value equal to 1, shifted by two to the right so that \\(f(x)=0\\) for \\(x&lt;2\\). Pretend that you can’t sample from \\(f\\) and use importance sampling where our sampling density, \\(g\\), is a Pareto distribution with \\(\\alpha=2\\) and \\(\\beta=3\\). Use \\(m=10000\\) to estimate \\(EX\\) and \\(E(X^{2})\\) and compare to the known expectations for the shifted exponential. Recall that \\(\\mbox{Var}(\\hat{\\phi})\\propto\\mbox{Var}(h(X)f(X)/g(X))\\). Create histograms of \\(h(x)f(x)/g(x)\\) and of the weights \\(f(x)/g(x)\\) to get an idea for whether \\(\\mbox{Var}(\\hat{\\phi})\\) is large. Note if there are any extreme weights that would have a very strong influence on \\(\\hat{\\phi}\\).\nNow suppose \\(f\\) is the Pareto distribution described above and pretend you can’t sample from \\(f\\) and use importance sampling where our sampling density, \\(g\\), is the exponential described above. Respond to the same questions as for part (b), comparing to the known values for the Pareto.\n\nExtra credit: This problem explores the smallest positive number that base Python or numpy can represent and how numbers just larger than the smallest positive number that can be represented.\n\nBy trial and error, find the base 10 representation of the smallest positive number that can be represented in Python. Hint: it’s rather smaller than \\(1\\times10^{-308}\\).\nExplain how it can be that we can store a number smaller than \\(1\\times2^{-1022}\\), which is the value of the smallest positive number that we discussed in class. Start by looking at the bit-wise representation of \\(1\\times2^{-1022}\\). What happens if you then figure out the natural representation of \\(1\\times2^{-1023}\\)? You should see that what you get is actually a very “well-known” number that is not equal to \\(1\\times2^{-1023}\\). Given the actual bit-wise representation of \\(1\\times2^{-1023}\\), show the progression of numbers smaller than that that can be represented exactly and show the smallest number that can be represented in Python written in both base 2 and base 10.\nHint: you’ll be working with numbers that are not normalized (i.e., denormalized); numbers that do not have 1 as the fixed number before the radix point in the floating point representation we discussed in Unit 8."
  },
  {
    "objectID": "ps/ps2.html",
    "href": "ps/ps2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "This covers material in Units 3 and 4.\nIt’s due at 10 am (Pacific) on September 15, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that using chunks of bash code in Qmd may be troublesome.\n\nYou will need to add engine: knitr to the YAML preface of your qmd document. The jupyter engine won’t work unless you install the Jupyter bash kernel, and even then you can’t mix bash and Python code chunks.\nFor the knitr engine, you’ll need to have R installed on your computer, including the knitr package. Quarto will then process the code chunks through knitr (which will use the reticulate package to handle Python chunks).\nIf you have trouble on your own computer, you can always render your solution for this problem set on an SCF machine (we won’t generally use bash chunks in future problem sets). (In particular I’m not quite sure what will happen if you render on Windows.)\nWe can help troubleshoot and feel free to post on Ed.\n\nYou will probably need to use sed in a basic way as we have used it so far in class and in the bash tutorial. You should not need to use more advanced functionality nor should you need to use awk, but you may if you want to."
  },
  {
    "objectID": "ps/ps2.html#comments",
    "href": "ps/ps2.html#comments",
    "title": "Problem Set 2",
    "section": "",
    "text": "This covers material in Units 3 and 4.\nIt’s due at 10 am (Pacific) on September 15, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that using chunks of bash code in Qmd may be troublesome.\n\nYou will need to add engine: knitr to the YAML preface of your qmd document. The jupyter engine won’t work unless you install the Jupyter bash kernel, and even then you can’t mix bash and Python code chunks.\nFor the knitr engine, you’ll need to have R installed on your computer, including the knitr package. Quarto will then process the code chunks through knitr (which will use the reticulate package to handle Python chunks).\nIf you have trouble on your own computer, you can always render your solution for this problem set on an SCF machine (we won’t generally use bash chunks in future problem sets). (In particular I’m not quite sure what will happen if you render on Windows.)\nWe can help troubleshoot and feel free to post on Ed.\n\nYou will probably need to use sed in a basic way as we have used it so far in class and in the bash tutorial. You should not need to use more advanced functionality nor should you need to use awk, but you may if you want to."
  },
  {
    "objectID": "ps/ps2.html#problems",
    "href": "ps/ps2.html#problems",
    "title": "Problem Set 2",
    "section": "Problems",
    "text": "Problems\n\nThis problem provides practice using shell scripting. We’ll use United Nations Food and Agriculture Organization (FAO) data on agricultural production that we saw in class. If you go to http://data.un.org/Explorer.aspx?d=FAO and click on Crops, you’ll see a bunch of agricultural products with View data links. Click on “apricots” as an example and you’ll see a Download button that allows you to download a CSV of the data. In class we inspected the HTTP requests that the site handles (using the Network information in the browser Developer Tools) and found out that you can download a file directly via a URL that sends a GET request in the following format: http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526& DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc. That downloads the data for Item 526 (apricots). You can see the itemCode for other products by hovering over View data link for the relevant product.\n\nUsing the shell to do the following. Download the data for apricots. Extract the data, excluding the metadata and the global total, into another file. Then subset the data to the year 2005. Based on the “area harvested” determine the ten regions/countries using the most land to produce apricots. Now automate your analysis and examine the top five regions/countries for 1965, 1975, 1985, 1995, and 2005. (Do not just copy and paste your code one time for each of the years!). Everything you do should be done using shell commands you save in your solution file. So you can’t say “I downloaded the data from such-and-such website” or “I unzipped the file”; you need to provide the bash code that we could run to repeat what you did. This is partly for practice in writing shell code and partly to enforce the idea that your work should be reproducible and documented.\nWarning: You may need to put the URL inside double quotes when using a UNIX shell command to download it. Also make sure there are no carriage returns in the URL (I had to break the URL above to fit on the page).\nTip: dealing with the comma separation in the file can be a problem as there are commas used inside character strings as well. You’ll probably want to convert the file to be delimited by something other than a comma - you can do this from the command line or if you would like to do it via Python, you can run python from the command line to run a little bit of Python code to do the conversion.\nWrite a bash function that takes as input a single item code (e.g., 526 for apricots, 572 for avocados), downloads the data, and prints out to the screen (i.e., to stdout) the data stored in the CSV file, such that the information could be piped to another UNIX command. Your function should detect if the user provides the wrong number of arguments and return a useful error message. It should also give useful help information if the user invokes the function as: myfun -h.\nWarning: the exact syntax for if statements in the shell can be finicky about spaces.\n\nAdd documentation, error-trapping and testing for your code from Problem 4 of PS1. You may use a modified version of your PS1 solution, perhaps because you found errors in what you did or wanted to make changes based on Chris’ solutions (to be distributed in class on Monday Sep. 11) or your discussions with other students. These topics will be covered in Lab 2 (Sep. 8) and are in some new material (as of Sep. 6) in Unit 4. Note: given our difficulties with the MLDb server on PS1, you may need to manually download the HTML for various cases that you use in developing and demonstrating your assertions and tests.\n\nAdd an informative doc string to your main, user-facing function.\nAdd a small number of assertions using assert as sanity checks of specific conditions that should be true if your code is operating correctly. These are generally intended to help in debugging.\nAdd exceptions for handling run-time errors. You should try to catch the various incorrect inputs a user could provide and anything else that could go wrong (e.g., what happens if the server refuses the request or if one is not online?). In some cases you will want to raise an error, but in others you may want to catch an error with try-except and return None.\nUse the pytest package to set up a small but thoughtful set of tests of your functions. In deciding on your tests, try to think about tricky cases that might cause problems in terms of what the website returns when you search it."
  },
  {
    "objectID": "ps/ps3.html",
    "href": "ps/ps3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "This covers material in Unit 5..\nIt’s due at 10 am (Pacific) on September 27, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements."
  },
  {
    "objectID": "ps/ps3.html#comments",
    "href": "ps/ps3.html#comments",
    "title": "Problem Set 3",
    "section": "",
    "text": "This covers material in Unit 5..\nIt’s due at 10 am (Pacific) on September 27, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements."
  },
  {
    "objectID": "ps/ps3.html#problems",
    "href": "ps/ps3.html#problems",
    "title": "Problem Set 3",
    "section": "Problems",
    "text": "Problems\n\nIn this problem we’ll create a little time-saving hack as a way to get practice with Python classes.\nSuppose I want to be lazy and when I type \"q\" in Python, Python should quit (i.e., I don’t want to have to type quit()). Write a bit of Python code to achieve this.\nHints: (a) What specifically happens if I type the name of an object? (b) You will probably want this code to be in an #| eval:false chunk, since it if it successful, its operation will cause Python to quit and presumably your document will not render.\nLet’s investigate the structure of the pandas package to get some experience with the structure of a large Python package and with how import and the __init__.py file(s) are used. You’ll need to go into the Pandas source code (see Unit 5). Note that the main __init__.py and the __init__.py files in the subpackages/submodules are complicated, and I’m not expecting you to understand everything about them. Also note that the following cases involve functions, classes, and class methods. Be sure to be clear to say which of those it is and in the method case(s), make sure you’re clear on what class the method is part of and any class inheritance structure. Import pandas and then consider the following questions:\n\nConsider pandas.core.config_init.is_terminal. What namespace is it (or its class) in? What file/module is is_terminal in? Is it a function, class, or a class method (and if a class method what is the class)? Describe how it is imported by discussing the relevant statement(s) in the relevant __init__.py file(s).\nConsider pandas.read_csv. What namespace is it (or its class) in? What file/module is read_csv in? Is it a function, class, or a class method (and if a class method what is the class)? Describe how it is imported by discussing the relevant statement(s) in the relevant __init__.py file(s).\nConsider pandas.arrays.BooleanArray. What namespace is it (or its class) in? What file/module is BooleanArray in? Is it a function, class, or a class method (and if a class method what is the class)? Describe how it is imported by discussing the relevant statement(s) in the relevant __init__.py file(s).\nConsider pandas.DataFrame.to_csv. What namespace is it (or its class) in? What file/module is to_csv in? Is it a function, class, or a class method (and if a class method what is the class)? Describe how it is imported by discussing the relevant statement(s) in the relevant __init__.py file(s).\n\nHints: (1) grep -R &lt;pattern&gt; &lt;directory&gt; will search all files within a directory recursively. (2) As you work on this, you may want to be able to modify one or more of the __init__.py files to better understand what is happening (e.g., by commenting out a line of code or adding a print statement). A good way to do this is to create a Conda environment in which pandas is installed, so you isolate any changes you make, e.g., conda create -n test_env python=3.11 pandas. Then you can edit code files in the environment and when you start Python and import pandas, you should see the effects of your changes. Alternatively, you could use the debugger to set breakpoint(s) in an __init__.py file. (3) Or you might create your own small toy package to experiment and see how things work with nested __init__.py files and various ways to use import.\nThe goal of this problem is two-fold: first to give you practice with regular expressions and string processing and the second to have you thinking about writing well-structured, readable code (similar to question 4 of PS1). The website https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union#axzz265cEKp1a has the text from all of the State of the Union speeches by US presidents. (These are the annual speeches in which the president speaks to Congress to “report” on the situation in the country.) Your task is to process the information and produce data on the speeches for the years 1900 to present. Note that while I present the problem below as subparts (a)-(i), your solution does NOT need to be divided into subparts in the same way. Your solution should do all of the downloading and processing from within Python so that your operations are self-contained and reproducible.\nYou can choose to use either a functional programming approach or an object-oriented approach, or possibly something that mixes the two. I strongly recommend that you use the approach that you are less familiar with so as to gain more experience. Please think about writing short, modular functions or methods, and operating in a vectorized manner (e.g., using map or possibly list comprehension). Think carefully about how to structure your objects to store the speech information so that the structure works well with your functions/methods.\nGiven you’ve already worked on webscraping, I’m providing some initial code for processing the main landing page in ps/ps3start.py in the course repository, and an example for getting the text of a speech. Also note that you will want to distinguish regular dashes (i.e., the hyphen in a hyphenated work like “team-building”) from a long (“em”) dash that separates clauses (unicode U+2014) – syntax from Unit 2 Section 5 may be helpful.\n\nExtract the URLs for the individual speeches from the URL above. Then use that information to read each speech into Python.\nFor each speech, extract the body of the speech.\nConvert the text so that all text that was not spoken by the president (e.g., Laughter and Applause) is stripped out.\nExtract the words and sentences from each speech as lists of strings, one element per sentence and one element per word. Note that there are probably some special cases here. Try to deal with as much as you can, but we’re not expecting perfection.\nFor each speech count the number of words and characters and compute the average word length.\nCount the following words or word stems: I, we, America{,n}, democra{cy,tic}, republic, Democrat{,ic}, Republican, free{,dom}, war, God [not including God bless], God {B,b}bless, {Jesus, Christ, Christian}, and any others that you think would be interesting.\nThe result of all of this activity should be well-structured data object(s) containing the information about the speeches.\nMake some basic plots that show how the variables have changed over time. Your response here does not have to be extensive but should illustrate what you would do if you were to proceed on to do extensive exploratory data analysis. If you’re not comfortable plotting in Python (that is the case for me) and you prefer to move the relevant data to R to make the plots, that is fine.\nExtra credit: Do some additional research and/or additional thinking to come up with additional variables that quantify speech in interesting ways. Do some plotting that illustrates how the speeches have changed over time.\n\nNow sketch out a design for a functional programming (FP) approach (if your solution to problem 3 used OOP) or an OOP approach (if your solution to problem 3 used functional programming). If you’re designing an OOP approach, decide what the classes would be and the fields and methods of those classes. If you’re designing a FP approach, decide what the functions would be and what inputs/output they would use. To be clear, you do not have to write any of the code for the methods/classes/functions; the idea is just to design the code. As your response in the OOP case, for each class, please provide a bulleted list of methods and bulleted list of fields and for each item briefly comment what the purpose is. Or in the FP case, for each function, provide a bulleted list of inputs and output and briefly comment on the purpose of each function."
  },
  {
    "objectID": "ps/ps1.html",
    "href": "ps/ps1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "This covers material in Units 2 and 4 as well as practice with Quarto.\nIt’s due at 10 am (Pacific) on September 6, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease note my comments in the syllabus about when to ask for help and about working together. In particular, please give the names of any other students that you worked with on the problem set and indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like)."
  },
  {
    "objectID": "ps/ps1.html#comments",
    "href": "ps/ps1.html#comments",
    "title": "Problem Set 1",
    "section": "",
    "text": "This covers material in Units 2 and 4 as well as practice with Quarto.\nIt’s due at 10 am (Pacific) on September 6, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease note my comments in the syllabus about when to ask for help and about working together. In particular, please give the names of any other students that you worked with on the problem set and indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like)."
  },
  {
    "objectID": "ps/ps1.html#formatting-requirements",
    "href": "ps/ps1.html#formatting-requirements",
    "title": "Problem Set 1",
    "section": "Formatting requirements",
    "text": "Formatting requirements\n\nYour electronic solution should be in the form of an Quarto file named ps1.qmd, with Python code chunks included in the file. Please see the Lab 1 and the dynamic documents tutorial for more information on how to do this.\nYour PDF submission should be the PDF produced from your qmd. Your GitHub submission should include the qmd file, any Python code files containing chunks that you read into your qmd file, and the final PDF, all named according to the submission guidelines.\nYour solution should not just be code - you should have text describing how you approached the problem and what the various steps were. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. Please see the grading rubric, and note that the output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately."
  },
  {
    "objectID": "ps/ps1.html#problems",
    "href": "ps/ps1.html#problems",
    "title": "Problem Set 1",
    "section": "Problems",
    "text": "Problems\n\nPlease read these lecture notes about how computers work, used in a class on statistical computing at CMU. Briefly (a few sentences) describe the difference between disk and memory based on that reference and/or other resources you find.\nThis problem uses the ideas and tools in Unit 2, Sections 1-3 to explore approaches to reading and writing data from files and to consider file sizes in ASCII plain text vs. binary formats in light of the fact that numbers are (generally) stored as 8 bytes per number in binary formats.\n\nGenerate a numpy array (named x) of random numbers from a standard normal distribution with 10 columns and as many rows as needed that the data takes up about 100 Mb in size. As part of your answer, show the arithmetic (formatted using LaTeX math syntax) you did to determine the number of rows.\nExplain the sizes of the two files created below. In discussing the CSV text file, how many characters do you expect to be in the file (i.e., you should be able to estimate this very accurately from first principles without using wc or any explicit program that counts characters). Hint: what do we know about numbers drawn from a standard normal distribution?\n\nimport os\nimport pandas as pd\nx = x.round(decimals = 10)\n\npd.DataFrame(x).to_csv('x.csv', header = False, index = False)\nprint(f\"{str(os.path.getsize('x.csv')/1e6)} MB\")\n\npd.DataFrame(x).to_pickle('x.pkl', compression = None) \nprint(f\"{str(os.path.getsize('x.pkl')/1e6)} MB\")\n\n167.358888 MB\n\n\n100.000572 MB\n\n\nSuppose we had rounded each number to three decimal places. Would using CSV have saved disk space relative to the pickle file?\nNow consider saving out the numbers one number per row in a CSV file. Given we no longer have to save all the commas, why is the file size unchanged (or perhaps even greater if you are on Windows)?\nRead the CSV file into Python using pandas.read_csv. Compare the speed of reading with and without providing the dtype argument and using the python vs c engines. Repeat the timing of your first attempt (without dtype and with the default engine) a few times. In some cases you might find that the first time is slower; if so this has to do with the operating system caching the file in memory (we’ll discuss this further in Unit 8).\nFinally, let’s consider reading the CSV file in chunks as discussed in Unit 2. Time how long it takes to read the first 100,000 rows.\nNow experiment with the skiprows to see if you can read in a large chunk of data from the middle of the file as quickly as the same size chunk from the start of the file. What does this indicate regarding whether Pandas/Python has to read in all the data up to the point where the chunk in the middle starts or can skip over it in some fashion? Is there any savings relative to reading all the initial rows and the chunk in the middle all at once?\nNow read the data sequentially in equal-sized chunks and determine if reading in the large chunk in the middle (after having already read the earlier chunks) takes the same amount of time as it did in part (f). Comment on what you’ve learned.\n\nPlease read Section 1 of Unit 4 on good programming/project practices and incorporate what you’ve learned from that reading into your solution for Problem 4. (You can skip the section on Assertions and Testing, as we’ll cover that in Lab.) As your response to this question, briefly (a few sentences) note what you did in your code for Problem 4 that reflects what you read. Please also note anything in Unit 4 that you disagree with, if you have a different stylistic perspective.\nWe’ll experiment with webscraping and manipulating HTML by getting song lyrics from the web. Go to http://mldb.org/search and (in the search bar in the middle, not at the left) enter the name of a song and choose to search by ‘Title’ and ‘All words’. In some cases the search goes directly to the lyrics of the song (presumably when there is no ambiguity) and in others it goes to a table of potential songs with that or similar name. (For example, compare ‘Dance in the Dark’ (or ‘Dancing in the Dark’) to ‘Leaving Las Vegas’.)\n\nBased on the GET request being sent to the MLDb server (in the cases like ‘Dance in the Dark’ where you get a table back rather than a single song’s lyrics), determine how to programmatically search for a song by ‘Title’ and ‘All words’ using Python, based on our explorations in Unit 2. Side question: what does the si parameter control?\nWarning: It’s possible that if you repeatedly query the site too quickly, it will start returning “503” errors because it detects automated usage (see problem 5 below). So, if you are going to run code from a script such that multiple queries would get done in quick succession, please put something like time.sleep(2) in between the calls that do the HTTP requests. Also when developing your code, once you have the code working to download the HTML, use the downloaded HTML to develop the remainder of your code that manipulates the HTML and don’t repeatedly re-download the HTML as you work on the remainder of the code.\nWrite an overall Python function (and modular helper functions to do particular pieces of the work needed) that takes as input a title and artist, searches by the title, and then (based on an exact match to the title and artist in the resulting set of song results) finds the URL of the page for the lyrics for that particular song. Then use that URL and return the lyrics, the artist, and the album(s). You can assume that the song you want is on the first page of results. If no exact match is found, just return None. Make sure to explain how your code extracts the HTML elements you need. Hint: you will need to use some string processing functions to do basic manipulations. We’ll see this more in Unit 5, but for now, you can find information in the https://berkeley-scf.github.io/tutorial-string-processing/text-manipulation#2-basic-text-manipulation-in-python. You should NOT need to use regular expressions (which we’ll cover in Units 3 and 5) or the re package, though you can if you want to.\nModify your function so it works either when the lyrics are returned directly from the initial search or when multiple songs are returned. Include checks in your code so that it fails gracefully if the user provides invalid input or MLDb doesn’t return a result.\n(Extra credit) Modify your code to handle cases (e.g., searching for “Dance with me”) that return more than one page of results.\n\nLook at the robots.txt file for MLDb and for Google Scholar (scholar.google.com) and the references in Unit 2 on the ethics of webscraping. Does it seem like it’s ok to scrape data from MLDb? What about Google Scholar?"
  },
  {
    "objectID": "howtos/accessingUnixCommandLine.html",
    "href": "howtos/accessingUnixCommandLine.html",
    "title": "Accessing the Unix Command Line",
    "section": "",
    "text": "You have several options for UNIX command-line access. You’ll need to choose one of these and get it working.\n\nMac OS (on your personal machine):\nOpen a Terminal by going to Applications -&gt; Utilities -&gt; Terminal\n\n\nWindows (on your personal machine):\n\nYou may be able to use the Ubuntu bash shell available in Windows.\nYour PC must be running a 64-bit version of Windows 10 Anniversary Update or later (build 1607+).\nPlease see these links for more information:\n\nhttp://blog.revolutionanalytics.com/2017/12/r-in-the-windows-subsystem-for-linux.html\nhttps://msdn.microsoft.com/en-us/commandline/wsl/install_guide\n\nFor more detailed instructions, see the Installing the Linux Subsystem on Windows tutorial.\n(Not recommended) There’s an older program called cygwin that provides a UNIX command-line interface.\n\nNote that when you install Git on Windows, you will get Git Bash. While you can use this to control Git, the functionality is limited so this will not be enough for general UNIX command-line access for the course.\n\n\nLinux (on your personal machine):\nIf you have access to a Linux machine, you very likely know how to access a terminal.\n\n\nAccess via DataHub (provided by UC Berkeley’s Data Science Education Program)\n\nGo to https://datahub.berkeley.edu\nClick on Sign in with bCourses, sign in via CalNet, and authorize DataHub to have access to your account.\nIn the mid-upper right, click on New and Terminal.\nTo end your session, click on Control Panel and Stop My Server. Note that Logout will not end your running session, it will just log you out of it.\n\n\n\nAccess via the Statistical Computing Facility (SCF)\nWith an SCF account (available here), you can access a bash shell in the ways listed below.\nThose of you in the Statistics Department should be in the process of getting an SCF account. Everyone else will need an SCF account when we get to the unit on parallel computing, but you can request an account now if you prefer.\n\nYou can login to our various Linux servers and access a bash shell that way. Please see http://statistics.berkeley.edu/computing/access.\nYou can also access a bash shell via the SCF JupyterHub interface; please see the Accessing Python instructions but when you click on New, choose Terminal. This is very similar to the DataHub functionality discussed above."
  },
  {
    "objectID": "howtos/quartoInstall.html",
    "href": "howtos/quartoInstall.html",
    "title": "Installing and Using Quarto",
    "section": "",
    "text": "Unless you plan to generate your problem set solutions on the SCF, you’ll need to install Quarto.\nOnce installed, you should be able to run commands such as quarto render FILE and quarto preview FILE from the command line.\nQuarto also runs from the Windows Command shell or PowerShell. We’ll add details/troubleshooting tips here as needed."
  },
  {
    "objectID": "howtos/gitInstall.html",
    "href": "howtos/gitInstall.html",
    "title": "Installing Git",
    "section": "",
    "text": "Here are some instructions for installing Git on your computer. Git is the version control software we’ll use in the course.\nYou can install Git by downloading and installing the correct binary from here.\nFor macOS, deeb recommends using the Homebrew option.\nGit comes installed on the SCF, so if you login to an SCF machine and want to use Git there, you don’t need to install Git.\n\nSidenotes on using Git with RStudio\nYou can work with Git through RStudio via RStudio projects.\nHere are some instructions. Here are some helpful guidelines from RStudio.\nYou may need to tell RStudio where the Git executable is located as follows.\n\nOn Windows, the git executable should be installed somewhere like: \"C:/Program Files (x86)/Git/bin/git.exe\"\nOn MacOS X, you can locate the executable by executing the following in Terminal: which git\nOnce you locate the executable, you may then need to confirm that RStudio is looking in the right place. Go to “Tools -&gt; Options -&gt; Git/SVN -&gt; Git executable” and confirm it has the correct information about the location of the git executable."
  },
  {
    "objectID": "howtos/windowsInstall.html",
    "href": "howtos/windowsInstall.html",
    "title": "R/Rstudio on Windows",
    "section": "",
    "text": "While R was built/designed for UNIX systems, it has been well adapted for Windows. Here, we’ll start with the basics of installing R on Windows. Then, we’ll cover the recommended editor (Rstudio), and how to build pdf documents using MikTeX.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial installs Windows-only versions of everything. Modern Windows systems have an Ubuntu subsystem available that we highly recommend. See the Installing the Linux Subsystem on Windows tutorial for setting up that configuration.\n\n\n\nInstalling R\nThe first step in installing the R language. This is available on CRAN (Comprehensive R Archive Network).\n\nGo to the CRAN webpage, www.r-project.org\nIn the first paragraph, click the link download R\nYou’re now on a page titled CRAN Mirrors, choose the mirror located closest to your geographic location\n\nMirrors are different servers that all host copies of the same website. You get best performance from the location closest to you.\n\nYou’re now on a paged titled The Comprehensive R Archive Network. The first box is labeled Downloand and Install R, click Download R for Windows\nClick base or install R for the first time, these take you to the same place\n\nFor more advanced things, you may need the Rtools download later. It isn’t necessary now, but remember that for the future.\n\nAt the top is a large-font link, Download R X.X.X for Windows, click this. It will begin downloading the Windows installer for R.\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\n\n\n\nInstalling RStudio\nRStudio is one of the best text editors for coding in R. It is our recommended option for beginning. After you are comfortable with the language, or if you use other languages as well, you may want to explore Atom or Sublime. More advanced options include Emacs with [ESS package][https://ess.r-project.org/] and vim with the Nvim-R plugin.\nTo install RStudio:\n\nGo to the RStudio Desktop download page, rstudio.com/products/rstudio/download/#download\nChoose the download for your OS, most likely the Windows 10/8/7 one\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\nOpen RStudio (R will run automatically in the background)\n\nYou may have to allow RStudio to run if prompted (depends on security settings and anti-virus software)\n\n\nOnce RStudio is installed, you can install or update packages in one of two ways:\n\nVia the console, using install.packages() or update.packages()\n\nVia the gui:\n\nIn the top bar, click on tools\nSelect Install Packages… to install packages\nSelect Check for Package Updates… to update packages\n\n\n\n\nCompiling PDF Documents\nFor the purposes of this class, you will be submitting homeworks as PDF documents that blend written text, code, and code-generated output. These documents are RMarkdown documents, and are dynamic documents that provide a convenient method for documenting your work (more on this in one of the lab sections). To do this, you need a LaTeX renderer. We recommend MiKTeX for Windows.\n\nGo to Getting MiKTeX to download MiKTeX for Windows, miktex.org/download\nThe first page should be Install on Windows, click Download at the bottom of the page\n\nClick the download to begin\n\n\n\n\n\n\n\nImportant\n\n\n\nFOLLOW THESE INSTALL INSTRUCTIONS.\nThe default options are fine in most places, but there is one that will cause problems.\n\n\n\nAccept the Copying Conditions, click next\nInstall only for you, click next\nUse the default directory, click next\nThis should be the Settings page. Under Install missing packages on-the-fly, change the setting to Yes, click next\n\n\n\nBecause we are using MiKTeX as an external renderer, it can’t ask you to install missing packages, and will then fail, so we have to set that installation as automatic.\n\n\nClick start (Optional, but highly recommended) Open RStudio, select a new .Rmd document, d then choose knit. This may take some time, because MiKTeX is installing new braries, but it ensures that your pipeline is setup correctly"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, numerical optimization, and numerical integration/differentiation. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python short course offered Aug. 16-17, 2023. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. In addition, we may have an optional hands-on practice session during the second or third week of class, and the GSI can also provide assistance."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, numerical optimization, and numerical integration/differentiation. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python short course offered Aug. 16-17, 2023. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. In addition, we may have an optional hands-on practice session during the second or third week of class, and the GSI can also provide assistance."
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Statistics 243 Fall 2023",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\noperate effectively in a UNIX environment and on remote servers and compute clusters;\nhave a solid understanding of general programming concepts and principles, and be able to program effectively (including having an advanced knowledge of Python functionality);\nbe familiar with concepts and tools for reproducible research and good scientific computing practices; and\nunderstand in depth and be able to make use of principles of numerical linear algebra, optimization, and simulation for statistics- and data science-related analyses and research."
  },
  {
    "objectID": "syllabus.html#topics-in-order-with-rough-timing",
    "href": "syllabus.html#topics-in-order-with-rough-timing",
    "title": "Statistics 243 Fall 2023",
    "section": "Topics (in order with rough timing)",
    "text": "Topics (in order with rough timing)\nThe ‘days’ here are (roughly) class sessions, as general guidance.\n\nIntroduction to UNIX, operating on a compute server (1 day)\nData formats, data access, webscraping, data structures (2 days)\nDebugging, good programming practices, reproducible research (1 day)\nThe bash shell and shell scripting, version control (3 days)\nProgramming concepts and advanced Python programming: text processing and regular expressions, object-oriented programming, functions and variable scope, memory use, efficient programming (9 days)\nParallel processing (2 days)\nWorking with databases, hashing, and big data (3 days)\nComputer arithmetic/representation of numbers on a computer (3 days)\nSimulation studies and Monte Carlo (2 days)\nNumerical linear algebra (5 days)\nOptimization (5 days)\nGraphics (1 day)"
  },
  {
    "objectID": "syllabus.html#personnel",
    "href": "syllabus.html#personnel",
    "title": "Statistics 243 Fall 2023",
    "section": "Personnel",
    "text": "Personnel\n\nInstructor:\n\nChris Paciorek (paciorek@stat.berkeley.edu)\n\nGSI\n\nAhmed Eldeeb (Deeb) (deeb@berkeley.edu)\n\nOffice hours can be found here.\nWhen to see us about an assignment: We’re here to help, including providing guidance on assignments. You don’t want to be futilely spinning your wheels for a long time getting nowhere. That said, before coming to see us about a difficulty, you should try something a few different ways and define/summarize for yourself what is going wrong or where you are getting stuck."
  },
  {
    "objectID": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "href": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "title": "Statistics 243 Fall 2023",
    "section": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses",
    "text": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat243/stat243-fall-2023\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/42474/discussion/\nbCourses site for course capture recordings (see Media Gallery) and possibly some other materials: https://bcourses.berkeley.edu/courses/1527498.\nGradescope for assignments (also linked from bCourses): https://www.gradescope.com/courses/569739\n\nAll course materials will be posted on here on the website (and on GitHub) except for video content, which will be in bCourses.\n\nCourse discussion\nWe will use the course Ed Discussion site for communication (announcements, questions, and discussion). You should ask questions about class material and problem sets through the site. Please use this site for your questions so that either James or I can respond and so that everyone can benefit from the discussion. I suggest you to modify your settings on Ed Discussion so you are informed by email of postings. In particular you are responsible for keeping track of all course announcements, which we’ll make on the Discussion forum. I strongly encourage you to respond to or comment on each other’s questions as well (this will help your class participation grade), although of course you should not provide a solution to a problem set problem. If you have a specific administrative question you need to direct just to me, it’s fine to email me directly or post privately on the Discussion site. But if you simply want to privately ask a question about content, then just come to an office hour or see me after class or James during/after section.\nIf you’re enrolled in the class you should be a member of the group and be able to access it. If you’re auditing or not yet enrolled and would like access, make sure to fill out the course survey and I will add you. In addition, we will use Gradescope for viewing grades."
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Statistics 243 Fall 2023",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on course webpage/GitHub, SCF tutorials, and potentially pre-recorded videos on bCourses.\nBack-up textbooks (generally available via UC Library via links below):\n\nFor bash: Newham, Cameron and Rosenblatt, Bill. Learning the bash Shell available electronically through UC Library\nFor Quarto: The Quarto reference guide\nFor statistical computing topics:\n\nGentle, James. Computational Statistics\nGentle, James. Matrix Algebra or Numerical Linear Algebra with Applications in Statistics\n\nOther resources with more detail on particular aspects of statistical computing concepts:\n\nLange, Kenneth; Numerical Analysis for Statisticians, 2nd ed. First edition available through UC library\nMonahan, John; Numerical Methods of Statistics"
  },
  {
    "objectID": "syllabus.html#section",
    "href": "syllabus.html#section",
    "title": "Statistics 243 Fall 2023",
    "section": "Section",
    "text": "Section\nThe GSI will lead a two-hour discussion section each week (there are two sections). By and large, these will only last for about one hour of actual content, but the second hour may be used as an office hour with the GSI or for troubleshooting software during the early weeks. The discussion sections will vary in format and topic, but material will include demonstrations on various topics (version control, debugging, testing, etc.), group work on these topics, discussion of relevant papers, and discussion of problem set solutions. The first section (1-3 pm) generally has more demand, so to avoid having too many people in the room, you should go to your assigned section unless you talk to me first."
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Statistics 243 Fall 2023",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course we’ll also use the Statistics Department Linux cluster. You can also use the SCF JupyterHub or the campus DataHub to access a bash shell or run an IPython notebook.\nThe software needed for the course is as follows:\n\nAccess to the UNIX command line (bash shell)\nGit\nPython (Anaconda/Miniconda is recommended but by no means required)\n\nWe have some tips for software installation (and access to DataHub), including suggestions for how to access a UNIX shell, which you’ll need to be able to do by the second week of class."
  },
  {
    "objectID": "syllabus.html#class-time",
    "href": "syllabus.html#class-time",
    "title": "Statistics 243 Fall 2023",
    "section": "Class time",
    "text": "Class time\nMy goal is to have classes be an interactive environment. This is both more interesting for all of us and more effective in learning the material. I encourage you to ask questions and will pose questions to the class to think about, respond to via online polling or Google forms, and discuss. To increase time for discussion and assimilation of the material in class, before some classes I may ask that you read material or work through tutorials in advance of class. Occasionally, I will ask you to submit answers to questions in advance of class as well.\nPlease do not use phones during class and limit laptop use to the material being covered.\nStudent backgrounds with computing will vary. For those of you with limited background on a topic, I encourage you to ask questions during class so I know what you find confusing. For those of you with extensive background on a topic (there will invariably be some topics where one of you will know more about it than I do or have more real-world experience), I encourage you to pitch in with your perspective. In general, there are many ways to do things on a computer, particularly in a UNIX environment and in Python, so it will help everyone (including me) if we hear multiple perspectives/ideas.\nFinally, class recordings for review or to make up for absence will be available through the bCourses Media Gallery, available on the Media Gallery tab on the bCourses page for the class."
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Statistics 243 Fall 2023",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nScheduling Conflicts\nCampus asks that I include this information about conflicts: Please notify me in writing by the second week of the term about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but I cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nThe main conflict that would be a problem would be the quizzes, whose dates I will determine in late August / early September.\nQuizzes are in-person. There is no remote option, and the only make-up accommodations I will make are for illness or serious personal issues. Do not schedule any travel that may conflict with a quiz.\n\n\nCourse grades\nThe grade for this course is primarily based on assignments due every 1-2 weeks, two quizzes (likely in early-mid October and mid-late November), and a final group project. I will also provide extra credit questions on some problem sets. There is no final exam. 50% of the grade is based on the problem sets, 25% on the quizzes, 15% on the project, and 10% on your participation in discussions on Ed, your responses to the in-class Google forms questions, as well as occasional brief questions that I will ask you to answer in advance of the next class.\nGrades will generally be As and Bs. An A involves doing all the work, getting full credit on most of the problem sets, doing well on the quizzes, and doing a thorough job on the final project.\n\n\nProblem sets\nWe will be less willing to help you if you come to our office hours or post a question online at the last minute. Working with computers can be unpredictable, so give yourself plenty of time for the assignments.\nThere are several rules for submitting your assignments.\n\nYou should prepare your assignments using Quarto.\nProblem set submission consists of both of the following:\n\nA PDF submitted electronically through Gradescope, by the start of class (10 am) on the due date, and\nAn electronic copy of the PDF, code file, and Quarto document pushed to your class GitHub repository, following the instructions to be provided by the GSI.\n\nOn-time submission will be determined based on the time stamp of when the PDF is submitted to Gradescope.\nAnswers should consist of textual response or mathematical expressions as appropriate, with key chunks of code embedded within the document. Extensive additional code can be provided as an appendix. Before diving into the code for a problem, you should say what the goal of the code is and your strategy for solving the problem. Raw code without explanation is not an appropriate solution. Please see our qualitative grading rubric for guidance. In general the rubric is meant to reinforce good coding practices and high-quality scientific communication.\nAny mathematical derivations may be done by hand and scanned with your phone if you prefer that to writing up LaTeX equations.\n\nNote: Quarto Markdown is an extension to the Markdown markup language that allows one to embed Python and R code within an HTML document. Please see the SCF dynamics document tutorial; there will be additional information in the first section and on the first problem set.\n\n\nSubmitting assignments\nIn the first section (September 1), we’ll discuss how to submit your problem sets both on Gradescope and via your class GitHub repository, located at https://github.berkeley.edu/&lt;your_calnet_username&gt;.\n\n\nProblem set grading\nThe grading scheme for problem sets is as follows. Each problem set will receive a numeric score for (1) presentation and explanation of results, (2) technical accuracy of code or mathematical derivation, and (3) code quality/style and creativity. For each of these three components, the possible scores are:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = satisfactory (you tried everything but there were pieces of what you did that didn’t solve or present/explain one or more problems in a complete way), and\n3 = full credit.\n\nAgain, the qualitative grading rubric provides guidance on what we want to see for full credit.\nFor components #1 and #3, many of you will get a score of 2 for some problem sets as you develop good coding practices. You can still get an A in the class despite this.\nYour total score for the PS is a weighted sum of the scores for the three components. If you turn in a PS late, I’ll bump you down by two points (out of the available). If you turn it in really late (e.g., after we start grading them), I will bump you down by four points. No credit after solutions are distributed.\n\n\nFinal project\nThe final project will be a joint coding project in groups of 3-4. I’ll assign an overall task, and you’ll be responsible for dividing up the work, coding, debugging, testing, and documentation. You’ll need to use the Git version control system for working in your group.\n\n\nRules for working together and the campus honor code\nI encourage you to work together and help each other out. However, the problem set solutions you submit must be your own. What do I mean by that?\n\nYou should first try to figure out a given problem on your own. After that, if you’re stuck or want to explore alternative approaches or check what you’ve done, feel free to consult with your fellow students and with the GSI and me.\nWhat does “consult with a fellow student mean”? You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nYou may use ChatGPT (or similar chatbots) for help with small sections of a problem (e.g., how to do some specific Python or bash task). You should not use ChatGPT to try to answer an entire question. You should carefully verify that the result is correct.\nYou must provide attribution for ideas obtained elsewhere, including other students and ChatGPT or similar chatbots.\n\nIf you got a specific idea for how to do part of a problem from a fellow student (or some other resource, including ChatGPT), you should note that in your solution in the appropriate place (for specific syntax ideas, note this in a code comment), just as you would cite a book or URL.\nYou MUST note on your problem set solution any fellow students who you worked/consulted with.\nYou do not need to cite any Ed Discussion posts nor any discussions with Chris or Deeb.\n\nUltimately, your solution to a problem set (writeup and code) must be your own, and you’ll hear from me if either look too similar to someone else’s.\n\nPlease see the last section of this document for more information on the Campus Honor Code, which I expect you to follow."
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Statistics 243 Fall 2023",
    "section": "Feedback",
    "text": "Feedback\nI welcome comments and suggestions and concerns. Particularly good suggestions will count towards your class participation grade."
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Statistics 243 Fall 2023",
    "section": "Accomodations for Students with Disabilities",
    "text": "Accomodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements."
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Statistics 243 Fall 2023",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all."
  },
  {
    "objectID": "howtos/accessingPython.html",
    "href": "howtos/accessingPython.html",
    "title": "Accessing Python",
    "section": "",
    "text": "We recommend using using the Anaconda (Python 3.11 distribution) on your laptop. Click “Download” and then click 64-bit “Graphical Installer” for your current operating system.\nOnce you’ve installed Python, please install the following packages:\nAssuming you installed Anaconda Python, you should be able to do this from the command line:\nWhile you’re welcome to work with Python in a Jupyter notebook for exploration (e.g., using the campus DataHub, you’ll need to submit Quarto (.qmd) documents with Python chunks for your problem sets. So you’ll need Python set up on your laptop or to use it by logging in to an SCF machine."
  },
  {
    "objectID": "howtos/accessingPython.html#python-from-the-command-line",
    "href": "howtos/accessingPython.html#python-from-the-command-line",
    "title": "Accessing Python",
    "section": "Python from the command line",
    "text": "Python from the command line\nOnce you get your SCF account, you can access Python or IPython from the UNIX command line as soon as you login to an SCF server. Just SSH to an SCF Linux machine (e.g., gandalf.berkeley.edu or radagast.berkeley.edu) and run ‘python’ or ‘ipython’ from the command line.\nMore details on using SSH are here. Note that if you have the Ubuntu subsystem for Windows, you can use SSH directly from the Ubuntu terminal."
  },
  {
    "objectID": "howtos/accessingPython.html#python-via-jupyter-notebook",
    "href": "howtos/accessingPython.html#python-via-jupyter-notebook",
    "title": "Accessing Python",
    "section": "Python via Jupyter notebook",
    "text": "Python via Jupyter notebook\nYou can use a Jupyter notebook to run Python code from the SCF JupyterHub or the Berkeley DataHub.\nIf you’re on the SCF JupyterHub, select Start My Server. Then, unless you are running long or parallelized code, just click Spawn (in other words, accept the default ‘standalone’ partition). On the next page select ‘New’ and ‘Python 3’.\nTo finish your session, click on Control Panel and Stop My Server. Do not click Logout."
  },
  {
    "objectID": "howtos/ps-submission.html",
    "href": "howtos/ps-submission.html",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "Problem set solutions should be written in Quarto Markdown (.qmd) source files, interspersing explanatory text with Python (and in some cases bash) code chunks. Please do not use Jupyter notebook (.ipynb) files as your underlying source file for your solutions.\nWhy?\n\nFor one or two of the initial problem sets you’ll need to include both bash and Python code. This isn’t possible in a single notebook.\nThe underlying format of .ipynb files is JSON. While this is a plain text format, the key-value pair structure is much less well-suited for use with Git version control (which relies on diff) than Markdown-based formats.\nOne can run chunks in a Jupyter notebook in arbitrary order. What is printed to PDF depends on the order in which the chunks are run and the results can differ from what one would expect based on reading the notebook sequentially and running the chunks sequentially. For example, consider the following experiment and you’ll see what I mean: (1) Have one code chunk with a = 3 and run it; (2) Add a second chunk with print(a) and run it; and (3) Change the first chunk to a=4 and DO NOT rerun the second chunk. Save the notebook to PDF. You’ll see that your “report” makes no sense. Here’s the result of me doing that experiment.\n\nIf you really want to do your initial explorations of the problems in a Jupyter notebook, with content then copied to qmd, that is fine.\nFor problem sets later in the semester, we may allow the work to be done in a Jupyter notebook (committed to the repository as the source file) and then submitted as a PDF, but the initial problem sets must be provided as qmd source files."
  },
  {
    "objectID": "howtos/ps-submission.html#submission-format",
    "href": "howtos/ps-submission.html#submission-format",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "Problem set solutions should be written in Quarto Markdown (.qmd) source files, interspersing explanatory text with Python (and in some cases bash) code chunks. Please do not use Jupyter notebook (.ipynb) files as your underlying source file for your solutions.\nWhy?\n\nFor one or two of the initial problem sets you’ll need to include both bash and Python code. This isn’t possible in a single notebook.\nThe underlying format of .ipynb files is JSON. While this is a plain text format, the key-value pair structure is much less well-suited for use with Git version control (which relies on diff) than Markdown-based formats.\nOne can run chunks in a Jupyter notebook in arbitrary order. What is printed to PDF depends on the order in which the chunks are run and the results can differ from what one would expect based on reading the notebook sequentially and running the chunks sequentially. For example, consider the following experiment and you’ll see what I mean: (1) Have one code chunk with a = 3 and run it; (2) Add a second chunk with print(a) and run it; and (3) Change the first chunk to a=4 and DO NOT rerun the second chunk. Save the notebook to PDF. You’ll see that your “report” makes no sense. Here’s the result of me doing that experiment.\n\nIf you really want to do your initial explorations of the problems in a Jupyter notebook, with content then copied to qmd, that is fine.\nFor problem sets later in the semester, we may allow the work to be done in a Jupyter notebook (committed to the repository as the source file) and then submitted as a PDF, but the initial problem sets must be provided as qmd source files."
  },
  {
    "objectID": "howtos/ps-submission.html#problem-set-solution-workflows",
    "href": "howtos/ps-submission.html#problem-set-solution-workflows",
    "title": "Problem Set Submissions",
    "section": "Problem set solution workflows",
    "text": "Problem set solution workflows\nHere we outline a few suggested workflows for developing your problem set solutions:\n\nOpen the qmd file in any editor you like (e.g., Emacs, Sublime, ….). From the command line (we think this will work from a Windows command line such as cmd.exe or PowerShell as well), run quarto preview FILE to show your rendered document live as you edit and save changes. You can put the preview window side by side with your editor, and the preview document should automatically render as you save your qmd file.\nUse VS Code with the following extensions: Python, Quarto, and Jupyter Notebooks. This allows you to execute and preview chunks (and whole document) inside VS Code. This is currently deeb’s favorite path due to how well it integrated with the Python debugger.\nUse RStudio (yes, RStudio), which can manage Python code and will display chunk output in the same way it does with R chunks. This path seems to work quite well and is recommended if you are already familiar with RStudio.\n\nLater in the semester, you may be allowed to work directly in Jupyter notebooks and use quarto to render from them directly. This has a few quirks and limitations, but may be allowed for some problem sets.\nPlease commit your work regularly to your repository as you develop your solutions."
  },
  {
    "objectID": "howtos/ps-submission.html#github-repository",
    "href": "howtos/ps-submission.html#github-repository",
    "title": "Problem Set Submissions",
    "section": "GitHub repository",
    "text": "GitHub repository\n\nSetting up your repository\nWe are creating repositories for everyone at github.berkeley.edu. Additionally, homeworks still need to be submitted as PDFs on Gradescope.\nSteps:\n\nLog into github.berkeley.edu using your Berkeley credentials. Because of how the system works, you will need to log in before your account is created. Nothing else needs to be done, just log in and log out.\nAfter accounts are created (may take a couple days after first login), when you log in again, you should see one private repository listed on the left side (e.g., stat243-fall-2022/ahv36). This is your class repository. Do not change the repository settings! They are set up for this class.\nClone the repo to your home directory (I would clone it into a directory just for repositories (e.g., I use ~/repos). In the top-level of your working directory, you should create a file named (exactly) .gitignore.\n\nThe .gitignore file causes Git to ignore transient or computer-specific files that Quarto generates. (more info at https://github.com/github/gitignore) In it, put (again, don’t put dashed lines):\n# cache directories\n/__pycache__\n\n# pickle files\n*.pkl\n*.pickle\n\n\nRepository Organization\nThe problem sets in your repository should be organized into folders with specific filenames.\nWhen we pull from your repository, our code will be assuming the following structure:\nyour_repo/\n├── ps1/\n│   ├── ps1.pdf\n│   ├── ps1.qmd \n│   ├── &lt;possibly auxiliary code or other files&gt;\n├── ps2/\n│   ├── ...\n├── ...\n├── ps8/\n├── .gitignore\n└── info.json\nThe file names are case-sensitive, so please keep everything lowercase."
  },
  {
    "objectID": "howtos/RandRStudioInstall.html",
    "href": "howtos/RandRStudioInstall.html",
    "title": "Installing R & RStudio",
    "section": "",
    "text": "On your laptop\nIf your version of R is older than 4.0.0, please install the latest version.\nTo install R, see:\n\nMacOS: install the R-4.2.1.pkg from https://cran.rstudio.com/bin/macosx\nWindows: https://cran.rstudio.com/bin/windows/base/\nLinux: https://cran.rstudio.com/bin/linux/\n\nThen install RStudio. To do so, see https://www.rstudio.com/ide/download/desktop, scrolling down to the “Installers for Supported Platforms” section and selecting the Installer for your operating system.\nVerify that you can install add-on R packages by installing the ‘fields’ package. In RStudio, go to ‘Tools-&gt;Install Packages’. In the resulting dialog box, enter ‘fields’ (without quotes) in the ‘Packages’ field. Depending on the location specified in the ‘Install to Library’ field, you may need to enter your administrator password. To be able to install packages to the directory of an individual user, you may need to do the following:\n\nIn R, enter the command Sys.getenv()['R_LIBS_USER'].\nCreate the directory specified in the result that R returns, e.g., on a Mac, this might be ~/Library/R/4.0/library.\n\nFor more detailed installation instructions for Windows, see Using R, RStudio, and LaTeX on Windows file.\n\n\nVia DataHub\nSee the instructions in Accessing the Unix Command Line for how to login to Datahub. Then in the mid-upper right, click on New and RStudio. Alternatively, to go directly to RStudio, go to https://r.datahub.berkeley.edu."
  },
  {
    "objectID": "howtos/windowsAndLinux.html",
    "href": "howtos/windowsAndLinux.html",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "",
    "text": "Windows 10 has a powerful new feature that allows a full Linux system to be installed and run from within Windows. This is incredibly useful for building/testing code in Linux, without having a dedicated Linux machine, but it poses strange new behaviors as two very different operating systems coexist in one place. Initially, this document mirrors the Windows Install tutorial, showing you how to install Ubuntu and setting up R, RStudio, and LaTex. Then, we cover some of the issues of running two systems together, starting with finding files, finding the Ubuntu subsystem, and file modifications."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-ubuntu",
    "href": "howtos/windowsAndLinux.html#installing-ubuntu",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nThere are 2 parts to installing a Linux subsystem in Windows. I will write this using Ubuntu as the example, as it is my preferred Linux distro, but several others are provided by Windows.\nSources:\n\nOfficial Windows Instructions\nUbuntu Update Instructions\n\n\n1) Enable Linux Subsystem\nBy default, the Linux subsystem is an optional addition in Windows. This feature has to be enabled prior to installing Linux. There are two ways to do it.\n\nCMD Line\nThe simplest way to enable the Linux subsystem is through PowerShell.\n\nOpen PowerShell as Administrator\nRun the following (on one line):\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\nRestart the computer\n\nGUI\nIf you don’t wish to use PowerShell, you can work your way through the control panel and turn on the Linux subsystem.\n\nOpen the settings page through the search bar\nGo to Programs and Features\nFind Turn Windows Features on or off on the right side\nEnable the Windows Subsystem for Linux option\nRestart the computer\n\n\n\n\n2) Install Linux Subsystem\nOnce the Linux subsystem feature has been enabled, there are multiple methods to download and install the Linux distro you want. I highly recommend installing Ubuntu from the Microsoft store. There are several other flavors available as well, but Ubuntu is generally the easiest to learn and the most well supported.\n\nOpen the Microsoft Store\nSearch for Ubuntu\n\nYou’re looking for the highest number followed by LTS, currently 20.04 LTS (or 18.04 LTS is fine too). This is the current long-term-release, meaning it will be supported for the next 5 years.\n\nClick on the tile, then click Get, and this should start the installation.\nFollow the prompts to install Ubuntu.\n\nAfter installing Ubuntu, it is advisable to update it. This is something you should do on a regular basis.\n\nOpen a Bash terminal.\nType sudo apt update to update your local package database.\nType sudo apt upgrade to upgrade your installed packages."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "href": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Using the Linux Terminal from R in Windows",
    "text": "Using the Linux Terminal from R in Windows\nTo get all the functionality of a UNIX-style commandline from within R (e.g., for bash code chunks), you should set the terminal under R in Windows to be the Linux subsystem."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "href": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "A Note on File Modification",
    "text": "A Note on File Modification\nDO NOT MODIFY LINUX FILES FROM WINDOWS\nIt is highly recommended that you never modify Linux files from Windows because of metadata corruption issues. Any files created under the Linux subsystem, only modify them with Linux tools. In contrast, you can create files in the Windows system and modify them with both Windows or Linux tools. There could be file permission issues because Windows doesn’t have the same concept of file permissions as Linux. So, if you intend to work on files using both Linux and Windows, create the files in the C drive under Windows, and you should be safe to edit them with either OS."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "href": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Windows from Linux",
    "text": "Finding Windows from Linux\nOnce you have some flavor of Linux installed, you need to be able to navigate from your Linux home directory to wherever Windows stores files. This is relatively simple, as the Windows Subsystem shows Windows to Linux as a mounted drive.\n\nSource\n\n\nOpen a Bash terminal.\nType cd / to get to the root directory.\nIn root, type cd /mnt. This gets you to the mount point for your drives.\nType ls to see what drives are available (you should see a c, and maybe d as well).\nType cd c to get into the Windows C drive. This is the root of the C directory for Windows.\nTo find your files, change directy into the users folder, then into your username.\n\ncd Users/&lt;your-user-name&gt;\nThis is your home directory in Windows. If you type ls here, you should see things like\n\nDocuments\nDownloads\nPictures\nVideos\netc…"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "href": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Linux from Windows",
    "text": "Finding Linux from Windows\nThis is slightly more tricky than getting from Linux to Windows. Windows stores the Linux files in a hidden subfolder so that you don’t mess with them from Windows. However, you can find them, and then the easiest way (note, do not read as safest or smartest) to find those files in the future is by creating a desktop shortcut.\n\nSource\n\n\nOpen File Explorer\nIn the address bar, type %userprofile%\\AppData\\Local\\Packages\n\n%userprofile% will expand to something like C:\\Users\\&lt;your-user-name&gt;\n\nLook for a folder related to the Linux distro that you installed\n\nThese names will change slightly over time, but look for something similar-ish.\nFor Ubuntu, look for something with CanonicalGroupLimited.UbuntuonWindows in it.\n\nCanonical is the creator/distributor of Ubuntu.\n\n\nClick LocalState\nClick rootfs\n\nThis is the root of your Linux distro.\n\nClick into home and then into your user name.\n\nThis is your home directory under Linux.\n\nDO NOT MODIFY THESE FILES FROM WINDOWS\n\nData corruption is a possibility.\n\n\nSo, the final path to find your home directory from windows will look like:\n%userprofile%\\AppData\\Local\\Packages\\&lt;Distro-Folder&gt;\\LocalStat\\rootfs\\home\\&lt;your-user-name&gt;\\"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing R on the Linux Subsystem",
    "text": "Installing R on the Linux Subsystem\nIMPORTANT: This section is only if you’d like to try using R under Linux. For class, using R under Windows should be fine.\nThe Linux Subsystem behaves exactly like a regular Linux installation, but for completeness, I will provide instructions here for people new to Linux. These instructions are written from the perspective of Ubuntu, but will be similar for other repos.\nR is not a part of the standard Ubuntu installation. So, we have to add the repository manually to our repository list. This is relatively straightforward, and R supports several versions of Ubuntu.\nSources:\n\nCRAN guide for Ubuntu\nDigital Ocean quick tutorial\n\n\nIn a bash window, type:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n\nThis adds the key to “sign”, or validate, the R repository\n\nThen, type:\nsudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'\n\ncloud.r-project.org is the default mirror, however, it is prudent to connect to the mirror closest to you geographically. Berkeley has it’s own mirror, so the command with the Berkeley mirror would look like\n\nsudo add-apt-repository 'deb https://cran.r-project.org/bin/linux/ubuntu/bionic-cran40/'\nFinally, type sudo apt install r-base, and press y to confirm installation\nTo test that it worked, type R into the console, and an R session should begin\n\nType q() to quit the R session"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Rstudio on the Linux Subsystem",
    "text": "Installing Rstudio on the Linux Subsystem\n\n\n\n\n\n\nWarning\n\n\n\nTHIS NO LONGER WORKS\n\n\nAs of Rstudio 1.5.x, it does not run on WSL. Link\nAlso possible issues, WSL has no GUI, and therefore can’t support anything that uses a GUI.\nThese instructions work, but Rstudio doesn’t run.\nSources:\n\nRstudio\nSource\n\n\nGo the the Rstudio website (link above) and download the appropriate Rstudio Desktop version.\n\nFor most people, this is the Ubuntu 18 (64-bit) installer.\nSave it somewhere that you can find it.\nYou should have a file similar to rstudio-&lt;version number&gt;-amd64.deb\n\nOpen a terminal window and navigate to wherever you saved the rstudio install file.\nType the command sudo dpkg -i ./rstudio-&lt;version number&gt;-amd64.deb\n\nThis tells the package installer (dpkg) to install (-i) the file specified (./thing.deb)\n\nType the command sudo apt-get install -f\n\nThis tells the package manager (apt-get) to fix (-f) any dependency issues that may have arisen when installing the package.\n\nType the command which rstudio to make sure the system can find it.\n\nOutput should be similar to /usr/bin/rstudio\n\nRun rstudio from linux by typing rstudio &\n\nThe & runs it in the background, allowing you to close the terminal window."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing LaTeX on the Linux Subsystem",
    "text": "Installing LaTeX on the Linux Subsystem\n\n\n\n\n\n\nImportant\n\n\n\nThis section is only if you’d like to try using LaTeX under Linux. For class, using LaTeX (or R Markdown) under Windows should be fine.\n\n\nLaTeX is a text-markup language used when generating documents from .Rmd files.\nSource LaTeX\n\nType sudo apt-get install texlive-full, press y to confirm installation\n\nGenerally, if you want to create and edit R Markdown documents you will also need a text editor to go with your LaTeX installation, but we won’t go into that here."
  },
  {
    "objectID": "ps/ps4.html",
    "href": "ps/ps4.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "This covers material in Unit 5, Sections 7-9.\nIt’s due at 10 am (Pacific) on October 9, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements."
  },
  {
    "objectID": "ps/ps4.html#comments",
    "href": "ps/ps4.html#comments",
    "title": "Problem Set 4",
    "section": "",
    "text": "This covers material in Unit 5, Sections 7-9.\nIt’s due at 10 am (Pacific) on October 9, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements."
  },
  {
    "objectID": "ps/ps4.html#problems",
    "href": "ps/ps4.html#problems",
    "title": "Problem Set 4",
    "section": "Problems",
    "text": "Problems\n\nThis problem will have you write a decorator that collects (and optionally reports) timing information on how long it takes the decorated function to run. Note that you can just present a unified solution; you don’t need to answer each part separately. Please set up the decorator in a module, but demonstrate use the decorator directly in your qmd file.\n\nWrite the decorator so that it times the function and prints timing information out for that single execution of the function, as well as returning the result of the function.\nModify the decorator so that when the function is called with a special “back-door” argument, called _REPORT, set to True, it returns a summary of the times for all previous executions of the function rather than invoking the actual computation. Note that your decorator should work regardless of how many arguments the decorated function takes. As an example of the desired behavior, myfun(_REPORT = True) rather than myfun(x,y) should cause the summary to be returned.\nHint: You’ll need to fool around with passing additional arguments into the wrapper function defined in the decorator function, and the order of how you do that relative to the *args and **kwargs will matter.\nNote: The use of the underscore in _REPORT is intended to avoid any conflicts in the event that the decorated function itself has a regular argument named REPORT. We want to set up the naming in situations like this such that our code won’t interact badly with code that would be written by users, which usually means having our naming be “special” in some way.\nSet things up so that if one sets a global variable called TIMING to be False, then no timing is done. I.e., by setting TIMING to True vs. False, we can toggle whether the decorator timing functionality is operating at all.\n\nThis problem explores how Python stores strings.\n\nLet’s consider the following lists of strings. Determine what storage is reused (if any) in storing ‘abc’ in the two lists.\n\na = ['abc', 'xyz', 'def', 'ghi']\nb = ['abc']*4\n\nNext, let’s dig into how much memory is used to store the information in a list of strings. Determine (i) how much memory is used to store a simple string (and how does this vary with the length of the string), including any metadata, (ii) how much memory is used for metadata of the list object, and (iii) how much is used for any references to the actual elements of the list (i.e., how the size of the list grows with the number of elements). Experimenting with lists of different lengths and strings of different lengths should allow you to work this out from examples without having to try to find technical documentation for Python’s internals.\n\nConsider multiplying an arbitrary \\(n \\times n\\) matrix \\(X\\) and a diagonal \\(n \\times n\\) matrix \\(D\\).\n\nHow many multiplications are done if you simply matrix multiply \\(D\\) and \\(X\\)?\nIn principle, how many multiplications need to be done to obtain the result without doing unnecessary calculations?\nHow can I use numpy functions/methods to compute \\(XD\\) efficiently?\nHow can I use numpy functions/methods to compute \\(DX\\) efficiently?\n\nThis problem asks you to efficiently compute a somewhat complicated log likelihood, arising from a computation from a student’s PhD research. The following is the probability mass function for an overdispersed binomial random variable: \\[\nP(Y =y;n,p,\\phi)  =  \\frac{f(y;n,p,\\phi)}{\\sum_{k=0}^{n}f(k;n,p,\\phi)} \\\\\n\\] \\[\nf(k;n,p,\\phi)  =  {n \\choose k}\\frac{k^{k}(n-k)^{n-k}}{n^{n}}\\left(\\frac{n^{n}}{k^{k}(n-k)^{n-k}}\\right)^{\\phi}p^{k\\phi}(1-p)^{(n-k)\\phi}\n\\]\nwhere the denominator of \\(P(Y =y;n,p,\\phi)\\) serves as a normalizing constant to ensure this is a valid probability mass function.\nWe’ll explore how would one efficiently code the computation of the denominator. For our purposes here you can take \\(n=10000\\), \\(p=0.3\\) and \\(\\phi=0.5\\) when you need to actually run your code. Recall that \\(0^0=1\\).\n\nWrite a basic version using map/apply style operations, where you have a function that carries out a single calculation of \\(f\\) for a value of \\(k\\) and then use map/apply to execute it for all the elements of the sum. Make sure to do all calculations on the log scale and only exponentiate before doing the summation. This avoids the possibility of numerical overflow or underflow that we’ll discuss in Unit 8.\nNow create a vectorized version using numpy arrays. Compare timing to the basic non-vectorized version.\nUse timing and profiling tools to understand what steps are slow and try to improve your efficiency. Keep an eye out for repeated calculations and calculations/operations that don’t need to be done. Compare timing to your initial vectorized version in (b)."
  },
  {
    "objectID": "ps/ps5.html",
    "href": "ps/ps5.html",
    "title": "Problem Set 5",
    "section": "",
    "text": "This covers material in Units 6 and 7 (and a bit into Unit 9).\nIt’s due at 10 am (Pacific) on October 27, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nRunning parallel code, working with large datasets, and using shared computers can result in delays and unexpected problems. Don’t wait until the last minute. The lengthy time period to complete this problem set is simply a logistical result of when the quiz is scheduled – you should start on the problem set before the quiz. We are happy to help troubleshoot problems you may have with accessing the SCF, running parallel Python code, submitting jobs to the SCF cluster, etc. We will not be happy to do so if it is clear that you started at the last minute.\nGiven that you’ll be running batch jobs and operating on remote servers, for problems 1 and 2 you’ll need to provide your code in chunks with #| eval: false. You can paste in any output you need to demonstrate your work. Remember that you can use “```” to delineate blocks of text you want printed verbatim.\nTo monitor an sbatch job, see this SCF doc."
  },
  {
    "objectID": "ps/ps5.html#comments",
    "href": "ps/ps5.html#comments",
    "title": "Problem Set 5",
    "section": "",
    "text": "This covers material in Units 6 and 7 (and a bit into Unit 9).\nIt’s due at 10 am (Pacific) on October 27, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nRunning parallel code, working with large datasets, and using shared computers can result in delays and unexpected problems. Don’t wait until the last minute. The lengthy time period to complete this problem set is simply a logistical result of when the quiz is scheduled – you should start on the problem set before the quiz. We are happy to help troubleshoot problems you may have with accessing the SCF, running parallel Python code, submitting jobs to the SCF cluster, etc. We will not be happy to do so if it is clear that you started at the last minute.\nGiven that you’ll be running batch jobs and operating on remote servers, for problems 1 and 2 you’ll need to provide your code in chunks with #| eval: false. You can paste in any output you need to demonstrate your work. Remember that you can use “```” to delineate blocks of text you want printed verbatim.\nTo monitor an sbatch job, see this SCF doc."
  },
  {
    "objectID": "ps/ps5.html#problems",
    "href": "ps/ps5.html#problems",
    "title": "Problem Set 5",
    "section": "Problems",
    "text": "Problems\n\nThis problem asks you to use Dask to process some Wikipedia traffic data and makes use of tools discussed in Section on October 6. The files in /scratch/users/paciorek/wikistats/dated_2017_small/dated (on the SCF) contain data on the number of visits to different Wikipedia pages on November 4, 2008 (which was the date of the US election in 2008 in which Barack Obama was elected). The columns are: date, time, language, webpage, number of hits, and page size. (Note that the Unit 7 Dask bag example and Question 2 below use a larger set of the same data.)\n\nIn an interactive session on the SCF Linux cluster (ideally on the low partition, but if necessary on the high partition), started using srun (with four cores requested) as discussed in Section:\n\nCopy the data files to a subdirectory of the /tmp directory of the machine your interactive session is running on. (Keep your code files in your home directory.) Putting the files on the local hard drive of the machine you are computing on reduces the amount of copying data across the network (in the situation where you read the data into your program multiple times) and should speed things up in step ii.\nWrite efficient Python code to do the following: Using the dask package as seen in Unit 6, with either map or a list of delayed tasks, write code that, in parallel, reads in the space-delimited files and filters to only the rows that refer to pages where “Barack_Obama” appears in the page title (column 4). You can use the code from Unit 6 as a template. Collect all the results into a single data frame. In your srun invocation and in your code, please use four cores in your parallelization so that other cores are saved for use by other users/students. IMPORTANT: before running the code on the full set of data, please test your code on a small subset first (and test your function on a single input file serially).\nTabulate the number of hits for each hour of the day. (I don’t care how you do this - using either Python or R is fine.) Make a (time-series) plot showing how the number of visits varied over the day. Note that the time zone is UTC/GMT, so you won’t actually see the evening times when Obama’s victory was announced - we’ll see that in Question 2. Feel free to do this step outside the context of the parallelization. You’ll want to use datetime functions from Pandas or the datetime package to manipulate the timing information (i.e., don’t use string manipulations).\nRemove the files from /tmp.\n\nTips:\n\nIn general, keep in mind various ideas from Unit 2 about reading data from files. A couple things that posed problems when I was prototyping this in using pandas.read_csv were that there are lines with fewer than six fields and that there are lines that have quotes that should be treated as part of the text of the fields and not as separators. To get things to work ok, I needed to set the dtype to str for the first two fields (for ease of dealing with the date/time info later) but NOT set the dtype for the other fields, and to use the quoting argument to handle the literal quotes.\nWhen starting your srun session, please include the flag --mem-per-cpu=5G when submitting the Slurm job. In general one doesn’t need to request memory when submitting jobs to the SCF cluster but there is a weird interaction between Dask and Slurm that I don’t quite understand that requires this.\n\nNow replicate steps (i) and (ii) but using sbatch to submit your job as a batch job to the SCF Linux cluster, where step (ii) involves running Python from the command line. You don’t need to make the plot again. As discussed here in the Dask documentation, put your Python/Dask code inside an if __name__ == '__main__' block.\nNote that you need to copy the files to /tmp in your submission script, so that the files are copied to /tmp on whichever node of the SCF cluster your job gets run on. Make sure that as part of your sbatch script you remove the files in /tmp at the end of the script. (Why? In general /tmp is cleaned out when a machine is rebooted, but this might take a while to happen and many of you will be copying files to the same hard drive so otherwise /tmp could run out of space.)\n\nConsider the Wikipedia traffic data for October 15-November 2008 (already available in /var/local/s243/wikistats/dated_2017_sub on any of the SCF cluster nodes in the low or high partitions). Again, explore the variation over time in the number of visits to Barack Obama-related Wikipedia sites, based on searching for “Barack_Obama” on English language Wikipedia pages. You should use Dask with distributed data structures to do the reading and filtering, as seen in Unit 7. Then group by day-hour (it’s fine to do the grouping/counting in Python in a way that doesn’t use Dask data structures). You can do this either in an interactive session using srun or a batch job using sbatch. And if you use srun, you can run Python itself either interactively or as a background job. Time how long it takes to do the Dask part of the computations to get a sense for how much time is involved working with this much data. Once you have done the filtering and gotten the counts for each day-hour, you can simply use standard Python or R code on your laptop to do some plotting to show how the traffic varied over the days of the full month-long time period and particularly over the hours of November 3-5, 2008 (election day was November 4 and Obama’s victory was declared at 11 pm Eastern time on November 4).\nNotes:\n\nThere are various ways to do this using Dask bags or Dask data frames, but I think the easiest in terms of using code that you’ve seen in Unit 7 is to read the data in and do the filtering using a Dask bag and then convert the Dask bag to a Dask dataframe to do the grouping and summarization. Alternatively you should be able to use foldby() from dask.bag, but figuring out what arguments to pass to foldby() is a bit involved.\nMake sure to test your code on a portion of the data before doing computation on the full dataset. Reading and filtering the whole dataset will take something like 30 minutes with 16 cores. You MUST test on a small number of files on your laptop or on one of the stand-alone SCF machines (e.g., radagast, gandalf, arwen) before trying to run the code on the full 40 GB (zipped) of data. For testing, the files are also available in /scratch/users/paciorek/wikistats/dated_2017_sub.\nWhen doing the full computation via your Slurm job submission:\n\nDon’t copy the data (unlike in Question 1) to avoid overloading our disks with each student having their own copy. Just use the data from /var/local/s243/wikistats/dated_2017_sub.\nPlease do not use more than 16 cores in your Slurm job submissions so that cores are available for your classmates. If your job is stuck in the queue you may want to run it with 8 rather than 16 cores.\nAs discussed in Section, when you use sbatch to submit a job to the SCF cluster or srun to run interactively, you should be using the --cpus-per-task flag to specify the number of cores that your computation will use. In your Python code, you can then either hard-code that same number of cores as the number of workers or (better) you can use the SLURM_CPUS_PER_TASK shell environment variable to tell Dask how many workers to start.\n\n\nSQL practice.\n\nUsing the Stack Overflow database, write SQL code that will determine how many users have asked both R- and Python-related questions (not necessarily, but possibly, about R and Python in the same question). There are various ways to do this; you might do this in a single query (for which there are various options), but it’s perfectly fine to create one or more views and then use those views to get the result as a subsequent query. (Hint: if you do this using joins, you’ll probably need to do the equivalent of a self join at some point.)\nExtend your query to include only users who have asked questions about R (that are not also about Python) and questions about Python (that are not also about R). How many such users are there?\n\n(Please wait to work on this until the week of Oct. 23 if you can.) This question prepares for the discussion of a simulation study in section on Friday October 27. The goal of the problem is to think carefully about the design and interpretation of simulation studies, which we’ll talk about in Unit 9, in particular in Section on Friday October 27. In particular, we’ll work with Cao et al. (2015), an article in the Journal of the Royal Statistical Society, Series B, which is a leading statistics journal. The article is available as cao_etal_2015.pdf under the ps directory on GitHub. Read Sections 1, 2.1, and 4 of the article. Also read Section 2 of Unit 9.\nYou don’t need to understand their method for fitting the regression (i.e., you can treat it as some black box algorithm) or the theoretical development. In particular, you don’t need to know what an estimating equation is - you can think of it as an alternative to maximum likelihood or to least squares for estimating the parameters of the statistical model. Equation 3 on page 759 is analogous to taking the sum of squares for a regression model, differentiating with respect to \\(\\beta\\), and setting equal to zero to solve to get \\(\\hat{\\beta}\\). In Equation 3, to find \\(\\hat{\\beta}\\) one sets the equation equal to zero and solves for \\(\\beta\\). As far as the kernel, its role is to weight each pair of observation and covariate value. This downweights pairs where the covariate is measured at a very different time than the observation.\nBriefly (a few sentences for each of the three questions below) answer the following questions.\n\nWhat are the goals of their simulation study, and what are the metrics that they consider in assessing their method?\nWhat choices did the authors have to make in designing their simulation study? What are the key aspects of the data generating mechanism that might affect their assessment of their method?\nConsider their Tables reporting the simulation results. For a method to be a good method, what would one want to see numerically in these columns?"
  },
  {
    "objectID": "ps/regex.html",
    "href": "ps/regex.html",
    "title": "Assignment: regex problems",
    "section": "",
    "text": "Overview\nRead the regular expression section of the bash shell tutorial and provide regular expression syntax to match the strings in the following scenarios. Any reasonable syntax si fine, and even better, challenge yourself to figure out multiple ways to answer each question.\nThis is an assignment, graded credit/no credit, and it does not need to follow the requirements for problem set submissions. Any format is fine (including hand-written and scanned).\n\n\nProblems\n\nMatch the strings “dog”, “Dog”, “dOg”, “doG”, “DOg”, etc. (The word ‘dog’ in any combination of lower and upper case.)\nMatch the strings “cat”, “at”, and “t”.\nMatch the strings “cat”, “caat”, “caaat”, etc.\nMatch two words separated by any amount of whitespace.\nMatch exactly two words separated by ay amount of whitespace and with or without whitespace at the beginning and end. (I.e., you shouldn’t match if there are three words.)"
  },
  {
    "objectID": "labs/06/scf.html",
    "href": "labs/06/scf.html",
    "title": "SCF Computing Cluster",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "labs/06/scf.html#logging-in",
    "href": "labs/06/scf.html#logging-in",
    "title": "SCF Computing Cluster",
    "section": "Logging in",
    "text": "Logging in\nThe SCF has a number of login nodes which you can access via ssh.\n\n\n\n\n\n\nNote\n\n\n\nFor info on using ssh (including on Windows), see here.\n\n\nFor example, I’ll use ssh to connect to the dorothy node:\njames@pop-os:~$ ssh &lt;scf-username&gt;@dorothy.berkeley.edu\nThe authenticity of host 'dorothy.berkeley.edu (128.32.135.58)' can't be established.\nED25519 key fingerprint is SHA256:rOY7ED/iIiTgI++Y4XHmiEl+tC+OmSGBvWp03CSII5E.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added 'dorothy.berkeley.edu' (ED25519) to the list of known hosts.\nNotice that upon first connecting to a server you haven’t visited there is a warning that the “authenticity of the host … can’t be established”. So long as you have typed in the hostname correctly (dorothy.berkeley.edu, in this case), and trust the host (we trust the SCF!) then you can type yes to add the host to your known hosts file (found on your local machine at ~/.ssh/known_hosts).\nYou’ll then be asked to enter your password for the SCF cluster. For privacy, you won’t see anything happen in your terminal when you type it in, so type carefully (you can use Backspace if you make a mistake) and press Enter when you’re done. If you were successful, you should see a welcome message and your shell prompt, like:\n&lt;scf-username&gt;@dorothy:~$\nTo get your bearings, you can type pwd to see where your home directory is located on the SCF cluster filesystem:\n&lt;scf-username&gt;@dorothy:~$ pwd\n/accounts/grad/&lt;scf-username&gt;\nYour home directory is likely also in the /accounts/grad/ directory, as mine is.\n\nOther login nodes\n\n\n\n\n\n\nImportant\n\n\n\nDon’t run computationally intensive tasks on the login nodes!\nThey are shared by all the SCF users, and should only be used for non-intensive interactive work such as job submission and monitoring, basic compilation, managing your disk space, and transferring data to/from the server.\n\n\nIf for some reason dorothy is not working for you, the SCF has a number of nodes which can be accessed from your local machine with commands of the form ssh &lt;scf-username&gt;@&lt;hostname&gt;.berkeley.edu. Currently, these are:\n\naragorn\narwen\ndorothy\ngandalf\ngollum\nhermione\nquidditch\nradagast\nshelob"
  },
  {
    "objectID": "labs/06/scf.html#disk-space",
    "href": "labs/06/scf.html#disk-space",
    "title": "SCF Computing Cluster",
    "section": "Disk space",
    "text": "Disk space\nYour home directory has a limited amount of disk space; you can check how much you have used and available using the quota command, which will show your home directory usage and quota on the line for the accounts filesystem.\n&lt;scf-username&gt;@dorothy:~$ quota\nFILESYSTEM  USE         QUOTA       %\n  accounts  5.29 G      20 G        26.5\nThe accounts filesystem is accessible from all the nodes on the SCF cluster. This means that regardless of which login or compute node you use, you will have access to the files in your home directory. Moreover, your home directory is backed up regularly, so you are protected from accidental data loss.\nIf you’re running out of space, you should try to selectively delete large files that you no longer need. See here for some tips on finding large files. If all else fails, you can request additional space or request access the /scratch filesystem. The latter is a good place for large datasets, but note that /scratch is not backed up, unlike your home directory. See the previous link for more info.\nFor temporary files (e.g., intermediate results of a computation that you don’t need to store for later), every machine has a /tmp filesystem. However, /tmp is always linked to the specific machine you are on, meaning that if you put something in /tmp on dorothy and then later go to aragorn, you won’t find your files in the /tmp directory there. If you go back to dorothy, you will likely find them again, but /tmp is automatically wiped when a machine reboots, so only use it for files you don’t care about preserving!"
  },
  {
    "objectID": "labs/06/scf.html#data-transfer-scp-sftp",
    "href": "labs/06/scf.html#data-transfer-scp-sftp",
    "title": "SCF Computing Cluster",
    "section": "Data transfer: SCP / SFTP",
    "text": "Data transfer: SCP / SFTP\nWe can use the scp and sftp protocols to transfer files to and from any login node on the SCF cluster. scp is a shell program that should be available by default on macOS and Linux, while on Windows you can use WinSCP. WinSCP can also do sftp transfers, or you can use FileZilla on any platform for sftp. Both WinSCP and FileZilla have a GUI that allows you to drag and drop files between your local machine and a remote host.\nThe syntax for scp is scp &lt;from-place&gt; &lt;to-place&gt;, and you’ll typically use scp on your local machine. For example, we’ll show how to transfer the file data.csv (you can find it here) :\n\nTo SCF while on your local machine\n# transfer to my home directory without renaming the file \nscp data.csv &lt;scf-username&gt;@dorothy.berkeley.edu:~/\n\n# transfer to the data/ subdirectory and rename the file\nscp data.csv &lt;scf-username&gt;@dorothy.berkeley.edu:~/data/new_name.csv\n\n# transfer to the dorothy-specific /tmp/ directory\nscp data.csv &lt;scf-username&gt;@dorothy.berkeley.edu:/tmp/\n\n\nFrom SCF while on your local machine\n# now &lt;from-place&gt; is a path in my home directory on the SCF \nscp &lt;scf-username&gt;@dorothy.berkeley.edu:~/data/new_name.csv ~/Desktop/data_scf.csv\nFor more information, see here. In particular, if you have a very large dataset to transfer, Globus is a better option than either scp or sftp."
  },
  {
    "objectID": "labs/lab3-debugging.html",
    "href": "labs/lab3-debugging.html",
    "title": "Lab 3: Debugging",
    "section": "",
    "text": "Today we’re going to explore the concept of debugging and some of the tooling that allows for debugging python code.\nIt’s widely recognized and accepted that any sizeable code base will have a non-trivial number of bugs (where does the term come from?). The main goal of testing is to make sure the main expected cases are behaving correctly.\nSometime you code doesn’t do what you expect or want it to do, and it’s not clear just by reading through it, what the problem is.\nIn interpreted languages (esp. those with interactive shells like python) one can sometimes run the code piece by piece and inspect the state of the variables.\nIf the code involves a loop or a function, a common practice is to judiciously place a few print statements that dump the state of some variables to the terminal so that you can spot the problem by tracing through it.\nWhen the code involves multiple functions and complex state, this strategy starts to break down. This is where you start rolling up your sleeves and invoking a debugger!\nDebugging can be a slow process, so you typically start a debugging session by deciding which line in your code you would like to start tracing the behavior from, and you place a breakpoint. Then you can have the debugger run the program up to that point and stop at it, allowing you to:\n1- inspect the current state of variables 2- step through the code line by line 3- step over or into functions as they are called 4- resume program execution"
  },
  {
    "objectID": "labs/lab3-debugging.html#debugging",
    "href": "labs/lab3-debugging.html#debugging",
    "title": "Lab 3: Debugging",
    "section": "",
    "text": "Today we’re going to explore the concept of debugging and some of the tooling that allows for debugging python code.\nIt’s widely recognized and accepted that any sizeable code base will have a non-trivial number of bugs (where does the term come from?). The main goal of testing is to make sure the main expected cases are behaving correctly.\nSometime you code doesn’t do what you expect or want it to do, and it’s not clear just by reading through it, what the problem is.\nIn interpreted languages (esp. those with interactive shells like python) one can sometimes run the code piece by piece and inspect the state of the variables.\nIf the code involves a loop or a function, a common practice is to judiciously place a few print statements that dump the state of some variables to the terminal so that you can spot the problem by tracing through it.\nWhen the code involves multiple functions and complex state, this strategy starts to break down. This is where you start rolling up your sleeves and invoking a debugger!\nDebugging can be a slow process, so you typically start a debugging session by deciding which line in your code you would like to start tracing the behavior from, and you place a breakpoint. Then you can have the debugger run the program up to that point and stop at it, allowing you to:\n1- inspect the current state of variables 2- step through the code line by line 3- step over or into functions as they are called 4- resume program execution"
  },
  {
    "objectID": "labs/lab3-debugging.html#advanced-debugging",
    "href": "labs/lab3-debugging.html#advanced-debugging",
    "title": "Lab 3: Debugging",
    "section": "advanced debugging",
    "text": "advanced debugging\nSome bugs are really tricky to catch. Those are typically the bugs that happen very rarely, and in unclear circumstances. In statistical computing and data analysis settings these might be conditions that happen in iterative algorithms sometimes, but not very often, and can be hard to reproduce.\nOne way to deal with these bugs is to start the debugging session with placing one or more conditional breakpoints. These are similar to regular breakpoints but are not going to cause the debugger to stop the execution of the program and hand you the controls unless a specific condition (that you specify) evaluates to true as the program is executing that particular line of code. You may use some conditions that are similar to what you would place in an assert statement (conditions that shouldn’t happen) or any other conditions that you think may be associated with the occurrence of the anomalous behavior your are debugging."
  },
  {
    "objectID": "labs/lab3-debugging.html#integrated-gui-debugger-with-vs-code",
    "href": "labs/lab3-debugging.html#integrated-gui-debugger-with-vs-code",
    "title": "Lab 3: Debugging",
    "section": "Integrated GUI debugger (with VS Code)",
    "text": "Integrated GUI debugger (with VS Code)\nToday we will experiment with the visual debugging tools integrated with IDEs. We will do that in VS Code (unless you have another IDE with debugger integration). We will load a piece of code, go through it to understand what it does, then try to discover the problem with it and fix it.\nHere’s a piece of code that implements the binary search algorithm to locate the first occurence of a number in a list of numbers:\n\nimport math\ndef binary_search(lst, T):\n    L = 0\n    R = len(lst) - 1\n    while L &lt; R:\n        m = math.floor((L + R) / 2)\n        if lst[m] &lt;= T:\n            L = m + 1\n        else:\n            R = m - 1\n    if lst[L] == T:\n        return L\n    return -1\n\nThere are a couple of things not quite right with this implementation, even though it will run and produce correct results for some cases.\nHere’s another piece of code implementing merge sort (also with some bugs in it):\n\ndef merge_sort(lst):\n    n = len(lst)\n    if n == 1:\n        return lst\n    return merge(merge_sort(lst[:n//2]), merge_sort(lst[n//2:]))\n\ndef merge(lst1, lst2):\n    merged = []\n    i, j = 0,0\n    while i &lt; len(lst1) and j &lt; len(lst2):\n        if i &lt; len(lst1) and j &lt; len(lst2) and lst1[i] &lt; lst2[j]:\n            merged.append(lst1[i])\n            i += 1\n        else:\n            merged.append(lst1[j])\n            j += 1\n    while i &lt; len(lst1):\n        merged.append(lst1[i])\n        i += 1\n    while j &lt; len(lst2):\n            merged.append(lst2[j])\n            j += 1\n    return merged\n        \nmerge_sort([3,1,5,1,6,3,9,12,8])\n\nYou can use this to practice stepping inside functions, and thinking about recursion.\nIncidentally, if you first sort, then find, you can get the quantile of a particular value within a collection (you’ll need to adjust the binary search a little to achieve this).\nAlternatively you could start by implementing (without using any existing functions) a function that inverst the order of the words in a string, and debug it until it works.\nHere’s a version of this function where I injected a couple of bugs, if you prefer to start from there:\n\ndef reverse_words(input):\n    working = list(input)\n    invert(working)\n    start = 0\n    for i, c in enumerate(working):\n        if c == ' ' and i != start:\n            invert(working, start, i)\n            start = i+1\n    return ''.join(working)\n\ndef invert(lst, start=None, end=None):\n    if None == start:\n        start = 0\n    if None == end:\n        end = len(lst)-1\n    \n    while start &lt; end:\n        tmp = lst[start]\n        lst[start] = lst[start]\n        lst[end] = tmp\n        start += 1\n        end -= 1\n\nreverse_words(\"These are my words.   I have spoken!\")\n\nNext time we will touch briefly on how to do debugging without an IDE with debugger integration."
  },
  {
    "objectID": "labs/py_vs_R.html",
    "href": "labs/py_vs_R.html",
    "title": "Lab 7: Python vs. R",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "labs/py_vs_R.html#instructions",
    "href": "labs/py_vs_R.html#instructions",
    "title": "Lab 7: Python vs. R",
    "section": "Instructions",
    "text": "Instructions\nPlease carefully read the full instructions before starting the assignment, as you will need to decide with your group how to organize and combine your efforts.\nYour group is expected to work on this for ~60 minutes\nIt’s OK if you don’t get through all of the questions in one hour (i.e., you can just submit what you have at the end of section), but it should be clear that you put some thought and effort into the questions your group worked on.\n\n\n\n\n\n\nTip\n\n\n\nDo your best to document your efforts and insights as you go.\n\n\n\nWe will separate into groups of (ideally) 2 or 3. I’ll try to make sure that each group has at least one person with some R experience.\nGroups should try to answer every question in the Main Questions section. Here are some options for working through these questions:\n\nOption A – Many minds, one task: The group works together, discussing and answering each question sequentially.\nOption B – Divide, consult, conquer: Individual group members work on different questions, consulting with each other as they go or as needed.\n\nAt the end of section (say, the last ~10-15 minutes), combine the solutions into one PDF. Each group member will then individually submit copies of this PDF on Gradescope.\n\nMake sure to include the name of each group member at the top of the PDF.\nHow you create the PDF is up to you but some options are:\n\n(recommended) Using Google Docs.\n\nSeparate documents, then combine: Each group member uses a separate Google Doc, then one person combines those documents into a final shared document.\nOne document: Everyone adds to a single document at the same time.\nYou can take screenshots of code / outputs that you run in Jupyter or RStudio (alternatively, the IPython or R consoles), or just copy paste code and outputs directly to the document.\nWhen your final shared document is ready, each group member will export it to PDF and make individual submission to Gradescope.\n\nCreate an R Markdown file with the code and render to PDF. This may be more of a headache to combine if each person is working separately.\n\n\n\n\nNo shows\nIf you miss lab this week, you can form a group of 2-3 students and submit this assignment on Gradescope by Monday October 17th at 10pm."
  },
  {
    "objectID": "labs/py_vs_R.html#questions",
    "href": "labs/py_vs_R.html#questions",
    "title": "Lab 7: Python vs. R",
    "section": "Questions",
    "text": "Questions\n\nMain Questions\n\n\n\n\n\n\nTip\n\n\n\nIdeally, you will make it through all of these, although if you run out of time that is okay.\n\n\n\nDo R functions behave like pass-by-value or pass-by-reference? In other words, if you pass in an object and modify it, does that affect the value of the object in the environment from which the function was called? Check this for a scalar, a list, and an R vector.\nCan R lists and vectors be modified in place, without copying the object?\nFor this the function .Internal(inspect) will be helpful. Here’s an example for a list.\n\n#| eval: false\nx &lt;- list(7, c('abc', 'def'), rnorm(5))\n.Internal(inspect(x))\n\n@5652776540f8 19 VECSXP g0c3 [REF(1)] (len=3, tl=0)\n@5652776b9740 14 REALSXP g0c1 [REF(3)] (len=1, tl=0) 7\n@56527580b0d8 16 STRSXP g0c2 [REF(1)] (len=2, tl=0)\n@5652776b97b0 09 CHARSXP g0c1 [REF(4),gp=0x60] [ASCII] [cached] \"abc\"\n@565275b60168 09 CHARSXP g0c1 [MARK,REF(14),gp=0x61] [ASCII] [cached] \"def\"\n@565275a97478 14 REALSXP g0c4 [REF(1)] (len=5, tl=0) -0.38248,-0.100364,-0.485605,1.15111,-0.111647\n\n#`5652776540f8` is the address of the overall list.\n#`5652776b9740` of the 1-element vector containing '7'.\n#`56527580b0d8` is the address of the character vector.\n#`565275a97478` is the address of the vector of random numbers.\n\nDoes R behave similarly to Python in terms of storing strings, as seen in PS4?\nIf you make a copy of an R vector does it use the same memory as the original vector and does changing an element of the original vector affect the copy of the vector?\nHow does variable scoping work in R - does it use lexical scoping and look for variables in the environment where a function was defined?\nCan you create a closure with embedded data, like we did in Python?\n\n\n\nAdditional Questions\n\n\n\n\n\n\nTip\n\n\n\nWork on these if you finish quickly/are curious\n\n\n\nConsider the relative efficiency of for loops versus vectorized calculations vs. apply for numeric vectors in R and see how it compares to the equivalent operations in python.\nCan you determine if the speed of looking up values in a named vector varies with the size of the dictionary (this will indicate if something like hashing is going on or if the lookup has to scan through all the elements)."
  },
  {
    "objectID": "labs/lab0-setup.html",
    "href": "labs/lab0-setup.html",
    "title": "Lab 0: Setup",
    "section": "",
    "text": "You will need to set up (or make sure you have access to) the following:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor or IDE of your choice (deeb recommends VS Code or sublime, but if you are already familiar with a specific editor, stick with it)\n\nAfter making sure you have access to all these 5 tools, it may be a good idea to go through some of the following tutorials on unix bash and unix commands. You can do this on your own or in the Lab section on Friday 8/25.\nAttending lab 0 is optional. If you successfully set up your environment to have all the listed tools, you don’t need to attend. You are welcome to attend to ask for help with the setup or to help your classmates with setting up their environments.\nYou are also welcome to come and ask for help on using any of these 5 tools/systems (esp. bash and unix commands this week), but priority will be given to environment setup questions.\nReach out to deeb @ deeb@berkeley.edu with any unresolved problems or if you discover something that needs to be changed with our howtos and instructions.\nDeeb’s unsolicited advice on languages and tools:\n\nWhichever editor you pick, make sure to spend some time every week learning a few keyboard shortcuts for it. The same goes for the bash shell (ctrl+a and ctrl+e are among my favorites) and for your OS of choice in general (and even your web browser!). Not only do keyboard shortcuts make you more efficient, but they dramatically reduce the cognitive load after a while, and so make your life less painful in the long run. They can be the difference between hating computers and loving them.\nProgramming languages come and go, but Unix is forever! Well, maybe not forever, but close enough. Invest more of your time in getting familiar with durable and proven paradigms. Different programming languages are suitable in different situations and change dramatically from one decade to the next, but the unix shell and commands are as pristine, long lived, and as widely applicable as you’ll find in the computing world. I have a much more mixed view of git, Python, R, and C++. Another example of a very durable computing paradigm is SQL, which we’ll get to in a few weeks."
  },
  {
    "objectID": "labs/lab0-setup.html#setting-up-your-environment-082523",
    "href": "labs/lab0-setup.html#setting-up-your-environment-082523",
    "title": "Lab 0: Setup",
    "section": "",
    "text": "You will need to set up (or make sure you have access to) the following:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor or IDE of your choice (deeb recommends VS Code or sublime, but if you are already familiar with a specific editor, stick with it)\n\nAfter making sure you have access to all these 5 tools, it may be a good idea to go through some of the following tutorials on unix bash and unix commands. You can do this on your own or in the Lab section on Friday 8/25.\nAttending lab 0 is optional. If you successfully set up your environment to have all the listed tools, you don’t need to attend. You are welcome to attend to ask for help with the setup or to help your classmates with setting up their environments.\nYou are also welcome to come and ask for help on using any of these 5 tools/systems (esp. bash and unix commands this week), but priority will be given to environment setup questions.\nReach out to deeb @ deeb@berkeley.edu with any unresolved problems or if you discover something that needs to be changed with our howtos and instructions.\nDeeb’s unsolicited advice on languages and tools:\n\nWhichever editor you pick, make sure to spend some time every week learning a few keyboard shortcuts for it. The same goes for the bash shell (ctrl+a and ctrl+e are among my favorites) and for your OS of choice in general (and even your web browser!). Not only do keyboard shortcuts make you more efficient, but they dramatically reduce the cognitive load after a while, and so make your life less painful in the long run. They can be the difference between hating computers and loving them.\nProgramming languages come and go, but Unix is forever! Well, maybe not forever, but close enough. Invest more of your time in getting familiar with durable and proven paradigms. Different programming languages are suitable in different situations and change dramatically from one decade to the next, but the unix shell and commands are as pristine, long lived, and as widely applicable as you’ll find in the computing world. I have a much more mixed view of git, Python, R, and C++. Another example of a very durable computing paradigm is SQL, which we’ll get to in a few weeks."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "This document provides guidance for submitting high-quality problem set (and project) solutions. This guidance is based on general good practices for scientific communication, reproducible research, and software development."
  },
  {
    "objectID": "rubric.html#general-presentation",
    "href": "rubric.html#general-presentation",
    "title": "Statistics 243 Fall 2023",
    "section": "General presentation",
    "text": "General presentation\n\nSimply presenting code or derivations is not sufficient.\nBriefly describe the overall goal or strategy before providing code/derivations.\nAs needed describe what the pieces of your code/derivation are doing to make it easier for a reader to follow the steps.\nKeep your output focused on illustrating what you did, without distracting from the flow of your solution by showing voluminous output. The output should illustrate and demonstrate, not overwhelm or obscure. If you need to show longer output, you can add it at the end as supplemental material.\nOutput should be produced by your code (i.e., from the code chunks running when the document is rendered), not by copying and pasting results into the document."
  },
  {
    "objectID": "rubric.html#coding-practice",
    "href": "rubric.html#coding-practice",
    "title": "Statistics 243 Fall 2023",
    "section": "Coding practice",
    "text": "Coding practice\n\nMinimize (or eliminate) use of global variables.\nBreak down work into core tasks and develop small, modular, self-contained functions (or class methods) to carry out those tasks.\nDon’t repeat code. As needed refactor code to create new functions (or class methods).\nFunctions and classes should be “weakly coupled”, interacting via their interfaces and not by having to know the internals of how they work.\nUse data structures appropriate for the computations that need to be done.\nDon’t hard code ‘magic’ numbers. Assign such numbers to variables with clear names, e.g., speed_of_light = 3e8.\nProvide reasonable default arguments to functions (or class methods) when possible.\nProvide tests (including unit tests) when requested (this is good general practice but we won’t require it in all cases).\nAvoid overly complicated syntax – try to use the clearest syntax you can to solve the problem.\nIn terms of speed, don’t worry about it too much so long as the code finishes real-world tasks in a reasonable amount of time. When optimizing, focus on the parts of the code that are the bottlenecks.\nUse functions already available in the language rather than recreating yourself."
  },
  {
    "objectID": "rubric.html#code-style",
    "href": "rubric.html#code-style",
    "title": "Statistics 243 Fall 2023",
    "section": "Code style",
    "text": "Code style\n\nFollow a consistent style. While you don’t have to follow Python’s PEP8 style guide exactly, please look at it and follow it generally.\nUse informative variable and function names and have a consistent naming style.\nUse whitespace (spaces, newlines) and parentheses to make the structure of the code easy to understand and the individual syntax pieces clear.\nUse consistent indentation to make the structure of the code easy to understand.\nProvide comments that give the goal of a given piece of code and why it does things, but don’t use comments to restate what the code does when it should be obvious from reading the code.\n\nProvide summaries for blocks of code.\nFor particularly complicated syntax, say what a given piece of code does."
  },
  {
    "objectID": "units/unit8-numbers.html",
    "href": "units/unit8-numbers.html",
    "title": "Numbers on a computer",
    "section": "",
    "text": "PDF\nReferences:\nA quick note that, as we’ve already seen, Python’s version of scientific notation is XeY, which means \\(X\\cdot10^{Y}\\).\nA second note is that the concepts developed here apply outside of Python, but we’ll illustrate the principles of computer numbers using Python. Python usually makes use of the double type (8 bytes) in C for the underlying representation of real-valued numbers in C variables, so what we’ll really be seeing is how such types behave in C on most modern machines. It’s actually a bit more complicated in that one can use real-valued numbers that use something other than 8 bytes in numpy by specifying a dtype.\nThe handling of integers is even more complicated. In numpy, the default is 8 byte integers, but other integer dtypes are available. And in Python itself, integers can be arbitrarily large."
  },
  {
    "objectID": "units/unit8-numbers.html#representing-real-numbers",
    "href": "units/unit8-numbers.html#representing-real-numbers",
    "title": "Numbers on a computer",
    "section": "Representing real numbers",
    "text": "Representing real numbers\n\nInitial exploration\nReals (also called floating points) are stored on the computer as an approximation, albeit a very precise approximation. As an example, if we represent the distance from the earth to the sun using a double, the error is around a millimeter. However, we need to be very careful if we’re trying to do a calculation that produces a very small (or very large number) and particularly when we want to see if numbers are equal to each other.\nIf you run the code here, the results may surprise you.\n\n0.3 - 0.2 == 0.1\n0.3\n0.2\n0.1 # Hmmm...\n\nnp.float64(0.3) - np.float64(0.2) == np.float64(0.1)\n\n0.75 - 0.5 == 0.25\n0.6 - 0.4 == 0.2\n## any ideas what is different about those two comparisons?\n\nNext, let’s consider the number of digits of accuracy we have for a variety of numbers. We’ll use format within a handy wrapper function, dg, defined earlier, to view as many digits as we want:\n\na = 0.3\nb = 0.2\ndg(a)\n\n0.29999999999999998890\n\ndg(b)\n\n0.20000000000000001110\n\ndg(a-b)\n\n0.09999999999999997780\n\ndg(0.1)\n\n0.10000000000000000555\n\ndg(1/3)\n\n0.33333333333333331483\n\n\nSo empirically, it looks like we’re accurate up to the 16th decimal place\nBut actually, the key is the number of digits, not decimal places.\n\ndg(1234.1234)\n\n1234.12339999999994688551\n\ndg(1234.123412341234)\n\n1234.12341234123391586763\n\n\nNotice that we can represent the result accurately only up to 16 significant digits. This suggests no need to show more than 16 significant digits and no need to print out any more when writing to a file (except that if the number is bigger than \\(10^{16}\\) then we need extra digits to correctly show the magnitude of the number if not using scientific notation). And of course, often we don’t need anywhere near that many.\nLet’s return to our comparison, 0.75-0.5 == 0.25.\n\ndg(0.75)\n\n0.75000000000000000000\n\ndg(0.50)\n\n0.50000000000000000000\n\n\nWhat’s different about the numbers 0.75 and 0.5 compared to 0.3, 0.2, 0.1?\n\n\nMachine epsilon\nMachine epsilon is the term used for indicating the (relative) accuracy of real numbers and it is defined as the smallest float, \\(x\\), such that \\(1+x\\ne1\\):\n\n1e-16 + 1.0\n\n1.0\n\nnp.array(1e-16) + np.array(1.0)\n\n1.0\n\n1e-15 + 1.0\n\n1.000000000000001\n\nnp.array(1e-15) + np.array(1.0)\n\n1.000000000000001\n\n2e-16 + 1.0\n\n1.0000000000000002\n\nnp.finfo(np.float64).eps\n\n2.220446049250313e-16\n\ndg(2e-16 + 1.0)\n\n1.00000000000000022204\n\n\n\n## What about in single precision, e.g. on a GPU?\nnp.finfo(np.float32).eps\n\n1.1920929e-07\n\n\n\n\nFloating point representation\nFloating point refers to the decimal point (or radix point since we’ll be working with base 2 and decimal relates to 10).\nTo proceed further we need to consider scientific notation, such as in writing Avogadro’s number as \\(+6.023\\times10^{23}\\). As a baseline for what is about to follow note that we can express a decimal number in the following expansion \\[6.037=6\\times10^{0}+0\\times10^{-1}+3\\times10^{-2}+7\\times10^{-3}\\] A real number on a computer is stored in what is basically scientific notation: \\[\\pm d_{0}.d_{1}d_{2}\\ldots d_{p}\\times b^{e}\\label{eq:floatRep}\\] where \\(b\\) is the base, \\(e\\) is an integer and \\(d_{i}\\in\\{0,\\ldots,b-1\\}\\). \\(e\\) is called the exponent and \\(d=d_{1}d_{2}\\ldots d_{p}\\) is called the mantissa.\nLet’s consider the choices that the computer pioneers needed to make in using this system to represent numbers on a computer using base 2 (\\(b=2\\)). First, we need to choose the number of bits to represent \\(e\\) so that we can represent sufficiently large and small numbers. Second we need to choose the number of bits, \\(p\\), to allocate to \\(d=d_{1}d_{2}\\ldots d_{p}\\), which determines the accuracy of any computer representation of a real.\nThe great thing about floating points is that we can represent numbers that range from incredibly small to very large while maintaining good precision. The floating point floats to adjust to the size of the number. Suppose we had only three digits to use and were in base 10. In floating point notation we can express \\(0.12\\times0.12=0.0144\\) as \\((1.20\\times10^{-1})\\times(1.20\\times10^{-1})=1.44\\times10^{-2}\\), but if we had fixed the decimal point, we’d have \\(0.120\\times0.120=0.014\\) and we’d have lost a digit of accuracy. (Furthermore, we wouldn’t be able to represent numbers bigger than \\(0.99\\).)\nMore specifically, the actual storage of a number on a computer these days is generally as a double in the form: \\[(-1)^{S}\\times1.d\\times2^{e-1023}=(-1)^{S}\\times1.d_{1}d_{2}\\ldots d_{52}\\times2^{e-1023}\\] where the computer uses base 2, \\(b=2\\), (so \\(d_{i}\\in\\{0,1\\}\\)) because base-2 arithmetic is faster than base-10 arithmetic. The leading 1 normalizes the number; i.e., ensures there is a unique representation for a given computer number. This avoids representing any number in multiple ways, e.g., either \\(1=1.0\\times2^{0}=0.1\\times2^{1}=0.01\\times2^{2}\\). For a double, we have 8 bytes=64 bits. Consider our representation as (\\(S,d,e\\)) where \\(S\\) is the sign. The leading 1 is the hidden bit and doesn’t need to be stored because it is always present. In general \\(e\\) is represented using 11 bits (\\(2^{11}=2048\\)), and the subtraction takes the place of having a sign bit for the exponent. (Note that in our discussion we’ll just think of \\(e\\) in terms of its base 10 representation, although it is of course represented in base 2.) This leaves \\(p=52 = 64-1-11\\) bits for \\(d\\).\nIn this code I force storage as a double by tacking on a decimal place, .0.\n\nbits(2.0**(-1)) # 1/2\n\n'0011111111100000000000000000000000000000000000000000000000000000'\n\nbits(2.0**0)  # 1\n\n'0011111111110000000000000000000000000000000000000000000000000000'\n\nbits(2.0**1)  # 2\n\n'0100000000000000000000000000000000000000000000000000000000000000'\n\nbits(2.0**1 + 2.0**0)  # 3\n\n'0100000000001000000000000000000000000000000000000000000000000000'\n\nbits(2.0**2)  # 4\n\n'0100000000010000000000000000000000000000000000000000000000000000'\n\nbits(-2)\n\n'1100000000000000000000000000000000000000000000000000000000000000'\n\n\nLet’s see that we can manually work out the bit-wise representation of 3.25:\n\nbits(3.25)\n\n'0100000000001010000000000000000000000000000000000000000000000000'\n\n\nSo that is \\(1.101 \\times 2^{1024-1023} = 1\\times 2^{1} + 1\\times 2^{0} + 1\\times 2^{-2}\\), where the 2nd through 12th bits are \\(10000000000\\), which code for \\(1\\times 2^{10}=1024\\).\nQuestion: Given a fixed number of bits for a number, what is the tradeoff between using bits for the \\(d\\) part vs. bits for the \\(e\\) part?\nLet’s consider what can be represented exactly:\n\ndg(.1)\n\n0.10000000000000000555\n\ndg(.5)\n\n0.50000000000000000000\n\ndg(.25)\n\n0.25000000000000000000\n\ndg(.26)\n\n0.26000000000000000888\n\ndg(1/32)\n\n0.03125000000000000000\n\ndg(1/33)\n\n0.03030303030303030387\n\n\nSo why is 0.5 stored exactly and 0.1 not stored exactly? By analogy, consider the difficulty with representing 1/3 in base 10."
  },
  {
    "objectID": "units/unit8-numbers.html#overflow-and-underflow",
    "href": "units/unit8-numbers.html#overflow-and-underflow",
    "title": "Numbers on a computer",
    "section": "Overflow and underflow",
    "text": "Overflow and underflow\nThe largest and smallest numbers we can represent are \\(2^{e_{\\max}}\\) and \\(2^{e_{\\min}}\\) where \\(e_{\\max}\\) and \\(e_{\\min}\\) are the smallest and largest possible values of the exponent. Let’s consider the exponent and what we can infer about the range of possible numbers. With 11 bits for \\(e\\), we can represent \\(\\pm2^{10}=\\pm1024\\) different exponent values (see np.finfo(np.float64).maxexp) (why is np.finfo(np.float64).minexp only -1022?). So the largest number we could represent is \\(2^{1024}\\). What is this in base 10?\n\nx = np.float64(10)\nx**308\n\n1e+308\n\nx**309\n\ninf\n\n&lt;string&gt;:1: RuntimeWarning: overflow encountered in double_scalars\n\nnp.log10(2.0**1024)\n\nError: OverflowError: (34, 'Numerical result out of range')\n\nnp.log10(2.0**1023)\n\n307.95368556425274\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\nWe could have been smarter about that calculation: \\(\\log_{10}2^{1024}=\\log_{2}2^{1024}/\\log_{2}10=1024/3.32\\approx308\\). The result is analogous for the smallest number, so we have that floating points can range between \\(1\\times10^{-308}\\) and \\(1\\times10^{308}\\), consistent with what numpy reports above. Producing something larger or smaller in magnitude than these values is called overflow and underflow respectively.\nLet’s see what happens when we underflow in numpy. Note that there is no warning.\n\nx**(-308)\n\n1e-308\n\nx**(-330)\n\n0.0\n\n\nSomething subtle happens for numbers like \\(10^{-309}\\) through \\(10^{-323}\\). They can actually be represented despite what I said above. Investigating that may be an extra credit problem on a problem set."
  },
  {
    "objectID": "units/unit8-numbers.html#integers-or-floats",
    "href": "units/unit8-numbers.html#integers-or-floats",
    "title": "Numbers on a computer",
    "section": "Integers or floats?",
    "text": "Integers or floats?\nValues stored as integers should overflow if they exceed the maximum integer.\nShould \\(2^{65}\\) overflow?\n\nnp.log2(np.iinfo(np.int64).max)\n\n63.0\n\nx = np.int64(2)\n# Yikes!\nx**64\n\n0\n\n\nPython’s int type doesn’t overflow.\n\n# Interesting:\nprint(2**64)\n\n18446744073709551616\n\nprint(2**100)\n\n1267650600228229401496703205376\n\n\nOf course, doubles won’t overflow until much larger values than 4- or 8-byte integers because we know they can be as big as \\(10^308\\).\n\nx = np.float64(2)\ndg(x**64, '.2f')\n\n18446744073709551616.00\n\ndg(x**100, '.2f')\n\n1267650600228229401496703205376.00\n\n\nHowever we need to think about what integer-valued numbers can and can’t be stored exactly in our base 2 representation of floating point numbers. It turns out that integer-valued numbers can be stored exactly as doubles when their absolute value is less than \\(2^{53}\\).\n\nChallenge: Why \\(2^{53}\\)? Write out what integers can be stored exactly in our base 2 representation of floating point numbers.\n\nYou can force storage as integers or doubles in a few ways.\n\nx = 3; type(x)\n\n&lt;class 'int'&gt;\n\nx = np.float64(x); type(x)\n\n&lt;class 'numpy.float64'&gt;\n\nx = 3.0; type(x)\n\n&lt;class 'float'&gt;\n\nx = np.float64(3); type(x)\n\n&lt;class 'numpy.float64'&gt;"
  },
  {
    "objectID": "units/unit8-numbers.html#precision",
    "href": "units/unit8-numbers.html#precision",
    "title": "Numbers on a computer",
    "section": "Precision",
    "text": "Precision\nConsider our representation as (S, d, e) where we have \\(p=52\\) bits for \\(d\\). Since we have \\(2^{52}\\approx0.5\\times10^{16}\\), we can represent about that many discrete values, which means we can accurately represent about 16 digits (in base 10). The result is that floats on a computer are actually discrete (we have a finite number of bits), and if we get a number that is in one of the gaps (there are uncountably many reals), it’s approximated by the nearest discrete value. The accuracy of our representation is to within 1/2 of the gap between the two discrete values bracketing the true number. Let’s consider the implications for accuracy in working with large and small numbers. By changing \\(e\\) we can change the magnitude of a number. So regardless of whether we have a very large or small number, we have about 16 digits of accuracy, since the absolute spacing depends on what value is represented by the least significant digit (the ulp, or unit in the last place) in \\(d\\), i.e., the \\(p=52\\)nd one, or in terms of base 10, the 16th digit. Let’s explore this:\n\n# large vs. small numbers\ndg(.1234123412341234)\n\n0.12341234123412339607\n\ndg(1234.1234123412341234) # not accurate to 16 decimal places \n\n1234.12341234123414324131\n\ndg(123412341234.123412341234) # only accurate to 4 places \n\n123412341234.12341308593750000000\n\ndg(1234123412341234.123412341234) # no places! \n\n1234123412341234.00000000000000000000\n\ndg(12341234123412341234) # fewer than no places! \n\n12341234123412340736.00000000000000000000\n\n\nWe can see the implications of this in the context of calculations:\n\ndg(1234567812345678.0 - 1234567812345677.0)\n\n1.00000000000000000000\n\ndg(12345678123456788888.0 - 12345678123456788887.0)\n\n0.00000000000000000000\n\ndg(12345678123456780000.0 - 12345678123456770000.0)\n\n10240.00000000000000000000\n\n\nThe spacing of possible computer numbers that have a magnitude of about 1 leads us to another definition of machine epsilon (an alternative, but essentially equivalent definition to that given previously. Machine epsilon tells us also about the relative spacing of numbers.\nFirst let’s consider numbers of magnitude one. The next biggest number we can represent after \\(1=1.00...00\\times2^{0}\\) is \\(1.000...01\\times2^{0}\\). The difference between those two numbers (i.e., the spacing) is \\[\n\\begin{aligned}\n\\epsilon & = &0.00...01 \\times 2^{0} \\\\\n& =& 0 \\times 2^{0} + 0 \\times 2^{-1} + \\cdots + 0\\times 2^{-51} + 1\\times2^{-52}\\\\\n& =& 1\\times2^{-52}\\\\\n& \\approx & 2.2\\times10^{-16}.\n\\end{aligned}\n\\]\nMachine epsilon gives the absolute spacing for numbers near 1 and the relative spacing for numbers with a different order of magnitude and therefore a different absolute magnitude of the error in representing a real. The relative spacing at \\(x\\) is \\[\\frac{(1+\\epsilon)x-x}{x}=\\epsilon\\] since the next largest number from \\(x\\) is given by \\((1+\\epsilon)x\\).\nSuppose \\(x=1\\times10^{6}\\). Then the absolute error in representing a number of this magnitude is \\(x\\epsilon\\approx2\\times10^{-10}\\). (Actually the error would be one-half of the spacing, but that’s a minor distinction.) We can see by looking at the numbers in decimal form, where we are accurate to the order \\(10^{-10}\\) but not \\(10^{-11}\\). This is equivalent to our discussion that we have only 16 digits of accuracy.\n\ndg(1000000.1)\n\n1000000.09999999997671693563\n\n\nLet’s see what arithmetic we can do exactly with integer-valued numbers stored as doubles and how that relates to the absolute spacing of numbers we’ve just seen:\n\n2.0**52\n\n4503599627370496.0\n\n2.0**52+1\n\n4503599627370497.0\n\n2.0**53\n\n9007199254740992.0\n\n2.0**53+1\n\n9007199254740992.0\n\n2.0**53+2\n\n9007199254740994.0\n\ndg(2.0**54)\n\n18014398509481984.00000000000000000000\n\ndg(2.0**54+2)\n\n18014398509481984.00000000000000000000\n\ndg(2.0**54+4)\n\n18014398509481988.00000000000000000000\n\nbits(2**53)\n\n'0100001101000000000000000000000000000000000000000000000000000000'\n\nbits(2**53+1)\n\n'0100001101000000000000000000000000000000000000000000000000000000'\n\nbits(2**53+2)\n\n'0100001101000000000000000000000000000000000000000000000000000001'\n\nbits(2**54)\n\n'0100001101010000000000000000000000000000000000000000000000000000'\n\nbits(2**54+2)\n\n'0100001101010000000000000000000000000000000000000000000000000000'\n\nbits(2**54+4)\n\n'0100001101010000000000000000000000000000000000000000000000000001'\n\n\nThe absolute spacing is \\(x\\epsilon\\), so we have spacings of \\(2^{52}\\times2^{-52}=1\\), \\(2^{53}\\times2^{-52}=2\\), \\(2^{54}\\times2^{-52}=4\\) for numbers of magnitude \\(2^{52}\\), \\(2^{53}\\), and \\(2^{54}\\), respectively.\nWith a bit more work (e.g., using Mathematica), one can demonstrate that doubles in Python in general are represented as the nearest number that can stored with the 64-bit structure we have discussed and that the spacing is as we have discussed. The results below show the spacing that results, in base 10, for numbers around 0.1. The numbers Python reports are spaced in increments of individual bits in the base 2 representation.\n\ndg(0.1234567812345678)\n\n0.12345678123456779729\n\ndg(0.12345678123456781)\n\n0.12345678123456781117\n\ndg(0.12345678123456782)\n\n0.12345678123456782505\n\ndg(0.12345678123456783)\n\n0.12345678123456782505\n\ndg(0.12345678123456784)\n\n0.12345678123456783892\n\nbits(0.1234567812345678)\n\n'0011111110111111100110101101110100010101110111110011010010000110'\n\nbits(0.12345678123456781)\n\n'0011111110111111100110101101110100010101110111110011010010000111'\n\nbits(0.12345678123456782)\n\n'0011111110111111100110101101110100010101110111110011010010001000'\n\nbits(0.12345678123456783)\n\n'0011111110111111100110101101110100010101110111110011010010001000'\n\nbits(0.12345678123456784)\n\n'0011111110111111100110101101110100010101110111110011010010001001'"
  },
  {
    "objectID": "units/unit8-numbers.html#working-with-higher-precision-numbers",
    "href": "units/unit8-numbers.html#working-with-higher-precision-numbers",
    "title": "Numbers on a computer",
    "section": "Working with higher precision numbers",
    "text": "Working with higher precision numbers\nAs we’ve seen, Python will automatically work with integers in arbitrary precision. (Note that R does not do this – R uses 4-byte integers, and for many calculations it’s best to use R’s numeric type because integers that aren’t really large can be expressed exactly.)\nFor higher precision floating point numbers you can make use of the gmpy2 package.\n\nimport gmpy2\ngmpy2.get_context().precision=200\ngmpy2.const_pi()\n\n## not sure why this shows ...00004\ngmpy2.mpfr(\".1234567812345678\")"
  },
  {
    "objectID": "units/unit8-numbers.html#computer-arithmetic-is-not-mathematical-arithmetic",
    "href": "units/unit8-numbers.html#computer-arithmetic-is-not-mathematical-arithmetic",
    "title": "Numbers on a computer",
    "section": "Computer arithmetic is not mathematical arithmetic!",
    "text": "Computer arithmetic is not mathematical arithmetic!\nAs mentioned for integers, computer number arithmetic is not closed, unlike real arithmetic. For example, if we multiply two computer floating points, we can overflow and not get back another computer floating point.\nAnother mathematical concept we should consider here is that computer arithmetic does not obey the associative and distributive laws, i.e., \\((a+b)+c\\) may not equal \\(a+(b+c)\\) on a computer and \\(a(b+c)\\) may not be the same as \\(ab+ac\\). Here’s an example with multiplication:\n\nval1 = 1/10; val2 = 0.31; val3 = 0.57\nres1 = val1*val2*val3\nres2 = val3*val2*val1\nres1 == res2\n\nFalse\n\ndg(res1)\n\n0.01766999999999999821\n\ndg(res2)\n\n0.01767000000000000168"
  },
  {
    "objectID": "units/unit8-numbers.html#calculating-with-integers-vs.-floating-points",
    "href": "units/unit8-numbers.html#calculating-with-integers-vs.-floating-points",
    "title": "Numbers on a computer",
    "section": "Calculating with integers vs. floating points",
    "text": "Calculating with integers vs. floating points\nIt’s important to note that operations with integers are fast and exact (but can easily overflow – albeit not with Python’s base int) while operations with floating points are slower and approximate. Because of this slowness, floating point operations (flops) dominate calculation intensity and are used as the metric for the amount of work being done - a multiplication (or division) combined with an addition (or subtraction) is one flop. We’ll talk a lot about flops in the unit on linear algebra."
  },
  {
    "objectID": "units/unit8-numbers.html#comparisons",
    "href": "units/unit8-numbers.html#comparisons",
    "title": "Numbers on a computer",
    "section": "Comparisons",
    "text": "Comparisons\nAs we saw, we should never test x == y unless:\n\nx and y are represented as integers,\nthey are integer-valued but stored as doubles that are small enough that they can be stored exactly), or\nthey are decimal numbers that have been created in the same way (e.g., 0.4-0.3 == 0.4-0.3 returns TRUE but 0.1 == 0.4-0.3 does not).\n\nSimilarly we should be careful about testing x == 0. And be careful of greater than/less than comparisons. For example, be careful of x[ x &lt; 0 ] = np.nan if what you are looking for is values that might be mathematically less than zero, rather than whatever is numerically less than zero.\n\n4 - 3 == 1\n\nTrue\n\n4.0 - 3.0 == 1.0\n\nTrue\n\n4.1 - 3.1 == 1.0\n\nFalse\n\n0.4-0.3 == 0.1\n\nFalse\n\n0.4-0.3 == 0.4-0.3\n\nTrue\n\n\nOne nice approach to checking for approximate equality is to make use of machine epsilon. If the relative spacing of two numbers is less than machine epsilon, then for our computer approximation, we say they are the same. Here’s an implementation that relies on the absolute spacing being \\(x\\epsilon\\) (see above).\n\nx = 12345678123456781000\ny = 12345678123456782000\n\ndef approx_equal(a,b):\n  if abs(a - b) &lt; np.finfo(np.float64).eps * abs(a + b):\n    print(\"approximately equal\")\n  else:\n    print (\"not equal\")\n\n\napprox_equal(a,b)\n\nnot equal\n\nx = 1234567812345678\ny = 1234567812345677\n\napprox_equal(a,b)   \n\nnot equal\n\n\nActually, we probably want to use a number slightly larger than machine epsilon to be safe.\nFinally, sometimes we encounter the use of an unusual integer as a symbol for missing values. E.g., a datafile might store missing values as -9999. Testing for this using == with floats should generally be ok:x [ x == -9999 ] = np.nan, because integers of this magnitude are stored exactly as floating point values. But to be really careful, you can read in as an integer or character type and do the assessment before converting to a float."
  },
  {
    "objectID": "units/unit8-numbers.html#calculations",
    "href": "units/unit8-numbers.html#calculations",
    "title": "Numbers on a computer",
    "section": "Calculations",
    "text": "Calculations\nGiven the limited precision of computer numbers, we need to be careful when in the following two situations.\n\nSubtracting large numbers that are nearly equal (or adding negative and positive numbers of the same magnitude). You won’t have the precision in the answer that you would like. How many decimal places of accuracy do we have here?\n\n# catastrophic cancellation w/ large numbers\ndg(123456781234.56 - 123456781234.00)\n\n0.55999755859375000000\n\n\nThe absolute error in the original numbers here is of the order \\(\\epsilon x=2.2\\times10^{-16}\\cdot1\\times10^{11}\\approx1\\times10^{-5}=.00001\\). While we might think that the result is close to the value 1 and should have error of about machine epsilon, the relevant absolute error is in the original numbers, so we actually only have about five significant digits in our result because we cancel out the other digits.\nThis is called catastrophic cancellation, because most of the digits that are left represent rounding error – many of the significant digits have cancelled with each other.\nHere’s catastrophic cancellation with small numbers. The right answer here is exactly 0.000000000000000000001234.\n\n# catastrophic cancellation w/ small numbers\nx = .000000000000123412341234\ny = .000000000000123412340000\n\n# So we know the right answer is .000000000000000000001234 exactly.  \n\ndg(x-y, '.35f')\n## [1] \"0.00000000000000000000123399999315140\"\n\n0.00000000000000000000123399999315140\n\n\nBut the result is accurate only to 8 places + 20 = 28 decimal places, as expected from a machine precision-based calculation, since the “1” is in the 13th position, after 12 zeroes (12+16=28). Ideally, we would have accuracy to 36 places (16 digits + the 20 zeroes), but we’ve lost 8 digits to catastrophic cancellation.\nIt’s best to do any subtraction on numbers that are not too large. For example, if we compute the sum of squares in a naive way, we can lose all of the information in the calculation because the information is in digits that are not computed or stored accurately: \\[s^{2}=\\sum x_{i}^{2}-n\\bar{x}^{2}\\]\n\n## No problem here:\nx = np.array([-1.0, 0.0, 1.0])\nn = len(x)\nnp.sum(x**2)-n*np.mean(x)**2 \n\n2.0\n\nnp.sum((x - np.mean(x))**2)\n\n## Adding/subtracting a constant shouldn't change the result:\n\n2.0\n\nx = x + 1e8\nnp.sum(x**2)-n*np.mean(x)**2  ## YIKES!\n\n0.0\n\nnp.sum((x - np.mean(x))**2)\n\n2.0\n\n\nA good principle to take away is to subtract off a number similar in magnitude to the values (in this case \\(\\bar{x}\\) is obviously ideal) and adjust your calculation accordingly. In general, you can sometimes rearrange your calculation to avoid catastrophic cancellation. Another example involves the quadratic formula for finding a root (p. 101 of Gentle).\nAdding or subtracting numbers that are very different in magnitude. The precision will be that of the large magnitude number, since we can only represent that number to a certain absolute accuracy, which is much less than the absolute accuracy of the smaller number:\n\ndg(123456781234.2)\n\n123456781234.19999694824218750000\n\ndg(123456781234.2 - 0.1)        # truth: 123456781234.1\n\n123456781234.09999084472656250000\n\ndg(123456781234.2 - 0.01)       # truth: 123456781234.19\n\n123456781234.19000244140625000000\n\ndg(123456781234.2 - 0.001)      # truth: 123456781234.199\n\n123456781234.19898986816406250000\n\ndg(123456781234.2 - 0.0001)     # truth: 123456781234.1999\n\n123456781234.19989013671875000000\n\ndg(123456781234.2 - 0.00001)    # truth: 123456781234.19999\n\n123456781234.19998168945312500000\n\ndg(123456781234.2 - 0.000001)   # truth: 123456781234.199999\n\n123456781234.19999694824218750000\n\n123456781234.2 - 0.000001 == 123456781234.2\n\nTrue\n\n\nThe larger number in the calculations above is of magnitude \\(10^{11}\\), so the absolute error in representing the larger number is around \\(1\\times10^{^{-5}}\\). Thus in the calculations above we can only expect the answers to be accurate to about \\(1\\times10^{-5}\\). In the last calculation above, the smaller number is smaller than \\(1\\times10^{-5}\\) and so doing the subtraction has had no effect. This is analogous to trying to do \\(1+1\\times10^{-16}\\) and seeing that the result is still 1.\nA work-around when we are adding numbers of very different magnitudes is to add a set of numbers in increasing order. However, if the numbers are all of similar magnitude, then by the time you add ones later in the summation, the partial sum will be much larger than the new term. A (second) work-around to that problem is to add the numbers in a tree-like fashion, so that each addition involves a summation of numbers of similar size.\n\nGiven the limited range of computer numbers, be careful when you are:\n\nMultiplying or dividing many numbers, particularly large or small ones. Never take the product of many large or small numbers as this can cause over- or under-flow. Rather compute on the log scale and only at the end of your computations should you exponentiate. E.g., \\[\\prod_{i}x_{i}/\\prod_{j}y_{j}=\\exp(\\sum_{i}\\log x_{i}-\\sum_{j}\\log y_{j})\\]\n\nLet’s consider some challenges that illustrate that last concern.\n\nChallenge: consider multiclass logistic regression, where you have quantities like this: \\[p_{j}=\\text{Prob}(y=j)=\\frac{\\exp(x\\beta_{j})}{\\sum_{k=1}^{K}\\exp(x\\beta_{k})}=\\frac{\\exp(z_{j})}{\\sum_{k=1}^{K}\\exp(z_{k})}\\] for \\(z_{k}=x\\beta_{k}\\). What will happen if the \\(z\\) values are very large in magnitude (either positive or negative)? How can we reexpress the equation so as to be able to do the calculation? Hint: think about multiplying by \\(\\frac{c}{c}\\) for a carefully chosen \\(c\\).\nSecond challenge: The same issue arises in the following calculation. Suppose I want to calculate a predictive density (e.g., in a model comparison in a Bayesian context): \\[\\begin{aligned}\nf(y^{*}|y,x) & = & \\int f(y^{*}|y,x,\\theta)\\pi(\\theta|y,x)d\\theta\\\\\n& \\approx & \\frac{1}{m}\\sum_{j=1}^{m}\\prod_{i=1}^{n}f(y_{i}^{*}|x,\\theta_{j})\\\\\n& = & \\frac{1}{m}\\sum_{j=1}^{m}\\exp\\sum_{i=1}^{n}\\log f(y_{i}^{*}|x,\\theta_{j})\\\\\n& \\equiv & \\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})\\end{aligned}\\] First, why do I use the log conditional predictive density? Second, let’s work with an estimate of the unconditional predictive density on the log scale, \\(\\log f(y^{*}|y,x)\\approx\\log\\frac{1}{m}\\sum_{j=1}^{m}\\exp(v_{j})\\). Now note that \\(e^{v_{j}}\\) may be quite small as \\(v_{j}\\) is the sum of log likelihoods. So what happens if we have terms something like \\(e^{-1000}\\)? So we can’t exponentiate each individual \\(v_{j}\\). This is what is known as the “log sum of exponentials” problem (and the solution as the “log-sum-exp trick”). Thoughts?\n\nNumerical issues come up frequently in linear algebra. For example, they come up in working with positive definite and semi-positive-definite matrices, such as covariance matrices. You can easily get negative numerical eigenvalues even if all the eigenvalues are positive or non-negative. Here’s an example where we use an squared exponential correlation as a function of time (or distance in 1-d), which is mathematically positive definite (i.e., all the eigenvalues are positive) but not numerically positive definite:\n\nxs = np.arange(100)\ndists = np.abs(xs[:, np.newaxis] - xs)\ncorr_matrix = np.exp(-(dists/10)**2)     # This is a p.d. matrix (mathematically).\nscipy.linalg.eigvals(corr_matrix)[80:99]  # But not numerically!\n\narray([-2.10937946e-16+9.49526594e-17j, -2.10937946e-16-9.49526594e-17j,\n       -1.77590164e-16+1.30160558e-16j, -1.77590164e-16-1.30160558e-16j,\n       -2.09305049e-16+0.00000000e+00j,  2.23869166e-16+3.21640840e-17j,\n        2.23869166e-16-3.21640840e-17j,  1.98271873e-16+9.08175827e-17j,\n        1.98271873e-16-9.08175827e-17j, -1.49116518e-16+0.00000000e+00j,\n       -1.23773149e-16+6.06467275e-17j, -1.23773149e-16-6.06467275e-17j,\n       -2.48071368e-18+1.51188749e-16j, -2.48071368e-18-1.51188749e-16j,\n       -4.08131705e-17+6.79669911e-17j, -4.08131705e-17-6.79669911e-17j,\n        1.27901871e-16+2.34695655e-17j,  1.27901871e-16-2.34695655e-17j,\n        5.23476667e-17+4.08642121e-17j])"
  },
  {
    "objectID": "units/unit8-numbers.html#final-note",
    "href": "units/unit8-numbers.html#final-note",
    "title": "Numbers on a computer",
    "section": "Final note",
    "text": "Final note\nHow the computer actually does arithmetic with the floating point representation in base 2 gets pretty complicated, and we won’t go into the details. These rules of thumb should be enough for our practical purposes. Monahan and the URL reference have many of the gory details."
  },
  {
    "objectID": "units/unit6-parallel.html",
    "href": "units/unit6-parallel.html",
    "title": "Parallel processing",
    "section": "",
    "text": "PDF\nReferences:\nThis unit will be fairly Linux-focused as most serious parallel computation is done on systems where some variant of Linux is running. The single-machine parallelization discussed here should work on Macs and Windows, but some of the details of what is happening under the hood are different for Windows.\nAs context, let’s consider some ways we might be able to achieve faster computation:"
  },
  {
    "objectID": "units/unit6-parallel.html#embarrassingly-parallel-ep-problems",
    "href": "units/unit6-parallel.html#embarrassingly-parallel-ep-problems",
    "title": "Parallel processing",
    "section": "Embarrassingly parallel (EP) problems",
    "text": "Embarrassingly parallel (EP) problems\nAn EP problem is one that can be solved by doing independent computations in separate processes without communication between the processes. You can get the answer by doing separate tasks and then collecting the results. Examples in statistics include\n\nsimulations with many independent replicates\nbootstrapping\nstratified analyses\nrandom forests\ncross-validation.\n\nThe standard setup is that we have the same code running on different datasets. (Note that different processes may need different random number streams, as we will discuss in the Simulation Unit.)\nTo do parallel processing in this context, you need to have control of multiple processes. Note that on a shared system with queueing/scheduling software set up, this will generally mean requesting access to a certain number of processors and then running your job in such a way that you use multiple processors.\nIn general, except for some modest overhead, an EP problem can ideally be solved with \\(1/p\\) the amount of time for the non-parallel implementation, given \\(p\\) CPUs. This gives us a speedup of \\(p\\), which is called linear speedup (basically anytime the speedup is of the form \\(kp\\) for some constant \\(k\\))."
  },
  {
    "objectID": "units/unit6-parallel.html#computer-architecture",
    "href": "units/unit6-parallel.html#computer-architecture",
    "title": "Parallel processing",
    "section": "Computer architecture",
    "text": "Computer architecture\nComputers now come with multiple processors for doing computation. Basically, physical constraints have made it harder to keep increasing the speed of individual processors, so the chip industry is now putting multiple processing units in a given computer and trying/hoping to rely on implementing computations in a way that takes advantage of the multiple processors.\nEveryday personal computers usually have more than one processor (more than one chip) and on a given processor, often have more than one core (multi-core). A multi-core processor has multiple processors on a single computer chip. On personal computers, all the processors and cores share the same memory.\nSupercomputers and computer clusters generally have tens, hundreds, or thousands of ‘nodes’, linked by a fast local network. Each node is essentially a computer with its own processor(s) and memory. Memory is local to each node (distributed memory). One basic principle is that communication between a processor and its memory is much faster than communication between processors with different memory. An example of a modern supercomputer is the Perlmutter supercomputer at Lawrence Berkeley National Lab, which has 3072 CPU-only nodes and 1792 nodes with GPUs, and a total of about 500,000 CPU cores. Each node has 512 GB of memory for a total of 2.3 PB of memory.\nFor our purposes, there is little practical distinction between multi-processor and multi-core situations. The main issue is whether processes share memory or not. In general, I won’t distinguish between cores and processors. We’ll just focus on the number of cores on given personal computer or a given node in a cluster."
  },
  {
    "objectID": "units/unit6-parallel.html#some-useful-terminology",
    "href": "units/unit6-parallel.html#some-useful-terminology",
    "title": "Parallel processing",
    "section": "Some useful terminology:",
    "text": "Some useful terminology:\n\ncores: We’ll use this term to mean the different processing units available on a single machine or node of a cluster. A given CPU will have multiple cores. (E.g, the AMD EPYC 7763 has 64 cores per CPU.)\nnodes: We’ll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.\nprocesses: instances of a program(s) executing on a machine; multiple processes may be executing at once. A given program may start up multiple processes at once. Ideally we have no more processes than cores on a node.\nworkers: the individual processes that are carrying out the (parallelized) computation. We’ll use worker and process interchangeably.\ntasks: individual units of computation; one or more tasks will be executed by a given process on a given core.\nthreads: multiple paths of execution within a single process; the operating system sees the threads as a single process, but one can think of them as ‘lightweight’ processes. Ideally when considering the processes and their threads, we would the same number of cores as we have processes and threads combined.\nforking: child processes are spawned that are identical to the parent, but with different process IDs and their own memory. In some cases if objects are not changed, the objects in the child process may refer back to the original objects in the original process, avoiding making copies.\nscheduler: a program that manages users’ jobs on a cluster. Slurm is a commonly used scheduler.\nload-balanced: when all the cores that are part of a computation are busy for the entire period of time the computation is running.\nsockets: some of R’s parallel functionality involves creating new R processes (e.g., starting processes via Rscript) and communicating with them via a communication technology called sockets."
  },
  {
    "objectID": "units/unit6-parallel.html#distributed-vs.-shared-memory",
    "href": "units/unit6-parallel.html#distributed-vs.-shared-memory",
    "title": "Parallel processing",
    "section": "Distributed vs. shared memory",
    "text": "Distributed vs. shared memory\nThere are two basic flavors of parallel processing (leaving aside GPUs): distributed memory and shared memory. With shared memory, multiple processors (which I’ll call cores for the rest of this document) share the same memory. With distributed memory, you have multiple nodes, each with their own memory. You can think of each node as a separate computer connected by a fast network.\n\nShared memory\nFor shared memory parallelism, each core is accessing the same memory so there is no need to pass information (in the form of messages) between different machines. However, unless one is using threading (or in some cases when one has processes created by forking), objects will still be copied when creating new processes to do the work in parallel. With threaded computations, multiple threads can access object(s) without making explicit copies. But in some programming contexts one needs to be careful that the threads on different cores doesn’t mistakenly overwrite places in memory that are used by other cores (this is generally not an issue in Python or R).\nWe’ll cover two types of shared memory parallelism approaches in this unit:\n\nthreaded linear algebra\nmulticore functionality\n\n\nThreading\nThreads are multiple paths of execution within a single process. If you are monitoring CPU usage (such as with top in Linux or Mac) and watching a job that is executing threaded code, you’ll see the process using more than 100% of CPU. When this occurs, the process is using multiple cores, although it appears as a single process rather than as multiple processes.\nNote that this is a different notion than a processor that is hyperthreaded. With hyperthreading a single core appears as two cores to the operating system.\n\n\n\nDistributed memory\nParallel programming for distributed memory parallelism requires passing messages between the different nodes. The standard protocol for doing this is MPI, of which there are various versions, including openMPI.\nWhile there are various Python and R that use MPI behind the scenes, we’ll only cover distributed memory parallelization via Dask, which doesn’t use MPI."
  },
  {
    "objectID": "units/unit6-parallel.html#gpus",
    "href": "units/unit6-parallel.html#gpus",
    "title": "Parallel processing",
    "section": "GPUs",
    "text": "GPUs\nGPUs (Graphics Processing Units) are processing units originally designed for rendering graphics on a computer quickly. This is done by having a large number of simple processing units for massively parallel calculation. The idea of general purpose GPU (GPGPU) computing is to exploit this capability for general computation.\nMost researchers don’t program for a GPU directly but rather use software (often machine learning software such as Tensorflow or PyTorch, or other software that automatically uses the GPU such as JAX) that has been programmed to take advantage of a GPU if one is available. The computations that run on the GPU are run in GPU kernels, which are functions that are launched on the GPU. The overall workflow runs on the CPU and then particular (usually computationally-intensive tasks for which parallelization is helpful) tasks are handed off to the GPU. GPUs and similar devices (e.g., TPUs) are often called “co-processors” in recognition of this style of workflow.\nThe memory on a GPU is distinct from main memory on the computer, so when writing code that will use the GPU, one generally wants to avoid having large amounts of data needing to be transferred back and forth between main (CPU) memory and GPU memory. Also, since there is overhead in launching a GPU kernel, one wants to avoid launching a lot of kernels relative to the amount of work being done by each kernel."
  },
  {
    "objectID": "units/unit6-parallel.html#some-other-approaches-to-parallel-processing",
    "href": "units/unit6-parallel.html#some-other-approaches-to-parallel-processing",
    "title": "Parallel processing",
    "section": "Some other approaches to parallel processing",
    "text": "Some other approaches to parallel processing\n\nSpark and Hadoop\nSpark and Hadoop are systems for implementing computations in a distributed memory environment, using the MapReduce approach, as discussed in Unit 7.\n\n\nCloud computing\nAmazon (Amazon Web Services’ EC2 service), Google (Google Cloud Platform’s Compute Engine service) and Microsoft (Azure) offer computing through the cloud. The basic idea is that they rent out their servers on a pay-as-you-go basis. You get access to a virtual machine that can run various versions of Linux or Microsoft Windows server and where you choose the number of processing cores you want. You configure the virtual machine with the applications, libraries, and data you need and then treat the virtual machine as if it were a physical machine that you log into as usual. You can also assemble multiple virtual machines into your own virtual cluster and use platforms such as databases and Spark on the cloud provider’s virtual machines."
  },
  {
    "objectID": "units/unit6-parallel.html#overview-key-idea",
    "href": "units/unit6-parallel.html#overview-key-idea",
    "title": "Parallel processing",
    "section": "Overview: Key idea",
    "text": "Overview: Key idea\nA key idea in Dask (and in R’s future package and the ray package for Python) involve abstracting (i.e, divorcing/separating) the specification of the parallelization in the code away from the computational resources that the code will be run on. We want to:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nAllow different users to run the same code on different computational resources (without touching the actual code that does the computation).\n\nThe computational resources on which the code is run is sometimes called the backend."
  },
  {
    "objectID": "units/unit6-parallel.html#overview-of-parallel-backends",
    "href": "units/unit6-parallel.html#overview-of-parallel-backends",
    "title": "Parallel processing",
    "section": "Overview of parallel backends",
    "text": "Overview of parallel backends\nOne sets the scheduler to control how parallelization is done, whether to run code on multiple machines, and how many cores on each machine to use.\nFor example to parallelize across multiple cores via separate Python processes, we’d do this.\n\nimport dask\ndask.config.set(scheduler='processes', num_workers = 4)  \n\nThis table shows the different types of schedulers.\n\n\n\n\n\n\n\n\n\nType\nDescription\nMulti-node\nCopies of objects made?\n\n\n\n\nsynchronous\nnot in parallel (serial)\nno\nno\n\n\nthreads (1)\nthreads within current Python session\nno\nno\n\n\nprocesses\nbackground Python sessions\nno\nyes\n\n\ndistributed (2)\nPython sessions across multiple nodes\nyes\nyes\n\n\n\nComments:\n\nNote that because of Python’s Global Interpreter Lock (GIL) (which prevents threading of Python code), many computations done in pure Python code won’t be parallelized using the ‘threads’ scheduler; however computations on numeric data in numpy arrays, Pandas dataframes and other C/C++/Cython-based code will parallelize.\nIt’s fine to use the distributed scheduler on one machine, such as your laptop. According to the Dask documentation, it has advantages over multiprocessing, including the diagnostic dashboard (see the tutorial) and better handling of when copies need to be made. In addition, one needs to use it for parallel map operations (see next section)."
  },
  {
    "objectID": "units/unit6-parallel.html#accessing-variables-and-workers-in-the-worker-processes",
    "href": "units/unit6-parallel.html#accessing-variables-and-workers-in-the-worker-processes",
    "title": "Parallel processing",
    "section": "Accessing variables and workers in the worker processes",
    "text": "Accessing variables and workers in the worker processes\nDask usually does a good job of identifying the packages and (global) variables you use in your parallelized code and importing those packages on the workers and copying necessary variables to the workers.\nHere’s a toy example that shows that the numpy package and a global variable n are automatically available in the worker processes without any action on our part.\nNote the use of the @delayed decorator to flag the function so that it operates in a lazy manner for use with Dask’s parallelization capabilities.\n\nimport dask\ndask.config.set(scheduler='processes', num_workers = 4, chunksize = 1)  \n\n&lt;dask.config.set object at 0x7fcd37ff9d10&gt;\n\nimport numpy as np\nn = 10\n\n@dask.delayed\ndef myfun(idx):\n   return np.random.normal(size = n)\n\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(myfun(i))  # add lazy task\n\ntasks\n\n[Delayed('myfun-fb6d5933-5a6e-455f-87c2-aee3ca9aadd3'), Delayed('myfun-62938a41-9c82-4e80-b9ee-09aba30dcbfb'), Delayed('myfun-91faca5a-ff4b-4eef-8736-acf786d1cbe3'), Delayed('myfun-e85f3b84-3ce1-477f-91a4-9069abb33748'), Delayed('myfun-1d342e40-ff5d-41a1-b10c-7f686d8714c0'), Delayed('myfun-73e39dc6-32fb-4cc5-9e8a-03e8ace1db71'), Delayed('myfun-e984bae0-6a28-480e-9165-86a980f5198e'), Delayed('myfun-b57c623a-d447-4d68-ae93-479e96cc5546')]\n\nresults = dask.compute(tasks)  # compute all in parallel\n\nIn other contexts (in various languages) you may need to explicitly copy objects to the workers (or load packages on the workers). This is sometimes called exporting variables.\nWe don’t have to flag the function in advanced with @delayed. We could also have directly called the decorator like this, which as the advantage of allowing us to run the function in the normal way if we simply invoke it.\n\ntasks.append(dask.delayed(myfun)(i))"
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-1-one-model-fit",
    "href": "units/unit6-parallel.html#scenario-1-one-model-fit",
    "title": "Parallel processing",
    "section": "Scenario 1: one model fit",
    "text": "Scenario 1: one model fit\nSpecific scenario: You need to fit a single statistical/machine learning model, such as a random forest or regression model, to your data.\nGeneral scenario: Parallelizing a single task.\n\nScenario 1A:\nA given method may have been written to use parallelization and you simply need to figure out how to invoke the method for it to use multiple cores.\nFor example the documentation for the RandomForestClassifier in scikit-learn’s ensemble module indicates it can use multiple cores – note the n_jobs argument (not shown here because the help info is very long).\n\nimport sklearn.ensemble\nhelp(sklearn.ensemble.RandomForestClassifier)\n\nYou’ll usually need to look for an argument with one of the words threads, processes, cores, cpus, jobs, etc. in the argument name.\n\n\nScenario 1B:\nIf a method does linear algebra computations on large matrices/vectors, Python (and R) can call out to parallelized linear algebra packages (the BLAS and LAPACK).\nThe BLAS is the library of basic linear algebra operations (written in Fortran or C). A fast BLAS can greatly speed up linear algebra in R relative to the default BLAS that comes with R. Some fast BLAS libraries are\n\nIntel’s MKL; available for educational use for free\nOpenBLAS; open source and free\nApple’s Accelerate framework BLAS (vecLib) for Macs; provided with your Mac\n\nIn addition to being fast when used on a single core, all of these BLAS libraries are threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores, provided your program is linked against the threaded BLAS installed on your machine and provided the shell environment variable OMP_NUM_THREADS is not set to one. (Macs make use of VECLIB_MAXIMUM_THREADS rather than OMP_NUM_THREADS and if MKL is being used, then one needs MKL_NUM_THREADS)\nFor parallel (threaded) linear algebra in Python, one can use an optimized BLAS with the numpy and (therefore) scipy packages, on Linux or using the Mac’s vecLib BLAS. Details will depend on how you install Python, numpy, and scipy. More details on figuring out what BLAS is being used and how to install a fast threaded BLAS on your own computer are here.\nDask and some other packages also provide threading, but pure Python code is not threaded.\nHere’s some code that illustrates the speed of using a threaded BLAS:\n\nimport numpy as np\nimport time\n\nx = np.random.normal(size = (6000, 6000))\n\nstart_time = time.time()\nx = np.dot(x.T, x) \nU = np.linalg.cholesky(x)\nelapsed_time = time.time() - start_time\nprint(\"Elapsed Time (8 threads):\", elapsed_time)\n\nWe’d need to restart Python after setting OMP_NUM_THREADS to 1 in order to compare the time when run in parallel vs. on a single core. That’s hard to demonstrate in this generated document, but when I ran it, it took 6.6 seconds, compared to 3 seconds using 8 cores.\nNote that for smaller linear algebra problems, we may not see any speed-up or even that the threaded calculation might be slower because of overhead in setting up the parallelization and because the parallelized linear algebra calculation involves more actual operations than when done serially.\n\n\nGPUs and linear algebra\nLinear algebra with large matrices is often a very good use case for GPUs.\nHere’s an example of using the GPU to multiply large matrices using PyTorch. We could do this similarly with Tensorflow or JAX. I’ve just inserted the timing from running this on an SCF machine with a powerful GPU.\n\nimport torch\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\ngpu = torch.device(\"cuda:0\")\n\nn = 10000\nx = torch.randn(n,n, device = gpu)\ny = torch.randn(n,n, device = gpu)\n\n## Time the matrix multiplication on GPU:\nstart.record()\nz = torch.matmul(x, y)\nend.record()\ntorch.cuda.synchronize()\nprint(start.elapsed_time(end))   # 120 ms.\n\n## Compare to CPU:\ncpu = torch.device(\"cpu\")\n\nx = torch.randn(n,n, device = cpu)\ny = torch.randn(n,n, device = cpu)\n\n## Time the matrix multiplication on CPU:\nstart.record()\nz = torch.matmul(x, y)\nend.record()\ntorch.cuda.synchronize()\nprint(start.elapsed_time(end))   # 18 sec.\n\nThe GPU calculation takes 100-200 milliseconds (ms), while the CPU calculation took 18 seconds using two CPU cores. That’s a speed-up of more than 100x!\nFor a careful comparison between GPU and CPU, we’d want to consider the effect of using 4-byte floating point numbers for the GPU calculation.\nWe’d also want to think about how many CPU cores should be used for the comparison."
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-2-three-different-prediction-methods-on-your-data",
    "href": "units/unit6-parallel.html#scenario-2-three-different-prediction-methods-on-your-data",
    "title": "Parallel processing",
    "section": "Scenario 2: three different prediction methods on your data",
    "text": "Scenario 2: three different prediction methods on your data\nSpecific scenario: You need to fit three different statistical/machine learning models to your data.\nGeneral scenario: Parallelizing a small number of tasks.\nWhat are some options?\n\nuse one core per model\nif you have rather more than three cores, apply the ideas here combined with Scenario 1 above - with access to a cluster and parallelized implementations of each model, you might use one node per model\n\nHere we’ll use the processes scheduler. In principal given this relies on numpy code, we could have also used the threads scheduler, but I’m not seeing effective parallelization when I try that.\n\nimport dask\nimport time\nimport numpy as np\n\ndef gen_and_mean(func, n, par1, par2):\n    return np.mean(func(par1, par2, size = n))\n\ndask.config.set(scheduler='processes', num_workers = 3, chunksize = 1)  \n\n&lt;dask.config.set object at 0x7fcd37ffe810&gt;\n\nn = 100000000\nt0 = time.time()\ntasks = []\ntasks.append(dask.delayed(gen_and_mean)(np.random.normal, n, 0, 1))\ntasks.append(dask.delayed(gen_and_mean)(np.random.gamma, n, 1, 1))\ntasks.append(dask.delayed(gen_and_mean)(np.random.uniform, n, 0, 1))\nresults = dask.compute(tasks)\nprint(time.time() - t0) \n\n3.6668434143066406\n\nt0 = time.time()\np = gen_and_mean(np.random.normal, n, 0, 1)\nq = gen_and_mean(np.random.gamma, n, 1, 1)\ns = gen_and_mean(np.random.uniform, n, 0, 1)\nprint(time.time() - t0) \n\n5.191020488739014\n\n\nQuestion: Why might this not have shown a perfect three-fold speedup?\nYou could also have used tools like a parallel map here as well, as we’ll discuss in the next scenario.\n\nLazy evaluation, synchronicity, and blocking\nIf we look at the delayed objects, we see that each one is a representation of the computation that needs to be done and that execution happens lazily. Also note that dask.compute executes synchronously, which means the main process waits until the dask.compute call is complete before allowing other commands to be run. This synchronous evaluation is also called a blocking call because execution of the task in the worker processes blocks the main process. In contrast, if control returns to the user before the worker processes are done, that would be asynchronous evaluation (aka, a non-blocking call).\nNote: the use of chunksize = 1 forces Dask to immediately start one task on each worker. Without that argument, by default it groups tasks so as to reduce the overhead of starting each task individually, but when we have few tasks, that prevents effective parallelization. We’ll discuss this in much more detail in Scenario 4."
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-3-10-fold-cv-and-10-or-fewer-cores",
    "href": "units/unit6-parallel.html#scenario-3-10-fold-cv-and-10-or-fewer-cores",
    "title": "Parallel processing",
    "section": "Scenario 3: 10-fold CV and 10 or fewer cores",
    "text": "Scenario 3: 10-fold CV and 10 or fewer cores\nSpecific scenario: You are running a prediction method on 10 cross-validation folds.\nGeneral scenario: Parallelizing tasks via a parallel map.\nThis illustrates the idea of running some number of tasks using the cores available on a single machine.\nHere I’ll illustrate using a parallel map, using this simulated dataset and basic use of RandomForestRegressor().\nFirst, let’s set up our fit function and simulate some data.\nIn this case our fit function uses global variables. The reason for this is that we’ll use Dask’s map function, which allows us to pass only a single argument. We could bundle the input data with the fold_idx value and pass as a larger object, but here we’ll stick with the simplicity of global variables.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\n\ndef cv_fit(fold_idx):\n    train_idx = folds != fold_idx\n    test_idx = folds == fold_idx\n    X_train = X.iloc[train_idx]\n    X_test = X.iloc[test_idx]\n    Y_train = Y[train_idx]\n    model = RandomForestRegressor()\n    model.fit(X_train, Y_train)\n    predictions = model.predict(X_test)\n    return predictions\n\n\nnp.random.seed(1)\n\n# Generate data\nn = 1000\np = 50\nX = pd.DataFrame(np.random.normal(size = (n, p)),\\\n                 columns=[f\"X{i}\" for i in range(1, p + 1)])\nY = X['X1'] + np.sqrt(np.abs(X['X2'] * X['X3'])) +\\\n    X['X2'] - X['X3'] + np.random.normal(size = n)\n\nn_folds = 10\nseq = np.arange(n_folds)\nfolds = np.random.permutation(np.repeat(seq, 100))\n\nTo do a parallel map, we need to use the distributed scheduler, but it’s fine to do that with multiple cores on a single machine (such as a laptop).\n\nn_cores = 2\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\ntasks = c.map(cv_fit, range(n_folds))\nresults = c.gather(tasks)\n# We'd need to sort the results appropriately to align them with the observations.\n\nNow suppose you have 4 cores (and therefore won’t have an equal number of tasks per core with the 10 tasks). The approach in the next scenario should work better."
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-4-parallelizing-over-prediction-methods",
    "href": "units/unit6-parallel.html#scenario-4-parallelizing-over-prediction-methods",
    "title": "Parallel processing",
    "section": "Scenario 4: parallelizing over prediction methods",
    "text": "Scenario 4: parallelizing over prediction methods\nScenario: parallelizing over prediction methods or other cases where execution time varies.\nIf you need to parallelize over prediction methods or in other contexts in which the computation time for the different tasks varies widely, you want to avoid having the parallelization group the tasks into batches in advance, because some cores may finish a lot more quickly than others. Starting the tasks one by one (not in batches) is called dynamic allocation.\nIn contrast, if the computation time is about the same for the different tasks (or if you have so many tasks that the effect of averaging helps with load-balancing) then you want to group the tasks into batches. This is called static allocation or prescheduling. This avoids the extra overhead (~1 millisecond per task) of scheduling many tasks.\n\nDynamic allocation\nWith Dask’s distributed scheduler, Dask starts up each delayed evaluation separately (i.e., dynamic allocation).\nWe’ll set up an artificial example with four slow tasks and 12 fast tasks and see the speed of running with the default of dynamic allocation under Dask’s distributed scheduler. Then in the next section, we’ll compare to the worst-case scenario with all four slow tasks in a single batch.\n\nimport scipy.special\n\nn_cores = 4\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\n## 4 slow tasks and 12 fast ones.\nn = np.repeat([10**7, 10**5, 10**5, 10**5], 4)\n\ndef fun(i):\n    print(f\"Working on {i}.\")\n    return np.mean(scipy.special.gammaln(np.exp(np.random.normal(size = n[i]))))\n    \n\nt0 = time.time()\nout = fun(1)\n\nWorking on 1.\n\nprint(time.time() - t0)\n\n0.5563864707946777\n\nt0 = time.time()\nout = fun(5)\n\nWorking on 5.\n\nprint(time.time() - t0)\n\n0.01022791862487793\n\nt0 = time.time()\ntasks = c.map(fun, range(len(n)))\nresults = c.gather(tasks)  \nprint(time.time() - t0)  # 0.8 sec. \n\n0.8854873180389404\n\ncluster.close()\n\nNote that with relatively few tasks per core here, we could have gotten unlucky if the tasks were in a random order and multiple slow tasks happen to be done by a single worker.\n\n\nStatic allocation\nNext, note that by default the ‘processes’ scheduler sets up tasks in batches, with a default chunksize of 6. In this case that means that the first 4 (slow) tasks are all allocated to a single worker.\n\ndask.config.set(scheduler='processes', num_workers = 4)\n\n&lt;dask.config.set object at 0x7fcd2391ac50&gt;\n\ntasks = []\np = len(n)\nfor i in range(p):\n    tasks.append(dask.delayed(fun)(i))  # add lazy task\n\nt0 = time.time()\nresults = dask.compute(tasks)  # compute all in parallel\nprint(time.time() - t0)   # 2.6 sec.\n\n1.7515089511871338\n\n\nTo force dynamic allocation, we can set chunksize = 1 (as was shown in our original example of using the processes scheduler).\n\ndask.config.set(scheduler='processes', num_workers = 4, chunksize = 1)\n\ntasks = []\np = len(n)\nfor i in range(p):\n    tasks.append(dask.delayed(fun)(i))  # add lazy task\n\nresults = dask.compute(tasks)  # compute all in parallel\n\n\n\nChoosing static vs. dynamic allocation in Dask\nWith the distributed scheduler, Dask starts up each delayed evaluation separately (i.e., dynamic allocation). And even with a distributed map() it doesn’t appear possible to ask that the tasks be broken up into batches. Therefore if you want static allocation, you could use the processes scheduler if using a single machine, or if you need to use the distributed schduler you could break up the tasks into batches manually.\nWith the processes scheduler, static allocation is the default, with a default chunksize of 6 tasks per batch. You can force dynamic allocation by setting chunksize = 1.\n(Note that in R, static allocation is the default when using the future package.)"
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-5-10-fold-cv-across-multiple-methods-with-many-more-than-10-cores",
    "href": "units/unit6-parallel.html#scenario-5-10-fold-cv-across-multiple-methods-with-many-more-than-10-cores",
    "title": "Parallel processing",
    "section": "Scenario 5: 10-fold CV across multiple methods with many more than 10 cores",
    "text": "Scenario 5: 10-fold CV across multiple methods with many more than 10 cores\nSpecific scenario: You are running an ensemble prediction method such as SuperLearner or Bayesian model averaging on 10 cross-validation folds, with many statistical/machine learning methods.\nGeneral scenario: parallelizing nested tasks or a large number of tasks, ideally across multiple machines.\nHere you want to take advantage of all the cores you have available, so you can’t just parallelize over folds.\nFirst we’ll discuss how to deal with the nestedness of the problem and then we’ll talk about how to make use of many cores across multiple nodes to parallelize over a large number of tasks.\n\nScenario 5A: nested parallelization\nOne can always flatten the looping, either in a for loop or in similar ways when using apply-style statements.\n\n## original code: multiple loops \nfor fold in range(n):\n  for method in range(M):\n     ### code here \n  \n## revised code: flatten the loops \nfor idx in range(n*M): \n    fold = idx // M \n    method = idx % M  \n    print(idx, fold, method)### code here \n\nRather than flattening the loops at the loop level (which you’d need to do to use map), one could just generate a list of delayed tasks within the nested loops.\n\nfor fold in range(n):\n  for method in range(M):\n     tasks.append(dask.delayed(myfun)(fold,method))\n\nThe future package in R has some nice functionality for easily parallelizing with nested loops.\n\n\nScenario 5B: Parallelizing across multiple nodes\nIf you have access to multiple machines networked together, including a Linux cluster, you can use Dask to start workers across multiple nodes (either in a nested parallelization situation with many total tasks or just when you have lots of unnested tasks to parallelize over). Here we’ll just illustrate how to use multiple nodes, but if you had a nested parallelization case you can combine the ideas just above with the use of multiple nodes.\nSimply start Python as you usually would. Then the following code will parallelize on workers across the machines specified.\n\nfrom dask.distributed import Client, SSHCluster\n# First host is the scheduler.\ncluster = SSHCluster(\n    [\"gandalf.berkeley.edu\", \"radagast.berkeley.edu\", \"radagast.berkeley.edu\",\n    \"arwen.berkeley.edu\", \"arwen.berkeley.edu\"]\n)\nc = Client(cluster)\n\n## On the SCF, Savio and other clusters using the SLURM scheduler,\n## you can figure out the machine names like this, repeating the\n## first machine for the scheduler:\n## \n## machines = subprocess.check_output(\"srun hostname\", shell = True,\n##            universal_newlines = True).strip().split('\\n')\n## machines = [machines[0]] + machines\n\ndef fun(i, n=10**6):\n    return np.mean(np.random.normal(size = n))\n\nn_tasks = 120\n\ntasks = c.map(fun, range(n_tasks))\nresults = c.gather(tasks)\n\n## And just to check we are actually using the various machines:\nimport subprocess\n\nc.gather(c.map(lambda x: subprocess.check_output(\"hostname\", shell = True), \\\n               range(4)))\n\ncluster.close()"
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-6-stratified-analysis-on-a-very-large-dataset",
    "href": "units/unit6-parallel.html#scenario-6-stratified-analysis-on-a-very-large-dataset",
    "title": "Parallel processing",
    "section": "Scenario 6: Stratified analysis on a very large dataset",
    "text": "Scenario 6: Stratified analysis on a very large dataset\nSpecific scenario: You are doing stratified analysis on a very large dataset and want to avoid unnecessary copies.\nGeneral scenario: Avoiding copies when working with large data in parallel.\nIn many parallelization tools, if you try to parallelize this case on a single node, you end up making copies of the original dataset, which both takes up time and eats up memory.\nHere when we use the processes scheduler, we make copies for each task.\n\ndef do_analysis(i,x):\n    # A fake \"analysis\", identical for each task.\n    print(id(x))   # Check the number of copies.\n    return np.mean(x)\n\nn_cores = 4\n\nx = np.random.normal(size = 5*10**7)   # our big \"dataset\"\n\ndask.config.set(scheduler='processes', num_workers = n_cores, chunksize = 1)\n\n&lt;dask.config.set object at 0x7fcd2310e810&gt;\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\nprint(time.time() - t0)\n\n6.334888458251953\n\n\nA better approach is to use the distributed scheduler (which is fine to use on a single machine or multiple machines), which makes one copy per worker instead of one per task, provided you apply delayed() to the global data object.\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = n_cores)\nc = Client(cluster)\n\nx = dask.delayed(x)\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\n\n/system/linux/mambaforge-3.11/lib/python3.11/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 47.69 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n\nprint(time.time() - t0)\n\n1.1176700592041016\n\ncluster.close()\n\nThat seems to work, though Dask suggests sending the data to the workers in advance. I’m not sure of the distinction between what it is recommending and use of dask.delayed(x).\nFurthermore, the id of x seems to be the same for all the tasks, even though we have four workers. The documentation indicates one copy per worker, but perhaps that’s not actually the case.\nEven better would be to use the threads scheduler, in which case all workers can access the same data objects with no copying (but of course we cannot modify the data in that case without potentially causing problems for the other tasks). Without the copying, this is really fast.\n\ndask.config.set(scheduler='threads', num_workers = n_cores)\n\n&lt;dask.config.set object at 0x7fcd2311e810&gt;\n\ntasks = []\np = 8\nfor i in range(p):\n    tasks.append(dask.delayed(do_analysis)(i,x))\n\nt0 = time.time()\nresults = dask.compute(tasks)\n\n140519032960464\n140519032960464\n140519032960464\n140519032960464\n140519032960464\n140519032960464\n140519032960464\n140519032960464\n\nprint(time.time() - t0)\n\n0.06301546096801758"
  },
  {
    "objectID": "units/unit6-parallel.html#scenario-7-simulation-study-with-n1000-replicates-parallel-random-number-generation",
    "href": "units/unit6-parallel.html#scenario-7-simulation-study-with-n1000-replicates-parallel-random-number-generation",
    "title": "Parallel processing",
    "section": "Scenario 7: Simulation study with n=1000 replicates: parallel random number generation",
    "text": "Scenario 7: Simulation study with n=1000 replicates: parallel random number generation\nWe’ll probably skip this for now and come back to it when we discuss random number generation in the Simulation Unit.\nThe key thing when thinking about random numbers in a parallel context is that you want to avoid having the same ‘random’ numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.\nSpecific scenario: You are running a simulation study with n=1000 replicates.\nGeneral scenario: Safely handling random number generation in parallel.\nEach replicate involves fitting two statistical/machine learning methods.\nHere, unless you really have access to multiple hundreds of cores, you might as well just parallelize across replicates.\nHowever, you need to think about random number generation. One option is to set the random number seed to different values for each replicate. One danger in setting the seed like that is that the random numbers in the different replicate could overlap somewhat. This is probably somewhat unlikely if you are not generating a huge number of random numbers, but it’s unclear how safe it is.\nWe can use functionality with numpy’s PCG64 or MT19937 generators to be completely safe in our parallel random number generation. Each provide a jumped() function that moves the RNG ahead as if one had generated a very large number of random variables (\\(2^{128}\\)) for the Mersenne Twister and nearly that for the PCG64).\nHere’s how we can set up the use of the PCG64 generator:\n\nbitGen = np.random.PCG64(1)\nrng = np.random.Generator(bitGen)\nrng.random(size = 3)\n\narray([0.51182162, 0.9504637 , 0.14415961])\n\n\nNow let’s see how to jump forward. And then verify that jumping forward two increments is the same as making two separate jumps.\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\narray([ 1.23362391,  0.42793616, -1.90447637])\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(2)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\narray([-0.31752967,  1.22269493,  0.28254622])\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\narray([-0.31752967,  1.22269493,  0.28254622])\n\n\nWe can also use jumped() with the Mersenne Twister.\n\nbitGen = np.random.MT19937(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\narray([ 0.12667829, -2.1031878 , -1.53950735])\n\n\nSo the strategy to parallelize across tasks (or potentially workers if random number generation is done sequentially for tasks done by a single worker) is to give each task the same seed and use jumped(i) where i indexes the tasks (or workers).\n\ndef myrandomfun(i):\n    bitGen = np.random.PCG(1)\n    bitGen = bitGen.jumped(i)\n    # insert code with random number generation\n\nOne caution is that it appears that the period for PCG64 is \\(2^{128}\\) and that jumped(1) jumps forward by nearly that many random numbers. That seems quite strange, and I don’t understand it.\nAlternatively as recommended in the docs:\n\nn_tasks = 10\nsg = np.random.SeedSequence(1)\nrngs = [Generator(PCG64(s)) for s in sg.spawn(n_tasks)]\n## Now pass elements of rng into your function that is being computed in parallel\n\ndef myrandomfun(rng):\n    # insert code with random number generation, such as:\n    z = rng.normal(size = 5)\n\nIn R, the rlecuyer package deals with this. The L’Ecuyer algorithm has a period of \\(2^{191}\\), which it divides into subsequences of length \\(2^{127}\\)."
  },
  {
    "objectID": "units/unit6-parallel.html#avoiding-repeated-calculations-by-calling-compute-once",
    "href": "units/unit6-parallel.html#avoiding-repeated-calculations-by-calling-compute-once",
    "title": "Parallel processing",
    "section": "Avoiding repeated calculations by calling compute once",
    "text": "Avoiding repeated calculations by calling compute once\nAs far as I can tell, Dask avoids keeping all the pieces of a distributed object or computation in memory. However, in many cases this can mean repeating computations or re-reading data if you need to do multiple operations on a dataset.\nFor example, if you are create a Dask distributed dataset from data on disk, I think this means that every distinct set of computations (each computational graph) will involve reading the data from disk again.\nOne implication is that if you can include all computations on a large dataset within a single computational graph (i.e., a call to compute) that may be much more efficient than making separate calls.\nHere’s an example with Dask dataframe on some air traffic delay data, where we make sure to do all our computations as part of one graph:\n\nimport dask\ndask.config.set(scheduler='processes', num_workers = 6)  \nimport dask.dataframe as ddf\nair = ddf.read_csv('/scratch/users/paciorek/243/AirlineData/csvs/*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1',   # (unexpected) latin1 value(s) 2001 file TailNum field\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object'})\n# specify dtypes so Pandas doesn't complain about column type heterogeneity\n\nimport time\nt0 = time.time()\nair.DepDelay.min().compute()   # about 200 seconds.\nprint(time.time()-t0)\nt0 = time.time()\nair.DepDelay.max().compute()   # about 200 seconds.\nprint(time.time()-t0)\nt0 = time.time()\n(mn, mx) = dask.compute(air.DepDelay.max(), air.DepDelay.min())  # about 200 seconds\nprint(time.time()-t0)"
  },
  {
    "objectID": "units/unit6-parallel.html#setting-the-number-of-threads-cores-used-in-threaded-code-including-parallel-linear-algebra-in-python-and-r",
    "href": "units/unit6-parallel.html#setting-the-number-of-threads-cores-used-in-threaded-code-including-parallel-linear-algebra-in-python-and-r",
    "title": "Parallel processing",
    "section": "Setting the number of threads (cores used) in threaded code (including parallel linear algebra in Python and R)",
    "text": "Setting the number of threads (cores used) in threaded code (including parallel linear algebra in Python and R)\nIn general, threaded code will detect the number of cores available on a machine and make use of them. However, you can also explicitly control the number of threads available to a process.\nFor most threaded code (that based on the openMP protocol), the number of threads can be set by setting the OMP_NUM_THREADS environment variable (VECLIB_MAXIMUM_THREADS on a Mac). E.g., to set it for four threads in the bash shell:\n\nexport OMP_NUM_THREADS=4\n\nDo this before starting your R or Python session or before running your compiled executable.\nAlternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g., here with R:\n\nOMP_NUM_THREADS=4 R CMD BATCH --no-save job.R job.out\n\n\nSpeed and threaded BLAS\nIn many cases, using multiple threads for linear algebra operations will outperform using a single thread, but there is no guarantee that this will be the case, in particular for operations with small matrices and vectors. You can compare speeds by setting OMP_NUM_THREADS to different values. In cases where threaded linear algebra is slower than unthreaded, you would want to set OMP_NUM_THREADS to 1.\nMore generally, if you are using the parallel tools in Section 4 to simultaneously carry out many independent calculations (tasks), it is likely to be more effective to use the fixed number of cores available on your machine so as to split up the tasks, one per core, without taking advantage of the threaded BLAS (i.e., restricting each process to a single thread)."
  },
  {
    "objectID": "units/unit6-parallel.html#overview-futures-and-the-r-future-package",
    "href": "units/unit6-parallel.html#overview-futures-and-the-r-future-package",
    "title": "Parallel processing",
    "section": "Overview: Futures and the R future package",
    "text": "Overview: Futures and the R future package\nWhat is a future? It’s basically a flag used to tag a given operation such that when and where that operation is carried out is controlled at a higher level. If there are multiple operations tagged then this allows for parallelization across those operations.\nAccording to Henrik Bengtsson (the future package developer) and those who developed the concept:\n\na future is an abstraction for a value that will be available later\nthe value is the result of an evaluated expression\nthe state of a future is either unresolved or resolved\n\nWhy use futures? The future package allows one to write one’s computational code without hard-coding whether or how parallelization would be done. Instead one writes the code in a generic way and at the beginning of one’s code sets the ‘plan’ for how the parallel computation should be done given the computational resources available. Simply changing the ‘plan’ changes how parallelization is done for any given run of the code.\nMore concisely, the key ideas are:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation)."
  },
  {
    "objectID": "units/unit6-parallel.html#overview-of-parallel-backends-1",
    "href": "units/unit6-parallel.html#overview-of-parallel-backends-1",
    "title": "Parallel processing",
    "section": "Overview of parallel backends",
    "text": "Overview of parallel backends\nOne uses plan() to control how parallelization is done, including what machine(s) to use and how many cores on each machine to use.\nFor example,\n\nplan(multiprocess)\n## spreads work across multiple cores\n# alternatively, one can also control number of workers\nplan(multiprocess, workers = 4)\n\nThis table gives an overview of the different plans.\n\n\n\n\n\n\n\n\n\nType\nDescription\nMulti-node\nCopies of objects made?\n\n\n\n\nmultisession\nuses additional R sessions as the workers\nno\nyes\n\n\nmulticore\nuses forked R processes as the workers\nno\nnot if object not modified\n\n\ncluster\nuses R sessions on other machine(s)\nyes\nyes"
  },
  {
    "objectID": "units/unit6-parallel.html#accessing-variables-and-workers-in-the-worker-processes-1",
    "href": "units/unit6-parallel.html#accessing-variables-and-workers-in-the-worker-processes-1",
    "title": "Parallel processing",
    "section": "Accessing variables and workers in the worker processes",
    "text": "Accessing variables and workers in the worker processes\nThe future package usually does a good job of identifying the packages and (global) variables you use in your parallelized code and loading those packages on the workers and copying necessary variables to the workers. It uses the globals package to do this.\nHere’s a toy example that shows that n and MASS::geyser are automatically available in the worker processes.\n\nlibrary(future)\nlibrary(future.apply)\n\nplan(multisession)\n\nlibrary(MASS)\nn &lt;- nrow(geyser)\n\nmyfun &lt;- function(idx) {\n   # geyser is in MASS package\n   return(sum(geyser$duration) / n)\n}\n\nfuture_sapply(1:5, myfun)\n\n[1] 3.460814 3.460814 3.460814 3.460814 3.460814\n\n\nIn other contexts in R (or other languages) you may need to explicitly copy objects to the workers (or load packages on the workers). This is sometimes called exporting variables."
  },
  {
    "objectID": "units/unit11-optim.html",
    "href": "units/unit11-optim.html",
    "title": "Optimization",
    "section": "",
    "text": "PDF\nReferences:\nVideos (optional):\nThere are various videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to."
  },
  {
    "objectID": "units/unit11-optim.html#golden-section-search",
    "href": "units/unit11-optim.html#golden-section-search",
    "title": "Optimization",
    "section": "Golden section search",
    "text": "Golden section search\nThis strategy requires only that the function be unimodal.\nAssume we have a single minimum, in \\([a,b]\\). We choose two points in the interval and evaluate them, \\(f(x_{1})\\) and \\(f(x_{2})\\). If \\(f(x_{1})&lt;f(x_{2})\\) then the minimum must be in \\([a,x_{2}]\\), and if the converse in \\([x_{1},b]\\). We proceed by choosing a new point in the new, smaller interval and iterate. At each step we reduce the length of the interval in which the minimum must lie. The primary question involves what is an efficient rule to use to choose the new point at each iteration.\nSuppose we start with \\(x_{1}\\) and \\(x_{2}\\) s.t. they divide \\([a,b]\\) into three equal segments. Then we use \\(f(x_{1})\\) and \\(f(x_{2})\\) to rule out either the leftmost or rightmost segment based on whether \\(f(x_{1})&lt;f(x_{2})\\). If we have divided equally, we cannot place the next point very efficiently because either \\(x_{1}\\) or \\(x_{2}\\) equally divides the remaining space, so we are forced to divide the remaining space into relative lengths of 0.25, 0.25, and 0.5. The next time around, we may only rule out the shorter segment, which leads to inefficiency.\nThe efficient strategy is to maintain the golden ratio between the distances between the points using \\(\\phi=(\\sqrt{5}-1)/2\\approx.618\\) (the golden ratio), which is determined by solving for \\(\\phi\\) in this equation: \\(\\phi-\\phi^{2}=2\\phi-1\\). We start with \\(x_{1}=a+(1-\\phi)(b-a)\\) and \\(x_{2}=a+\\phi(b-a)\\). Then suppose \\(f(x_{1})&lt;f(x_{2})\\) so the minimum must be in \\([a,x_{2}]\\). Since \\(x_{1}-a&gt;x_{2}-x_{1}\\), we now choose \\(x_{3}\\) in the interval \\([a,x_{1}]\\) to produce three subintervals, \\([a,x_{3}],\\,[x_{3},x_{1}],\\,[x_{1},x_{2}]\\). We choose to place \\(x_{3}\\) s.t. it uses the golden ratio in the interval \\([a,x_{1}]\\), namely \\(x_{3}=a+(1-\\phi)(x_{2}-a)\\). This means that the length of the first subinterval is \\((\\phi-\\phi^{2})(b-a)\\) and the length of the third subinterval is \\((2\\phi-1)(b-a)\\), but those lengths are equal because we found \\(\\phi\\) to satisfy \\(\\phi-\\phi^{2}=2\\phi-1\\).\nThe careful choice of \\(\\phi\\) allows us to narrow the search interval by an equal proportion,\\(1-\\phi\\), in each iteration. Eventually we have narrowed the minimum to between \\(x_{t-1}\\) and \\(x_{t}\\), where the difference \\(|x_{t}-x_{t-1}|\\) is sufficiently small (within some tolerance - see Section 4 for details), and we report \\((x_{t}+x_{t-1})/2\\)."
  },
  {
    "objectID": "units/unit11-optim.html#bisection-method",
    "href": "units/unit11-optim.html#bisection-method",
    "title": "Optimization",
    "section": "Bisection method",
    "text": "Bisection method\nThe bisection method requires the existence of the first derivative but has the advantage over the golden section search of halving the interval at each step. We again assume unimodality.\nWe start with an initial interval \\((a_{0},b_{0})\\) and proceed to shrink the interval. Let’s choose \\(a_{0}\\) and \\(b_{0}\\), and set \\(x_{0}\\) to be the mean of these endpoints. Now we update according to the following algorithm, assuming our current interval is \\([a_{t},b_{t}]\\).\n\nIf \\(f^{\\prime}(a_{t})f^{\\prime}(x_{t})&lt;0\\), then \\([a_{t+1},b_{t+1}] = [a_{t},x_{t}]\\)\nIf \\(f^{\\prime}(a_{t}) f^{\\prime}(x_{t})&gt;0\\), then \\([a_{t+1},b_{t+1}] = [x_{t},b_{t}]\\)\n\nand set \\(x_{t+1}\\) to the mean of \\(a_{t+1}\\) and \\(b_{t+1}\\). The basic idea is that if the derivative at both \\(a_{t}\\) and \\(x_{t}\\) is negative, then the minimum must be between \\(x_{t}\\) and \\(b_{t}\\), based on the intermediate value theorem. If the derivatives at \\(a_{t}\\) and \\(x_{t}\\) are of different signs, then the minimum must be between \\(a_{t}\\) and \\(x_{t}\\).\nSince the bisection method reduces the size of the search space by one-half at each iteration, one can work out that each decimal place of precision requires 3-4 iterations. Obviously bisection is more efficient than the golden section search because we reduce by \\(0.5&gt;0.382=1-\\phi\\), so we’ve gained information by using the derivative. It requires an evaluation of the derivative however, while golden section just requires an evaluation of the original function.\nBisection is an example of a bracketing method, in which we trap the minimum within a nested sequence of intervals of decreasing length. These tend to be slow, but if the first derivative is continuous, they are robust and don’t require that a second derivative exist."
  },
  {
    "objectID": "units/unit11-optim.html#newton-raphson-newtons-method",
    "href": "units/unit11-optim.html#newton-raphson-newtons-method",
    "title": "Optimization",
    "section": "Newton-Raphson (Newton’s method)",
    "text": "Newton-Raphson (Newton’s method)\n\nOverview\nWe’ll talk about Newton-Raphson (N-R) as an optimization method rather than a root-finding method, but they’re just different perspectives on the same algorithm.\nFor N-R, we need two continuous derivatives that we can evaluate. The benefit is speed, relative to bracketing methods. We again assume the function is unimodal. The minimum must occur at \\(x^{*}\\) s.t. \\(f^{\\prime}(x^{*})=0\\), provided the second derivative is non-negative at \\(x^{*}\\). So we aim to find a zero (a root) of the first derivative function. Assuming that we have an initial value \\(x_{0}\\) that is close to \\(x^{*}\\), we have the Taylor series approximation \\[f^{\\prime}(x)\\approx f^{\\prime}(x_{0})+(x-x_{0})f^{\\prime\\prime}(x_{0}).\\] Now set \\(f^{\\prime}(x)=0\\), since that is the condition we desire (the condition that holds when we are at \\(x^{*}\\)), and solve for \\(x\\) to get \\[x_{1}=x_{0}-\\frac{f^{\\prime}(x_{0})}{f^{\\prime\\prime}(x_{0})},\\] and iterate, giving us updates of the form \\(x_{t+1}=x_{t}-\\frac{f^{\\prime}(x_{t})}{f^{\\prime\\prime}(x_{t})}\\). What are we doing intuitively? Basically we are taking the tangent to \\(f(x)\\) at \\(x_{0}\\) and extrapolating along that line to where it crosses the x-axis to find \\(x_{1}\\). We then reevaluate \\(f(x_{1})\\) and continue to travel along the tangents.\nOne can prove that if \\(f^{\\prime}(x)\\) is twice continuously differentiable, is convex, and has a root, then N-R converges from any starting point.\nNote that we can also interpret the N-R update as finding the analytic minimum of the quadratic Taylor series approximation to \\(f(x)\\).\nNewton’s method converges very quickly (as we’ll discuss in Section 4), but if you start too far from the minimum, you can run into serious problems.\n\n\nSecant method variation on N-R\nSuppose we don’t want to calculate the second derivative required in the divisor of N-R. We might replace the analytic derivative with a discrete difference approximation based on the secant line joining \\((x_{t},f^{\\prime}(x_{t}))\\) and \\((x_{t-1},f^{\\prime}(x_{t-1}))\\), giving an approximate second derivative: \\[f^{\\prime\\prime}(x_{t})\\approx\\frac{f^{\\prime}(x_{t})-f^{\\prime}(x_{t-1})}{x_{t}-x_{t-1}}.\\] For this variant on N-R, we need two starting points, \\(x_{0}\\) and \\(x_{1}\\).\nAn alternative to the secant-based approximation is to use a standard discrete approximation of the derivative such as \\[f^{\\prime\\prime}(x_{t})\\approx\\frac{f^{\\prime}(x_{t}+h)-f^{\\prime}(x_{t}-h)}{2h}.\\]\n\n\nHow can Newton’s method go wrong?\nLet’s think about what can go wrong - namely when we could have \\(f(x_{t+1})&gt;f(x_{t})\\)? To be concrete (and without loss of generality), let’s assume that \\(f(x_{t})&gt;0\\), in other words that \\(x^{*}&lt;x_{t}\\).\n\nAs usual, we can develop some intuition by starting with the worst case that \\(f^{\\prime\\prime}(x_{t})\\) is 0, in which case the method would fail as \\(x_{t+1}\\) would be \\(-\\infty\\).\nNow suppose that \\(f^{\\prime\\prime}(x_{t})\\) is a small positive number. Basically, if \\(f^{\\prime}(x_{t})\\) is relatively flat, we can get that \\(|x_{t+1}-x^{*}|&gt;|x_{t}-x^{*}|\\) because we divide by a small value for the second derivative, causing \\(x_{t+1}\\) to be far from \\(x_{t}\\) (though it does at least go in the correct direction). We’ll see an example on the board and the demo code (see below).\nNewton’s method can also go uphill (going in the wrong direction, away from \\(x^{*}\\)) when the second derivative is negative, with the method searching for a maximum, since we would have \\(x_{t+1}&gt;x_{t}\\). Another way to think of this is that Newton’s method does not automatically minimize the function, rather it finds local optima.\n\nIn all these cases Newton’s method could diverge, failing to converge on the optimum.\n\nDivergence\nFirst let’s see an example of divergence. The left and middle panels show two cases of convergence, while the right panel shows divergence. In the right panel, the initial second derivative value is small enough that \\(x_{2}\\) is further from \\(x^{*}\\) than \\(x_{1}\\) and then \\(x_{3}\\) is yet further away. In all cases the sequence of \\(x\\) values is indicated by the red letters.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef fp(x, theta=1):\n    ## First derivative - we want the root of this.\n    return np.exp(x * theta) / (1 + np.exp(x * theta)) - 0.5\n\ndef fpp(x, theta=1):\n    ## Second derivative - used to scale the optimization steps.\n    return np.exp(x * theta) / ((1 + np.exp(x * theta)) ** 2)\n\ndef make_plot(xs, xvals, fp, fpp, subplot, title):\n    plt.plot(xs, fp(xs), '-', label=\"f'(x)\", color = 'grey')\n    plt.plot(xs, fpp(xs), '--', label=\"f''(x)\", color = 'grey')\n    for i in range(len(xvals)):\n        plt.text(xvals[i], fp(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"f'(x)\")\n    plt.title(title)\n    plt.legend(loc='upper left')\n\n \nxs = np.linspace(-15, 15, 300)\n\nn = 10\nxvals = np.zeros(n)\n\n## Good starting point\nx0 = 1\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals, fp, fpp, 1, \"converges quickly\")\nplt.show()\n\n## Ok starting point\nx0 = 2\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals, fp, fpp, 2, \"converges\")\nplt.show()\n\n## Bad starting point\n\nx0 = 2.5\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n\n## print(xvals)\n\nmake_plot(xs, xvals, fp, fpp, 3, \"diverges\")\n## whoops!\n\nplt.show()\n\n\n\n\n\n\n\n/tmp/ipykernel_3595389/389944842.py:58: RuntimeWarning:\n\ndivide by zero encountered in double_scalars\n\n/tmp/ipykernel_3595389/389944842.py:6: RuntimeWarning:\n\ninvalid value encountered in double_scalars\n\n/tmp/ipykernel_3595389/389944842.py:10: RuntimeWarning:\n\ninvalid value encountered in double_scalars\n\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\n\n\nValueError: Image size of 16264910x898 pixels is too large. It must be less than 2^16 in each direction.\n\n\n&lt;Figure size 672x480 with 1 Axes&gt;\n\n\n\n\nMultiple optima: converging to the wrong optimum\nIn the first row of the next figure, let’s see an example of climbing uphill and finding a local maximum rather than minimum. The other rows show convergence. In all cases the minimum is at \\(x^{*}\\approx3.14\\)\n\n# Define the original function\ndef f(x):\n    return np.cos(x)\n\n# Define the gradient\ndef fp(x):\n    return -np.sin(x)\n\n# Define the second derivative\ndef fpp(x):\n    return -np.cos(x)# original fxn\n\ndef make_plot2(xs, xvals, f, fp, num, title):\n    # gradient subplot\n    plt.subplot(3, 2, num)\n    plt.plot(xs, fp(xs), '-', label=\"f'(x)\")\n    plt.scatter(np.pi, fp(np.pi))\n    for i in range(len(xvals)):\n        plt.text(xvals[i], fp(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel('x')\n    plt.ylabel(\"f'(x)\")\n    plt.title(title[0])\n    plt.legend(loc='lower right')\n    # function subplot\n    plt.subplot(3, 2, num+1)\n    plt.plot(xs, f(xs), '-', label=\"f(x)\")\n    plt.scatter(np.pi, f(np.pi))\n    for i in range(len(xvals)):\n        plt.text(xvals[i], f(xvals[i]), i, fontsize=14, color = 'red')\n    plt.xlabel('x')\n    plt.ylabel(\"f(x)\")\n    plt.title(title[1])\n    plt.legend(loc='lower right')\n \n\nxs = np.linspace(0, 2 * np.pi, num=300)\n\nx0 = 5.5 # starting point\nfp(x0) # positive\nfpp(x0) # negative\nx1 = x0 - fp(x0)/fpp(x0) # whoops, we've gone uphill \n## because of the negative second derivative\nxvals = np.zeros(n)\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n## print(xvals)\n\nplt.figure(figsize=(10, 8))\n\nmake_plot2(xs, xvals, f, fp, 1, title =\n    ['uphill to local maximum, gradient view', 'uphill to local maximum, function view'])\n\n## In contrast, with better starting points we can find the minimum\n## (but this nearly diverges).\n\nx0 = 4.3 # ok starting point\nfp(x0) \nfpp(x0) \nx1 = x0 - fp(x0)/fpp(x0)  # going downhill\n\nxvals[0] = x0\nfor t in range(1,10):\n    xvals[t] = xvals[t-1] - fp(xvals[t-1]) / fpp(xvals[t-1])\n## print(xvals)\n\nmake_plot2(xs, xvals, f, fp, 3, title =\n    ['nearly diverges, gradient view', 'nearly diverges, function view'])\n\n## With a better starting point, we converge quickly.\n\nx0 = 3.8 # good starting point\nfp(x0) \nfpp(x0) \nx1 = x0 - fp(x0)/fpp(x0) \n\nmake_plot2(xs, xvals, f, fp, 5, title =\n    ['better starting point, gradient view', 'better starting point, function view'])\n\nplt.show()\n\n\n\n\n\n\nImproving Newton’s method\nOne nice, general idea is to use a fast method such as Newton’s method safeguarded by a robust, but slower method. Here’s how one can do this for N-R, safeguarding with a bracketing method such as bisection. Basically, we check the N-R proposed move to see if N-R is proposing a step outside of where the root is known to lie based on the previous steps and the gradient values for those steps. If so, we could choose the next step based on bisection.\nAnother approach is backtracking. If a new value is proposed that yields a larger value of the function, backtrack to find a value that reduces the function. One possibility is a line search but given that we’re trying to reduce computation, a full line search is often unwise computationally (also in the multivariate Newton’s method, we are in the middle of an iterative algorithm for which we will just be going off in another direction anyway at the next iteration). A basic approach is to keep backtracking in halves. A nice alternative is to fit a polynomial to the known information about that slice of the function, namely \\(f(x_{t+1})\\), \\(f(x_{t})\\), \\(f^{\\prime}(x_{t})\\) and \\(f^{\\prime\\prime}(x_{t})\\) and find the minimum of the polynomial approximation."
  },
  {
    "objectID": "units/unit11-optim.html#convergence-metrics",
    "href": "units/unit11-optim.html#convergence-metrics",
    "title": "Optimization",
    "section": "Convergence metrics",
    "text": "Convergence metrics\nWe might choose to assess whether \\(f^{\\prime}(x_{t})\\) is near zero, which should assure that we have reached the critical point. However, in parts of the domain where \\(f(x)\\) is fairly flat, we may find the derivative is near zero even though we are far from the optimum. Instead, we generally monitor \\(|x_{t+1}-x_{t}|\\) (for the moment, assume \\(x\\) is scalar). We might consider absolute convergence: \\(|x_{t+1}-x_{t}|&lt;\\epsilon\\) or relative convergence, \\(\\frac{|x_{t+1}-x_{t}|}{|x_{t}|}&lt;\\epsilon\\). Relative convergence is appealing because it accounts for the scale of \\(x\\), but it can run into problems when \\(x_{t}\\) is near zero, in which case one can use \\(\\frac{|x_{t+1}-x_{t}|}{|x_{t}|+\\epsilon}&lt;\\epsilon\\). We would want to account for machine precision in thinking about setting \\(\\epsilon\\). For relative convergence a reasonable choice of \\(\\epsilon\\) would be to use the square root of machine epsilon or about \\(1\\times10^{-8}\\).\nProblems with the optimization may show up in a convergence measure that fails to decrease or cycles (oscillates). Software generally has a stopping rule that stops the algorithm after a fixed number of iterations; these can generally be changed by the user. When an algorithm stops because of the stopping rule before the convergence criterion is met, we say the algorithm has failed to converge. Sometimes we just need to run it longer, but often it indicates a problem with the function being optimized or with your starting value.\nFor multivariate optimization, we use a distance metric between \\(x_{t+1}\\) and \\(x_{t}\\), such as \\(\\|x_{t+1}-x_{t}\\|_{p}\\) , often with \\(p=1\\) or \\(p=2\\)."
  },
  {
    "objectID": "units/unit11-optim.html#starting-values",
    "href": "units/unit11-optim.html#starting-values",
    "title": "Optimization",
    "section": "Starting values",
    "text": "Starting values\nGood starting values are important because they can improve the speed of optimization, prevent divergence or cycling, and prevent finding local optima.\nUsing random or selected multiple starting values can help with multiple optima (aka multimodality).\nHere’s a function (the Rastrigin function) with multiple optima that is commonly used for testing methods that claim to work well for multimodal problems. This is a hard function to optimize with respect to, particularly in higher dimensions (one can do it in higher dimensions than 2 by simply making the \\(x\\) vector longer but having the same structure). In particular Rastrigin with 30 dimensions is considered to be very hard.\n\ndef rastrigin(x):\n    A = 10\n    n = len(x)\n    return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\nconst = 5.12\nnGrid = 100\ngr = np.linspace(-const, const, num=nGrid)\n\n# Create a grid of x values\nx1, x2 = np.meshgrid(gr, gr)\nxs = np.column_stack((x1.ravel(), x2.ravel()))\n\n# Calculate the Rastrigin function for each point in the grid\ny = np.apply_along_axis(rastrigin, 1, xs)\n\n# Create a plot\nplt.figure(figsize=(8, 6))\nplt.imshow(y.reshape((nGrid, nGrid)), extent=[-const, const, -const, const], origin='lower', cmap='viridis')\nplt.colorbar()\nplt.title('Rastrigin Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()"
  },
  {
    "objectID": "units/unit11-optim.html#convergence-rates",
    "href": "units/unit11-optim.html#convergence-rates",
    "title": "Optimization",
    "section": "Convergence rates",
    "text": "Convergence rates\nLet \\(\\epsilon_{t}=|x_{t}-x^{*}|\\). If the limit\n\\[\\lim_{t\\to\\infty}\\frac{|\\epsilon_{t+1}|}{|\\epsilon_{t}|^{\\beta}}=c\\] exists for \\(\\beta&gt;0\\) and \\(c\\ne0\\), then a method is said to have order of convergence \\(\\beta\\). This basically measures how big the error at the \\(t+1\\)th iteration is relative to that at the \\(t\\)th iteration, with the approximation that \\(|\\epsilon_{t+1}|\\approx c|\\epsilon_{t}|^{\\beta}\\).\nBisection doesn’t formally satisfy the criterion needed to make use of this definition, but roughly speaking it has linear convergence (\\(\\beta=1\\)), so the magnitude of the error decreases by a factor of \\(c\\) at each step. Next we’ll see that N-R has quadratic convergence (\\(\\beta=2\\)), which is fast.\nTo analyze convergence of N-R, consider a Taylor expansion of the gradient at the minimum, \\(x^{*}\\), around the current value, \\(x_{t}\\): \\[f^{\\prime}(x^{*})=f^{\\prime}(x_{t})+(x^{*}-x_{t})f^{\\prime\\prime}(x_{t})+\\frac{1}{2}(x^{*}-x_{t})^{2}f^{\\prime\\prime\\prime}(\\xi_{t})=0,\\] for some \\(\\xi_{t}\\in[x^{*},x_{t}]\\). Making use of the N-R update equation: \\(x_{t+1}=x_{t}-\\frac{f^{\\prime}(x_{t})}{f^{\\prime\\prime}(x_{t})}\\) to substitute , and some algebra, we have \\[\\frac{|x^{*}-x_{t+1}|}{(x^{*}-x_{t})^{2}}=\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(\\xi_{t})}{f^{\\prime\\prime}(x_{t})}.\\] If the limit of the ratio on the right hand side exists and is equal to \\(c\\): \\[c=\\lim_{x_{t}\\to x^{*}}\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(\\xi_{t})}{f^{\\prime\\prime}(x_{t})}=\\frac{1}{2}\\frac{f^{\\prime\\prime\\prime}(x^{*})}{f^{\\prime\\prime}(x^{*})}\\] then we see that \\(\\beta=2\\).\nIf \\(c\\) were one, then we see that if we have \\(k\\) digits of accuracy at \\(t\\), we’d have \\(2k\\) digits at \\(t+1\\) (e.g., \\(|\\epsilon_{t}|=0.01\\) results in \\(|\\epsilon_{t+1}|=0.0001\\)), which justifies the characterization of quadratic convergence being fast. In practice \\(c\\) will moderate the rate of convergence. The smaller \\(c\\) the better, so we’d like to have the second derivative be large and the third derivative be small. The expression also indicates we’ll have a problem if \\(f^{\\prime\\prime}(x_{t})=0\\) at any point (think about what this corresponds to graphically - what is our next step when \\(f^{\\prime\\prime}(x_{t})=0\\)?). The characteristics of the derivatives determine the domain of attraction (the region in which we’ll converge rather than diverge) of the minimum.\nGivens and Hoeting show that using the secant-based approximation to the second derivative in N-R has order of convergence, \\(\\beta\\approx1.62\\).\nHere’s an example of convergence comparing bisection and N-R:\n\nnp.set_printoptions(precision=10)\n\n# Define the original function\ndef f(x):\n    return np.cos(x)\n\n# Define the gradient\ndef fp(x):\n    return -np.sin(x)\n\n# Define the second derivative\ndef fpp(x):\n    return -np.cos(x)\n\nxstar = np.pi  # known minimum\n\n## Newton-Raphson (N-R) method\nx0 = 2\nxvals = [x0] + [None] * 9\nfor t in range(1, 10):\n    xvals[t] = xvals[t - 1] - fp(xvals[t - 1]) / fpp(xvals[t - 1])\n\nprint(xvals)\n\n## Bisection method\ndef bisec_step(interval, fp):\n    interval = interval.copy()\n    xt = np.mean(interval)\n    if fp(interval[0]) * fp(xt) &lt;= 0:\n        interval[1] = xt\n    else:\n        interval[0] = xt\n    return interval\n\nn_it = 30\na0 = 2\nb0 = (3 * np.pi / 2) - (xstar - a0)\ninterval = np.zeros((n_it, 2))\ninterval[0,:] = [a0, b0]\n\nfor t in range(1, n_it):\n    interval[t,:] = bisec_step(interval[t-1,:], fp)\n\nprint(np.mean(interval, axis=1))\n\n[2, 4.185039863261519, 2.467893674514666, 3.266186277569106, 3.1409439123176353, 3.1415926536808043, 3.141592653589793, 3.141592653589793, 3.141592653589793, 3.141592653589793]\n[2.7853981634 3.1780972451 2.9817477042 3.0799224747 3.1290098599\n 3.1535535525 3.1412817062 3.1474176293 3.1443496678 3.142815687\n 3.1420486966 3.1416652014 3.1414734538 3.1415693276 3.1416172645\n 3.141593296  3.1415813118 3.1415873039 3.1415903    3.141591798\n 3.141592547  3.1415929215 3.1415927343 3.1415926406 3.1415926875\n 3.1415926641 3.1415926524 3.1415926582 3.1415926553 3.1415926538]"
  },
  {
    "objectID": "units/unit11-optim.html#profiling",
    "href": "units/unit11-optim.html#profiling",
    "title": "Optimization",
    "section": "Profiling",
    "text": "Profiling\nA core technique for likelihood optimization is to analytically maximize over any parameters for which this is possible. Suppose we have two sets of parameters, \\(\\theta_{1}\\) and \\(\\theta_{2}\\), and we can analytically maximize w.r.t \\(\\theta_{2}\\). This will give us \\(\\hat{\\theta}_{2}(\\theta_{1})\\), a function of the remaining parameters over which analytic maximization is not possible. Plugging in \\(\\hat{\\theta}_{2}(\\theta_{1})\\) into the objective function (in this case generally the likelihood or log likelihood) gives us the profile (log) likelihood solely in terms of the obstinant parameters. For example, suppose we have the regression likelihood with correlated errors: \\[Y\\sim\\mathcal{N}(X\\beta,\\sigma^{2}\\Sigma(\\rho)),\\] where \\(\\Sigma(\\rho)\\) is a correlation matrix that is a function of a parameter, \\(\\rho\\). The maximum w.r.t. \\(\\beta\\) is easily seen to be the GLS estimator \\(\\hat{\\beta}(\\rho)=(X^{\\top}\\Sigma(\\rho)^{-1}X)^{-1}X^{\\top}\\Sigma(\\rho)^{-1}Y\\). (In general such a maximum is a function of all of the other parameters, but conveniently it’s only a function of \\(\\rho\\) here.) This gives us the initial profile likelihood \\[\\frac{1}{(\\sigma^{2})^{n/2}|\\Sigma(\\rho)|^{1/2}}\\exp\\left(-\\frac{(Y-X\\hat{\\beta}(\\rho))^{-\\top}\\Sigma(\\rho)^{-1}(Y-X\\hat{\\beta}(\\rho))}{2\\sigma^{2}}\\right).\\] We then notice that the likelihood is maximized w.r.t. \\(\\sigma^{2}\\) at \\[\\hat{\\sigma^{2}}(\\rho)=\\frac{(Y-X\\hat{\\beta}(\\rho))^{\\top}\\Sigma(\\rho)^{-1}(Y-X\\hat{\\beta}(\\rho))}{n}.\\] This gives us the final profile likelihood, \\[\\frac{1}{|\\Sigma(\\rho)|^{1/2}}\\frac{1}{(\\hat{\\sigma^{2}}(\\rho))^{n/2}}\\exp(-\\frac{1}{2}n),\\] a function of \\(\\rho\\) only, for which numerical optimization is much simpler."
  },
  {
    "objectID": "units/unit11-optim.html#newton-raphson-newtons-method-1",
    "href": "units/unit11-optim.html#newton-raphson-newtons-method-1",
    "title": "Optimization",
    "section": "Newton-Raphson (Newton’s method)",
    "text": "Newton-Raphson (Newton’s method)\nFor multivariate \\(x\\) we have the Newton-Raphson update \\(x_{t+1}=x_{t}-f^{\\prime\\prime}(x_{t})^{-1}f^{\\prime}(x_{t})\\), or in our other notation, \\[x_{t+1}=x_{t}-H_{f}(x_{t})^{-1}\\nabla f(x_{t}).\\]\nLet’s consider a very simple example of nonlinear least squares. We’ll use the famous Mauna Loa atmospheric carbon dioxide record.\nLet’s suppose (I have no real reason to think this) that we think that the data can be well-represented by this nonlinear model: \\[Y_{i}=\\beta_{0}+\\beta_{1}\\exp(t_i/\\beta_{2}+\\epsilon_{i}.\\]\nSome of the things we need to worry about with Newton’s method in general about are (1) good starting values, (2) positive definiteness of the Hessian, and (3) avoiding errors in deriving the derivatives.\nA note on the positive definiteness: since the Hessian may not be positive definite (although it may well be, provided the function is approximately locally quadratic), one can consider modifying the Cholesky decomposition of the Hessian to enforce positive definiteness by adding diagonal elements to \\(H_{f}\\) as necessary.\n\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_csv('co2_annmean_mlo.csv', header = 0, names = ['year','co2','unc'])\n\nplt.scatter(data.year, data.co2)\nplt.xlabel('year')\nplt.ylabel(\"CO2\")\nplt.show()\n\n## Center years for better numerical behavior\ndata.year = data.year - np.mean(data.year)\n### Linear fit - not a good model\nX = sm.add_constant(data.year)  \nmodel = sm.OLS(data.co2, X).fit()\n\nplt.scatter(data.year, data.co2)\nplt.plot(data.year, model.fittedvalues, '-')\n\nplt.show()\n\n\n\n\n\n\n\nWe need some starting values. Having centered the year variable, \\(\\beta_2\\) seems plausibly like it would be order of magnitude of 10, which is about the magnitude of the year values.\n\nbeta2_init = 10\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nplt.scatter(data.year, data.co2)\n\ndef fit(params):\n    return params[0] + params[1] * np.exp(data.year / params[2])\n\nbeta = (beta0_init, beta1_init, beta2_init)\nplt.plot(data.year, fit(beta), '-')\nplt.show()\n\n\n\n\nThat’s not great. How about changing the scale of beta2 more?\n\nbeta2_init = 100\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nplt.scatter(data.year, data.co2)\nbeta = (beta0_init, beta1_init, beta2_init)\nplt.plot(data.year, fit(beta), '-')\n\nplt.show()\n\n\n\n\nLet’s get derivative information using automatic differentation (the algorithmic implementation of the chain rule for derivatives also used in gradient descent in deep learning, as well as various other contexts). We’ll use Jax, but PyTorch or Tensorflow are other options. We need to use the Jax versions of various numpy operations in order to be able to get the derivatives.\n\nimport jax.numpy as jnp\nimport jax\n\ndef loss(params):\n    fitted = params[0] + params[1]*jnp.exp(jnp.array(data.year)/params[2])\n    return jnp.sum((fitted - jnp.array(data.co2))**2.0)\n\nderiv1 = jax.grad(loss)\nderiv2 = jax.hessian(loss)\n\nderiv1(jnp.array([beta0_init, beta1_init, beta2_init]))\nhess = deriv2(jnp.array([beta0_init, beta1_init, beta2_init]))\nhess\n\nnp.linalg.eig(hess)[0]\n\narray([ 2.6369040e+02, -2.5794024e-03,  1.3906310e+01], dtype=float32)\n\n\nThe Hessian is not positive definite. We could try tricks such as adding to the diagonal of the Hessian or using the pseudo-inverse (i.e., setting all negative eigenvalues in the inverse to zero).\nInstead, let’s try a bit more to find starting values where the Hessian is positive definite. The order of magnitude of our initial value for \\(\\beta_2\\) seems about right, so let’s try halving or doubling it.\n\nbeta2_init = 50\nimplicit_covar = np.exp(data.year/beta2_init)\n\nX = sm.add_constant(implicit_covar)\nmodel = sm.OLS(data.co2, X).fit()\nbeta0_init, beta1_init = model.params\n\nhess = deriv2(jnp.array([beta0_init, beta1_init, beta2_init]))\n\nnp.linalg.eig(hess)[0]\n\narray([3.0403909e+02, 2.4109183e-01, 5.5710766e+01], dtype=float32)\n\n\nThat seems better. Let’s try with that.\n\nn_it = 10\nxvals = np.zeros(shape = (n_it, 3))\nxvals[0, :] = (beta0_init, beta1_init, beta2_init)\n\nfor t in range(1, n_it):\n  jxvals = jnp.array(xvals[t-1, :])\n  hess = deriv2(jxvals)\n  e = np.linalg.eig(hess)\n  if(np.any(e[0] &lt; 0)):\n    raise ValueError(\"not positive definite\")\n  xvals[t, :] = xvals[t-1, :] - np.linalg.solve(hess, deriv1(jxvals))\n  print(loss(xvals[t,:]))\n\nbeta_hat = xvals[t,:]\n\nplt.scatter(data.year, data.co2)\nplt.plot(data.year, fit(beta_hat), 'r-')\n\nplt.show()\n\n38.304596\n30.817617\n30.444086\n30.442629\n30.44241\n30.442383\n30.442421\n30.442448\n30.442455\n\n\n\n\n\nThat looks pretty good, but the lack of positive definiteness/sensitivity to starting values should make us cautious. That said, in this case we can visually assess the fit and see that it looks pretty good.\nNext we’ll see that some optimization methods used commonly for statistical models (in particular Fisher scoring and iterative reweighted least squares (IRLS or IWLS) are just Newton-Raphson in disguise."
  },
  {
    "objectID": "units/unit11-optim.html#fisher-scoring-variant-on-n-r-optional",
    "href": "units/unit11-optim.html#fisher-scoring-variant-on-n-r-optional",
    "title": "Optimization",
    "section": "Fisher scoring variant on N-R (optional)",
    "text": "Fisher scoring variant on N-R (optional)\nThe Fisher information (FI) is the expected value of the outer product of the gradient of the log-likelihood with itself\n\\[I(\\theta)=E_{f}(\\nabla f(y)\\nabla f(y)^{\\top}),\\]\nwhere the expected value is with respect to the data distribution. Under regularity conditions (true for exponential families), the expectation of the Hessian of the log-likelihood is minus the Fisher information, \\(E_{f}H_{f}(y)=-I(\\theta)\\). We get the observed Fisher information by plugging the data values into either expression instead of taking the expected value.\nThus, standard N-R can be thought of as using the observed Fisher information to find the updates. Instead, if we can compute the expectation, we can use minus the FI in place of the Hessian. The result is the Fisher scoring (FS) algorithm. Basically instead of using the Hessian for a given set of data, we are using the FI, which we can think of as the average Hessian over repeated samples of data from the data distribution. FS and N-R have the same convergence properties (i.e., quadratic convergence) but in a given problem, one may be computationally or analytically easier. Givens and Hoeting comment that FS works better for rapid improvements at the beginning of iterations and N-R better for refinement at the end. \\[\\begin{aligned}\n(NR):\\,\\theta_{t+1} & = & \\theta_{t}-H_{f}(\\theta_{t})^{-1}\\nabla f(\\theta_{t})\\\\\n(FS):\\,\\theta_{t+1} & = & \\theta_{t}+I(\\theta_{t})^{-1}\\nabla f(\\theta_{t})\\end{aligned}\\]\nThe Gauss-Newton algorithm for nonlinear least squares involves using the FI in place of the Hessian in determining a Newton-like step. nls() in R uses this approach.\n\nConnections between statistical uncertainty and ill-conditionedness\nWhen either the observed or expected FI matrix is nearly singular this means we have a small eigenvalue in the inverse covariance (the precision), which means a large eigenvalue in the covariance matrix. This indicates some linear combination of the parameters has low precision (high variance), and that in that direction the likelihood is nearly flat. As we’ve seen with N-R, convergence slows with shallow gradients, and we may have numerical problems in determining good optimization steps when the likelihood is sufficiently flat. So convergence problems and statistical uncertainty go hand in hand. One, but not the only, example of this occurs when we have nearly collinear regressors."
  },
  {
    "objectID": "units/unit11-optim.html#irls-iwls-for-generalized-linear-models-glms",
    "href": "units/unit11-optim.html#irls-iwls-for-generalized-linear-models-glms",
    "title": "Optimization",
    "section": "IRLS (IWLS) for Generalized Linear Models (GLMs)",
    "text": "IRLS (IWLS) for Generalized Linear Models (GLMs)\nAs many of you know, iterative reweighted least squares (also called iterative weighted least squares) is the standard method for estimation with GLMs. It involves linearizing the model and using working weights and working variances and solving a weighted least squares (WLS) problem (recalling that the generic WLS solution is \\(\\hat{\\beta}=(X^{\\top}WX)^{-1}X^{\\top}WY\\)).\nExponential families can be expressed as \\[f(y;\\theta,\\phi)=\\exp((y\\theta-b(\\theta))/a(\\phi)+c(y,\\phi)),\\] with \\(E(Y)=b^{\\prime}(\\theta)\\) and \\(\\mbox{Var}(Y)=b^{\\prime\\prime}(\\theta)\\). If we have a GLM in the canonical parameterization (log link for Poisson data, logit for binomial), we have the natural parameter \\(\\theta\\) equal to the linear predictor, \\(\\theta=\\eta\\). A standard linear predictor would simply be \\(\\eta=X\\beta\\).\nConsidering N-R for a GLM in the canonical parameterization (and ignoring \\(a(\\phi)\\), which is one for logistic and Poisson regression), one can show that the gradient of the GLM log-likelihood is the inner product of the covariates and a residual vector, \\(\\nabla l(\\beta)=(Y-E(Y))^{\\top}X\\), and the Hessian is \\(H_{l}(\\beta)=-X^{\\top}WX\\) where \\(W\\) is a diagonal matrix with \\(\\{\\mbox{Var}(Y_{i})\\}\\) on the diagonal (the working weights). Note that both \\(E(Y)\\) and the variances in \\(W\\) depend on \\(\\beta\\), so these will change as we iteratively update \\(\\beta\\). Therefore, the N-R update is \\[\\beta_{t+1}=\\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})\\] where \\(E(Y)_{t}\\) and \\(W_{t}\\) are the values at the current parameter estimate, \\(\\beta_{t}\\) . For example, for logistic regression (here with \\(n_{i}=1\\)), \\(W_{t,ii}=p_{ti}(1-p_{ti})\\) and \\(E(Y)_{ti}=p_{ti}\\) where \\(p_{ti}=\\frac{\\exp(X_{i}^{\\top}\\beta_{t})}{1+\\exp(X_{i}^{\\top}\\beta_{t})}\\). In the canonical parameterization of a GLM, the Hessian does not depend on the data, so the observed and expected FI are the same, and therefore N-R and FS are the same.\nThe update above can be rewritten in the standard form of IRLS as a WLS problem, \\[\\begin{aligned}\n\\beta_{t+1} & = \\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})\\\\\n& = (X^{\\top}W_{_{t}}X)^{-1}(X^{\\top}W_{_{t}}X)\\beta_{t}+(X^{\\top}W_{_{t}}X)^{-1}X^{\\top}(Y-E(Y)_{t})\\\\\n& = (X^{\\top}W_{_{t}}X)^{-1}X^{\\top}W_{t}\\left[X\\beta_{t}+W_{t}^{-1}(Y-E(Y)_{t})\\right]\\\\\n& = (X^{\\top}W_{_{t}}X)^{-1}X^{\\top}W_{t}\\tilde{Y}_{t},\\end{aligned}\\] where the so-called working observations are \\(\\tilde{Y}_{t}=X\\beta_{t}+W_{t}^{-1}(Y-E(Y)_{t})\\). Note that these are on the scale of the linear predictor. The interpretation is that the working observations are equal to the current fitted values, \\(X\\beta_{t}\\), plus weighted residuals where the weight (the inverse of the variance) takes the actual residuals and scales to the scale of the linear predictor.\nWhile IRLS is standard for GLMs, you can also use general purpose optimization routines.\nIRLS is a special case of the general Gauss-Newton method for nonlinear least squares."
  },
  {
    "objectID": "units/unit11-optim.html#descent-methods-and-newton-like-methods",
    "href": "units/unit11-optim.html#descent-methods-and-newton-like-methods",
    "title": "Optimization",
    "section": "Descent methods and Newton-like methods",
    "text": "Descent methods and Newton-like methods\nMore generally a Newton-like method has updates of the form \\[x_{t+1}=x_{t}-\\alpha_{t}M_{t}^{-1}f^{\\prime}(x_{t}).\\] We can choose \\(M_{t}\\) in various ways, including as an approximation to the second derivative.\nThis opens up several possibilities:\n\nusing more computationally efficient approximations to the second derivative,\navoiding steps that do not go in the correct direction (i.e., go uphill when minimizing), and\nscaling by \\(\\alpha_{t}\\) so as not to step too far.\n\nLet’s consider a variety of strategies.\n\nDescent methods\nThe basic strategy is to choose a good direction and then choose the longest step for which the function continues to decrease. Suppose we have a direction, \\(p_{t}\\). Then we need to move \\(x_{t+1}=x_{t}+\\alpha_{t}p_{t}\\), where \\(\\alpha_{t}\\) is a scalar, choosing a good \\(\\alpha_{t}\\). We might use a line search (e.g., bisection or golden section search) to find the local minimum of \\(f(x_{t}+\\alpha_{t}p_{t})\\) with respect to \\(\\alpha_{t}\\). However, we often would not want to run to convergence, since we’ll be taking additional steps anyway.\nSteepest descent chooses the direction as the steepest direction downhill, setting \\(M_{t}=I\\), since the gradient gives the steepest direction uphill (the negative sign in the equation below has us move directly downhill rather than directly uphill). Given the direction, we want to scale the step \\[x_{t+1}=x_{t}-\\alpha_{t}f^{\\prime}(x_{t})\\] where the contraction, or step length, parameter \\(\\alpha_{t}\\) is chosen sufficiently small to ensure that we descend, via some sort of line search. The critical downside to steepest descent is that when the contours are elliptical, it tends to zigzag; here’s an example.\nMy original code for this was in R, so I’m just leaving it that way rather than having to do a lot of fine-tuning to get the image to display the way I want in Python.\n(Note that I do a full line search (using the golden section method via optimize()) at each step in the direction of steepest descent - this is generally computationally wasteful, but I just want to illustrate how steepest descent can go wrong, even if you go the “right” amount in each direction.)\npar(mai = c(.5,.4,.1,.4))\nf &lt;- function(x){\n    x[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000\n}\nfp &lt;- function(x){\n    c(2 * x[1]/1000 + 4 * x[2]/1000,\n    4 * x[1]/1000 + 10 * x[2]/1000)\n}\nlineSearch &lt;- function(alpha, xCurrent, direction, FUN){\n    newx &lt;- xCurrent + alpha * direction\n    FUN(newx)\n}\nnIt &lt;- 50\nxvals &lt;- matrix(NA, nr = nIt, nc = 2)\nxvals[1, ] &lt;- c(7, -4)\nfor(t in 2:50){\n    newalpha &lt;- optimize(lineSearch, interval = c(-5000, 5000),\n        xCurrent = xvals[t-1, ], direction = fp(xvals[t-1, ]),\n        FUN = f)$minimum \n    xvals[t, ] &lt;- xvals[t-1, ] + newalpha * fp(xvals[t-1, ])\n}\nx1s &lt;- seq(-5, 8, len = 100); x2s = seq(-5, 2, len = 100)\nfx &lt;- apply(expand.grid(x1s, x2s), 1, f)\n## plot f(x) surface on log scale\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100), \n    xlim = c(-5, 8), ylim = c(-5,2)) \nlines(xvals) ## overlay optimization path\n\n\n\nPath of steepest descent\n\n\nIf the contours are circular, steepest descent works well. Newton’s method deforms elliptical contours based on the Hessian. Another way to think about this is that steepest descent does not take account of the rate of change in the gradient, while Newton’s method does.\nThe general descent algorithm is \\[x_{t+1}=x_{t}-\\alpha_{t}M_{t}^{-1}f'(x_{t}),\\] where \\(M_{t}\\) is generally chose to approximate the Hessian and \\(\\alpha_{t}\\) allows us to adjust the step in a smart way. Basically, since the negative gradient tells us the direction that descends (at least within a small neighborhood), if we don’t go too far, we should be fine and should work our way downhill. One can work this out formally using a Taylor approximation to \\(f(x_{t+1})-f(x_{t})\\) and see that we make use of \\(M_{t}\\) being positive definite. (Unfortunately backtracking with positive definite \\(M_{t}\\) does not give a theoretical guarantee that the method will converge. We also need to make sure that the steps descend sufficiently quickly and that the algorithm does not step along a level contour of \\(f\\).)\nThe conjugate gradient algorithm for iteratively solving large systems of equations is all about choosing the direction and the step size in a smart way given the optimization problem at hand.\n\n\nQuasi-Newton methods such as BFGS\nOther replacements for the Hessian matrix include estimates that do not vary with \\(t\\) and finite difference approximations. When calculating the Hessian is expensive, it can be very helpful to substitute an approximation.\nA basic finite difference approximation requires us to compute finite differences in each dimension, but this could be computationally burdensome. A more efficient strategy for choosing \\(M_{t+1}\\) is to (1) make use of \\(M_{t}\\) and (2) make use of the most recent step to learn about the curvature of \\(f^{\\prime}(x)\\) in the direction of travel. One approach is to use a rank one update to \\(M_{t}\\).\nA basic strategy is to choose \\(M_{t+1}\\) such that the secant condition is satisfied: \\[M_{t+1}(x_{t+1}-x_{t})=\\nabla f(x_{t+1})-\\nabla f(x_{t}),\\] which is motivated by the fact that the secant approximates the gradient in the direction of travel. Basically this says to modify \\(M_{t}\\) in such a way that we incorporate what we’ve learned about the gradient from the most recent step. \\(M_{t+1}\\) is not fully determined based on this, and we generally impose other conditions, in particular that \\(M_{t+1}\\) is symmetric and positive definite. Defining \\(s_{t}=x_{t+1}-x_{t}\\) and \\(y_{t}=\\nabla f(x_{t+1})-\\nabla f(x_{t})\\), the unique, symmetric rank one update (why is the following a rank one update?) that satisfies the secant condition is \\[M_{t+1}=M_{t}+\\frac{(y_{t}-M_{t}s_{t})(y_{t}-M_{t}s_{t})^{\\top}}{(y_{t}-M_{t}s_{t})^{\\top}s_{t}}.\\] If the denominator is positive, \\(M_{t+1}\\) may not be positive definite, but this is guaranteed for non-positive values of the denominator. One can also show that one can achieve positive definiteness by shrinking the denominator toward zero sufficiently.\nA standard approach to updating \\(M_{t}\\) is a commonly-used rank two update that generally results in \\(M_{t+1}\\) being positive definite is \\[M_{t+1}=M_{t}-\\frac{M_{t}s_{t}(M_{t}s_{t})^{\\top}}{s_{t}^{\\top}M_{t}s_{t}}+\\frac{y_{t}y_{t}^{\\top}}{s_{t}^{\\top}y_{t}},\\] which is known as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. This is one of the methods used in R in optim().\nQuestion: how can we update \\(M_{t}^{-1}\\) to \\(M_{t+1}^{-1}\\) efficiently? It turns out there is a way to update the Cholesky of \\(M_{t}\\) efficiently and this is a better approach than updating the inverse.\nThe order of convergence of quasi-Newton methods is generally slower than the quadratic convergence of N-R because of the approximations but still faster than linear. In general, quasi-Newton methods will do much better if the scales of the elements of \\(x\\) are similar. Lange suggests using a starting point for which one can compute the expected information, to provide a good starting value \\(M_{0}\\).\nNote that for estimating a covariance based on the numerical information matrix, we would not want to rely on \\(M_{t}\\) from the final iteration, as the approximation may be poor. Rather we would spend the effort to better estimate the Hessian directly at \\(x^{*}\\).\n\n\nStochastic gradient descent\nStochastic gradient descent (SGD) is the hot method in machine learning, commonly used for fitting deep neural networks. It allows you to optimize an objective function with respect to what is often a very large number of parameters even when the data size is huge.\nGradient descent is a simplification of Newton’s method that does not rely on the second derivative, but rather chooses the direction using the gradient and then a step size, \\(\\alpha_{t}\\): \\[x_{t+1}=x_{t}-\\alpha_{t}f^{\\prime}(x_{t})\\]\nThe basic idea of stochastic gradient descent is to replace the gradient with a function whose expected value is the gradient, \\(E(g(x_{t}))=f^{\\prime}(x_{t})\\): \\[x_{t+1}=x_{t}-\\alpha_{t}g(x_{t})\\] Thus on average we should go in a good (downhill) direction. Given that we know that strictly following the gradient can lead to slow convergence, it makes some intuitive sense that we could still do ok without using the exact gradient. One can show formally that SGD will converge for convex functions.\nSGD can be used in various contexts, but the common one we will focus on is when \\[\\begin{aligned}\nf(x) & = \\sum_{i=1}^{n}f_{i}(x)\\\\\nf^{\\prime}(x) & = \\sum_{i=1}^{n}f^{\\prime}_{i}(x)\\end{aligned}\\] for large \\(n\\). Thus calculation of the gradient is \\(O(n)\\), and we may not want to incur that computational cost. How could we implement SGD in such a case? At each iteration we could randomly choose an observation and compute the contribution to the gradient from that data point, or we could choose a random subset of the data (this is mini-batch SGD), or there are variations where we systematically cycle through the observations or cycle through subsets. However, in some situations, convergence is actually much faster when using randomness. And if the data are ordered in some meaningful way we definitely do not want to cycle through the observations in that order, as this can result in a biased estimate of the gradient and slow convergence. So one generally randomly shuffles the data before starting SGD. Note that using subsets rather than individual observations is likely to be more effective as it can allow us to use optimized matrix/vector computations.\nHow should one choose the step size, \\(\\alpha_{t}\\) (also called the learning rate)? One might think that as one gets close to the optimum, if one isn’t careful, one might simply bounce around near the optimum in a random way, without actually converging to the optimum. So intuition suggests that \\(\\alpha_{t}\\) should decrease with \\(t\\). Some choices of step size have included:\n\n\\(\\alpha_{t}=1/t\\)\nset a schedule, such that for \\(T\\) iterations, \\(\\alpha_{t}=\\alpha\\), then for the next \\(T\\), \\(\\alpha_{t}=\\alpha\\gamma\\), then for the next \\(T\\), \\(\\alpha_{t}=\\alpha\\gamma^{2}\\). A heuristic is for \\(\\gamma\\in(0.8,0.9)\\).\nrun with \\(\\alpha_{t}=\\alpha\\) for \\(T\\) iterations, then with \\(\\alpha_{t}=\\alpha/2\\) for \\(2T\\), then with \\(\\alpha_{t}=\\alpha/4\\) for \\(4T\\) and so forth."
  },
  {
    "objectID": "units/unit11-optim.html#coordinate-descent-gauss-seidel",
    "href": "units/unit11-optim.html#coordinate-descent-gauss-seidel",
    "title": "Optimization",
    "section": "Coordinate descent (Gauss-Seidel)",
    "text": "Coordinate descent (Gauss-Seidel)\nGauss-Seidel is also known a back-fitting or cyclic coordinate descent. The basic idea is to work element by element rather than having to choose a direction for each step. For example backfitting used to be used to fit generalized additive models of the form \\(E(Y)=f_{1}(z_{1})+f_{2}(z_{2})+\\ldots+f_{p}(z_{p})\\).\nThe basic strategy is to consider the \\(j\\)th component of \\(f^{\\prime}(x)\\) as a univariate function of \\(x_{j}\\) only and find the root, \\(x_{j,t+1}\\) that gives \\(f^{\\prime}_{j}(x_{j,t+1})=0\\). One cycles through each element of \\(x\\) to complete a single cycle and then iterates. The appeal is that univariate root-finding/minimization is easy, often more stable than multivariate, and quick.\nHowever, Gauss-Seidel can zigzag, since you only take steps in one dimension at a time, as we see here. (Again the code is in R.)\nf &lt;- function(x){\n    return(x[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000)\n}\nf1 &lt;- function(x1, x2){ # f(x) as a function of x1\n    return(x1^2/1000 + 4*x1*x2/1000 + 5*x2^2/1000)\n}\nf2 &lt;- function(x2, x1){ # f(x) as a function of x2\n    return(x1^2/1000 + 4*x1*x2/1000 + 5*x2^2/1000)\n}\nx1s &lt;- seq(-5, 8, len = 100); x2s = seq(-5, 2, len = 100)\nfx &lt;- apply(expand.grid(x1s, x2s), 1, f)\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\nnIt &lt;- 49\nxvals &lt;- matrix(NA, nr = nIt, nc = 2)\nxvals[1, ] &lt;- c(7, -4)\n## 5, -10\nfor(t in seq(2, nIt, by = 2)){\n    ## Note that full optimization along each axis is unnecessarily\n    ## expensive (since we are going to just take another step in the next\n    ## iteration. Just using for demonstration here.\n    newx1 &lt;- optimize(f1, x2 = xvals[t-1, 2], interval = c(-40, 40))$minimum\n    xvals[t, ] &lt;- c(newx1, xvals[t-1, 2])\n    newx2 &lt;- optimize(f2, x1 = newx1, interval = c(-40, 40))$minimum\n    xvals[t+1, ] &lt;- c(newx1, newx2)\n}\nlines(xvals)\n\n\n\nCoordinate descent\n\n\nIn the notes for Unit 9 on linear algebra, I discussed the use of Gauss-Seidel to iteratively solve \\(Ax=b\\) in situations where factorizing \\(A\\) (which of course is \\(O(n^{3})\\)) is too computationally expensive.\n\nThe lasso\nThe lasso uses an L1 penalty in regression and related contexts. A standard formulation for the lasso in regression is to minimize \\[\\|Y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{j}|\\beta_{j}|\\] to find \\(\\hat{\\beta}(\\lambda)\\) for a given value of the penalty parameter, \\(\\lambda\\). A standard strategy to solve this problem is to use coordinate descent, either cyclically, or by using directional derivatives to choose the coordinate likely to decrease the objective function the most (a greedy strategy). We need to use directional derivatives because the penalty function is not differentiable, but does have directional derivatives in each direction. The directional derivative of the objective function for \\(\\beta_{j}\\) is \\[-2\\sum_{i}x_{ij}(Y_{i}-X_{i}^{\\top}\\beta)\\pm\\lambda\\] where we add \\(\\lambda\\) if \\(\\beta_{j}\\geq0\\) and you subtract \\(\\lambda\\) if \\(\\beta_{j}&lt;0\\). If \\(\\beta_{j,t}\\) is 0, then a step in either direction contributes \\(+\\lambda\\) to the derivative as the contribution of the penalty.\nOnce we have chosen a coordinate, we set the directional derivative to zero and solve for \\(\\beta_{j}\\) to obtain \\(\\beta_{j,t+1}\\).\nThe glmnet package in R (described in this Journal of Statistical Software paper) implements such optimization for a variety of penalties in linear model and GLM settings, including the lasso. This Mittal et al. paper describes similar optimization for survival analysis with very large \\(p\\), exploiting sparsity in the \\(X\\) matrix for computational efficiency; note that they do not use Newton-Raphson because the matrix operations are infeasible computationally.\nOne nice idea that is used in lasso and related settings is the idea of finding the regression coefficients for a variety of values of \\(\\lambda\\), combined with “warm starts”. A general approach is to start with a large value of \\(\\lambda\\) for which all the coefficients are zero and then decrease \\(\\lambda\\). At each new value of \\(\\lambda\\), use the estimated coefficients from the previous value as the starting values. This should allow for fast convergence and gives what is called the “solution path”. Often \\(\\lambda\\) is chosen based on cross-validation.\nThe LARS (least angle regression) algorithm uses a similar strategy that allows one to compute \\(\\hat{\\beta}_{\\lambda}\\) for all values of \\(\\lambda\\) at once.\nThe lasso can also be formulated as the constrained minimization of \\(\\|Y-X\\beta\\|_{2}^{2}\\) s.t. \\(\\sum_{j}|\\beta_{j}|\\leq c\\), with \\(c\\) now playing the role of the penalty parameter. Solving this minimization problem would take us in the direction of quadratic programming, a special case of convex programming, discussed in Section 9."
  },
  {
    "objectID": "units/unit11-optim.html#nelder-mead",
    "href": "units/unit11-optim.html#nelder-mead",
    "title": "Optimization",
    "section": "Nelder-Mead",
    "text": "Nelder-Mead\nThis approach avoids using derivatives or approximations to derivatives. This makes it robust, but also slower than Newton-like methods. The basic strategy is to use a simplex, a polytope of \\(p+1\\) points in \\(p\\) dimensions (e.g., a triangle when searching in two dimensions, tetrahedron in three dimensions...) to explore the space, choosing to shift, expand, or contract the polytope based on the evaluation of \\(f\\) at the points.\nThe algorithm relies on four tuning factors: a reflection factor, \\(\\alpha&gt;0\\); an expansion factor, \\(\\gamma&gt;1\\); a contraction factor, \\(0&lt;\\beta&lt;1\\); and a shrinkage factor, \\(0&lt;\\delta&lt;1\\). First one chooses an initial simplex: \\(p+1\\) points that serve as the vertices of a convex hull.\n\nEvaluate and order the points, \\(x_{1},\\ldots,x_{p+1}\\) based on \\(f(x_{1})\\leq\\ldots\\leq f(x_{p+1})\\). Let \\(\\bar{x}\\) be the average of the first \\(p\\) \\(x\\)’s.\n(Reflection) Reflect \\(x_{p+1}\\) across the hyperplane (a line when \\(p+1=3\\)) formed by the other points to get \\(x_{r}\\), based on \\(\\alpha\\).\n\n\\(x_{r}=(1+\\alpha)\\bar{x}-\\alpha x_{p+1}\\)\n\nIf \\(f(x_{r})\\) is between the best and worst of the other points, the iteration is done, with \\(x_{r}\\) replacing \\(x_{p+1}\\). We’ve found a good direction to move.\n(Expansion) If \\(f(x_{r})\\) is better than all of the other points, expand by extending \\(x_{r}\\) to \\(x_{e}\\) based on \\(\\gamma\\), because this indicates the optimum may be further in the direction of reflection. If \\(f(x_{e})\\) is better than \\(f(x_{r})\\), use \\(x_{e}\\) in place of \\(x_{p+1}\\). If not, use \\(x_{r}\\). The iteration is done.\n\n\\(x_{e}=\\gamma x_{r}+(1-\\gamma)\\bar{x}\\)\n\nIf \\(f(x_{r})\\) is worse than all the other points, but better than \\(f(x_{p+1})\\), let \\(x_{h}=x_{r}\\). Otherwise \\(f(x_{r})\\) is worse than \\(f(x_{p+1})\\) so let \\(x_{h}=x_{p+1}\\). In either case, we want to concentrate our polytope toward the other points.\n\n(Contraction) Contract \\(x_{h}\\) toward the hyperplane formed by the other points, based on \\(\\beta\\), to get \\(x_{c}\\). If the result improves upon \\(f(x_{h})\\) replace \\(x_{p+1}\\) with \\(x_{c}\\). Basically, we haven’t found a new point that is better than the other points, so we want to contract the simplex away from the bad point.\n\n\\(x_{c}=\\beta x_{h}+(1-\\beta)\\bar{x}\\)\n\n(Shrinkage) Otherwise (if \\(x_{c}\\) is not better than \\(x_{h}\\)), replace \\(x_{p+1}\\) with \\(x_{h}\\) and shrink the simplex toward \\(x_{1}\\). Basically this suggests our step sizes are too large and we should shrink the simplex, shrinking towards the best point.\n\n\\(x_{i}=\\delta x_{i}+(1-\\delta)x_{1}\\) for \\(i=2,\\ldots,p+1\\)\n\n\n\nConvergence is assessed based on the sample variance of the function values at the points, the total of the norms of the differences between the points in the new and old simplexes, or the size of the simplex. In class we’ll work through some demo code (not shown here) that illustrates the individual steps in an iteration of Nelder-Mead.\nWe can see the points at which the function was evaluated in the same quadratic example we saw in previous sections. The left hand panel shows the steps from a starting point somewhat far from the optimum, with the first 9 points numbered. In this case, we start with points 1, 2, and 3. Point 4 is a reflection. At this point, it looks like point 5 is a contraction but that doesn’t exactly follow the algorithm above (since Point 4 is between Points 2 and 3 so the iteration should end without a contraction), so perhaps the algorithm as implemented is a bit different than as described above. In any event, the new set is (2, 3, 4). Then point 6 and point 7 are reflection and expansion steps and the new set is (3, 4, 6). Points 8 and 9 are again reflection and expansion steps. The right hand panel shows the steps from a starting point near (actually at) the optimum. Points 4 and 5 are reflection and expansion steps, with the next set being (1, 2, 5). Now step 6 is a reflection but it is the worst of all the points, so point 7 is a contraction of point 2 giving the next set (1, 5, 7). Point 8 is then a reflection and point 9 is a contraction of point 5.\n(Again some code in R.)\nf &lt;- function(x, plot = TRUE, verbose = FALSE) {\n    result &lt;- x[1]^2/1000 + 4*x[1]*x[2]/1000 + 5*x[2]^2/1000\n    if(verbose) print(result)\n    if(plot && cnt &lt; 10) {\n        points(x[1], x[2], pch = as.character(cnt))\n        if(cnt &lt; 10) cnt &lt;&lt;- cnt + 1 else cnt &lt;&lt;- 1\n        if(interactive())\n            invisible(readline(prompt = \"Press &lt;Enter&gt; to continue...\"))\n    } else if(plot) points(x[1], x[2])\n    return(result)\n}\n\npar(mfrow = c(1,2), mgp = c(1.8,.7,0), mai = c(.5,.45,.1,.5), cex = 0.7)\n\nx1s &lt;- seq(-5, 10, len = 100); x2s = seq(-5, 2, len = 100)\nfx &lt;- apply(expand.grid(x1s, x2s), 1, f, FALSE)\ncnt &lt;- 1\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\ninit &lt;- c(7, -4)\noptim(init, f, method = \"Nelder-Mead\", verbose = FALSE)\n\npar(cex = 0.7)\nx1s &lt;- seq(-.2, .2, len = 100); x2s = seq(-.12, .12, len = 100)\nfx &lt;- apply(expand.grid(x1s, x2s), 1, f, FALSE)\ncnt &lt;- 1\nfields::image.plot(x1s, x2s, matrix(log(fx), 100, 100))\ninit &lt;- c(-0, 0)\noptim(init, f, method = \"Nelder-Mead\", verbose = FALSE)\n\n\n\nNelder-Mead\n\n\nHere’s an online graphical illustration of Nelder-Mead.\nThis is the default in optim() in R. It is an option (by specifying method='Nelder-mead') for scipy.optimize.minimize (BFGS or a variant is the default)."
  },
  {
    "objectID": "units/unit11-optim.html#simulated-annealing-sa-optional",
    "href": "units/unit11-optim.html#simulated-annealing-sa-optional",
    "title": "Optimization",
    "section": "Simulated annealing (SA) (optional)",
    "text": "Simulated annealing (SA) (optional)\nSimulated annealing is a stochastic descent algorithm, unlike the deterministic algorithms we’ve already discussed. It has a couple critical features that set it aside from other approaches. First, uphill moves are allowed; second, whether a move is accepted is stochastic, and finally, as the iterations proceed the algorithm becomes less likely to accept uphill moves.\nAssume we are minimizing a negative log likelihood as a function of \\(\\theta\\), \\(f(\\theta)\\).\nThe basic idea of simulated annealing is that one modifies the objective function, \\(f\\) in this case, to make it less peaked at the beginning, using a “temperature” variable that changes over time. This helps to allow moves away from local minima, when combined with the ability to move uphill. The name comes from an analogy to heating up a solid to its melting temperature and cooling it slowly - as it cools the atoms go through rearrangements and slowly freeze into the crystal configuration that is at the lowest energy level.\nHere’s the algorithm. We divide up iterations into stages, \\(j=1,2,\\ldots\\) in which the temperature variable, \\(\\tau_{j}\\), is constant. Like MCMC, we require a proposal distribution to propose new values of \\(\\theta\\).\n\nPropose to move from \\(\\theta_{t}\\) to \\(\\tilde{\\theta}\\) from a proposal density, \\(g_{t}(\\cdot|\\theta_{t})\\), such as a normal distribution centered at \\(\\theta_{t}\\).\nAccept \\(\\tilde{\\theta}\\) as \\(\\theta_{t+1}\\) according to the probability \\(\\min(1,\\exp((f(\\theta_{t})-f(\\tilde{\\theta}))/\\tau_{j})\\) - i.e., accept if a uniform random deviate is less than that probability. Otherwise set \\(\\theta_{t+1}=\\theta_{t}\\). Notice that for larger values of \\(\\tau_{j}\\) the differences between the function values at the two locations are reduced (just like a large standard deviation spreads out a distribution). So the exponentiation smooths out the objective function when \\(\\tau_{j}\\) is large.\nRepeat steps 1 and 2 \\(m_{j}\\) times.\nIncrement the temperature and cooling schedule: \\(\\tau_{j}=\\alpha(\\tau_{j-1})\\) and \\(m_{j}=\\beta(m_{j-1})\\). Back to step 1.\n\nThe temperature should slowly decrease to 0 while the number of iterations, \\(m_{j}\\), should be large. Choosing these ‘schedules’ is at the core of implementing SA. Note that we always accept downhill moves in step 2 but we sometimes accept uphill moves as well.\nFor each temperature, SA produces an MCMC based on the Metropolis algorithm. So if \\(m_{j}\\) is long enough, we should sample from the stationary distribution of the Markov chain, \\(\\exp(-f(\\theta)/\\tau_{j}))\\). Provided we can move between local minima, the chain should gravitate toward the global minima because these are increasingly deep (low values) relative to the local minima as the temperature drops. Then as the temperature cools, \\(\\theta_{t}\\) should get trapped in an increasingly deep well centered on the global minimum. There is a danger that we will get trapped in a local minimum and not be able to get out as the temperature drops, so the temperature schedule is quite important in trying to avoid this.\nA wide variety of schedules have been tried. One approach is to set \\(m_{j}=1\\forall j\\) and \\(\\alpha(\\tau_{j-1})=\\frac{\\tau_{j-1}}{1+a\\tau_{j-1}}\\) for a small \\(a\\). For a given problem it can take a lot of experimentation to choose \\(\\tau_{0}\\) and \\(m_{0}\\) and the values for the scheduling functions. For the initial temperature, it’s a good idea to choose it large enough that \\(\\exp((f(\\theta_{i})-f(\\theta_{j}))/\\tau_{0})\\approx1\\) for any pair \\(\\{\\theta_{i},\\theta_{j}\\}\\) in the domain, so that the algorithm can visit the entire space initially.\nSimulated annealing can converge slowly. Multiple random starting points or stratified starting points can be helpful for finding a global minimum. However, given the slow convergence, these can also be computationally burdensome."
  },
  {
    "objectID": "units/unit11-optim.html#core-optimization-functions",
    "href": "units/unit11-optim.html#core-optimization-functions",
    "title": "Optimization",
    "section": "Core optimization functions",
    "text": "Core optimization functions\nScipy provides various useful optimization functions via scipy.optimize, including many of the algorithms discussed in this unit.\n\nminimize_scalar implements golden section search (golden) and interpolation combined with golden section search (brent, akin to optimize in R).\nminimize implements various methods for multivariate optimization including Nelder-Mead, BFGS and conjugate gradients (discussed a bit in Unit 9). You can choose which method you prefer and can try multiple methods. You can supply a gradient function for use with the Newton-related methods but it can also calculate numerical derivatives on the fly. You can have the BFGS implementation in minimize return the Hessian at the optimum (based on a numerical estimate), which then allows straighforward calculation of asymptotic variances based on the information matrix.\nOne can provide a variety of nonlinear, linear, and simple bounds constraints as well, though certain types of constraints can only be used with certain algorithms.\n\nIn the demo code (not shown here; see the source qmd file), we’ll work our way through a real example of optimizing a likelihood for some climate data on extreme precipitation.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_file = os.path.join('..', 'data', 'precipData.txt')\ny_hundredths = np.genfromtxt(data_file, missing_values = 'NA')  # precip in hundredths of inches\ny_hundredths = y_hundredths[~np.isnan(y_hundredths)]\ny = y_hundredths / 100  # precip now in inches\n\nnpy=31+28+31 # number of days in winter season\ncutoff = 1 / 25.4  # Convert 1 mm to inches\nthresh = np.percentile(y[y &gt; cutoff], 98,)\n\n# Create a histogram of the data\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(y, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Precipitation (inches)')\nplt.ylabel('Frequency')\nplt.title('Histogram of All Data')\n\nplt.subplot(1, 2, 2)\nplt.hist(y[y &gt; thresh], bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Precipitation (inches)')\nplt.ylabel('Frequency')\nplt.title(f'Histogram of Wet Days (Precip &gt; {np.round(thresh,2)} inches)')\nplt.tight_layout()\nplt.show()\n\n# Define objective (negative log-likelihood) function\n\ndef pp_negloglik(par, y, thresh, npy):\n    mu, sc, sh = par\n    uInd = y &gt; thresh\n    \n    # Invalid parameter values or data/parameter combos:\n    if sc &lt;= 0:\n        return 1e6\n    if (1 + ((sh * (thresh - mu)) / sc) &lt; 0):\n        return 1e6\n        \n    y = (y - mu) / sc\n    y = 1 + sh * y\n    \n    if np.min(y[uInd]) &lt;= 0:\n        return 1e6\n    else:\n        ytmp = y.copy()\n        ytmp[~uInd] = 1 # 'zeroes' out those below the threshold after applying the log in next line\n        \n        l = np.sum(uInd * np.log(sc)) + np.sum(uInd * np.log(ytmp) * (1 / sh + 1)) + \\\n                   (len(y) / npy) * np.mean((1 + (sh * (thresh - mu)) / sc) ** (-1 / sh))\n                   \n    return l\n\n\n# Initial parameter values\ny_exc = y[y &gt; thresh]\nin2 = np.sqrt(6 * np.var(y_exc)) / np.pi\nin1 = np.mean(y_exc) - 0.57722 * in2\ninit0 = [in1, in2, 0.1]\n\n# Optimization using Nelder-Mead\nstart_time = time.time()\nfit1 = minimize(pp_negloglik, init0, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nend_time = time.time()\nprint(\"Nelder-Mead Optimization:\")\nprint(fit1)\nprint(\"Execution Time:\", end_time - start_time, \"seconds\")\n\n# Optimization using BFGS\nstart_time = time.time()\nfit2 = minimize(pp_negloglik, init0, args=(y, thresh, npy), method='BFGS', options={'disp': True})\nend_time = time.time()\nprint(\"\\nBFGS Optimization:\")\nprint(fit2)\nprint(\"Execution Time:\", end_time - start_time, \"seconds\")\n\nmle = fit2.x\nvar = fit2.hess_inv\nse = np.sqrt(np.diag(fit2.hess_inv))\n\n# Different starting values\ninit1 = [np.mean(y[y &gt; thresh]), np.std(y[y &gt; thresh]), -0.1]\nfit1a = minimize(pp_negloglik, init1, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nfit2a = minimize(pp_negloglik, init1, args=(y, thresh, npy), method='BFGS', options={'disp': True})\n\n# Bad starting value for BFGS\ninit2 = [thresh, 0.01, .5]\nfit1b = minimize(pp_negloglik, init2, args=(y, thresh, npy), method='Nelder-Mead', options={'disp': True})\nfit2b = minimize(pp_negloglik, init2, args=(y, thresh, npy), method='BFGS', options={'disp': True})\nfit2b.hess_inv\n\n# Data on a different scale\ny_exc2 = y[y &gt; thresh] * 1000\ny2 = y * 1000\nthresh2 = thresh * 1000\n\ninit3 = [np.mean(y_exc2), np.std(y_exc2), 0.1]\nfit3 = minimize(pp_negloglik, init3, args=(y2, thresh2, npy), method='Nelder-Mead', options={'disp': True})\nfit4 = minimize(pp_negloglik, init3, args=(y2, thresh2, npy), method='BFGS', options={'disp': True})\nfit4.hess_inv\n\n# Plot the objective function\nn_grid = 30\nloc_vals = np.linspace(0, 5, n_grid)\nscale_vals = np.linspace(0.1, 3, n_grid)\nshape_vals = np.linspace(-0.3, 0.3, 16)\n\nloc_grid, scale_grid, shape_grid = np.meshgrid(loc_vals, scale_vals, shape_vals, indexing='ij')\npar_grid = np.column_stack((loc_grid.ravel(), scale_grid.ravel(), shape_grid.ravel()))\nobj = np.apply_along_axis(pp_negloglik, 1, par_grid, y=y, thresh=thresh, npy=npy)\n\nfig, axes = plt.subplots(4, 4, figsize=(14, 10))\nfor i, shape_value in enumerate(shape_vals):\n    ax = axes[i // 4, i % 4]\n    obj_matrix = np.array(obj[par_grid[:,2] == shape_value]).reshape(n_grid, n_grid).T\n    im = ax.imshow(obj_matrix, extent=(0, 5, 0.1, 3), cmap='viridis', vmin=40, vmax=80, aspect='auto', origin='lower')\n    ax.set_title(f\"shape = {shape_value:.2f}\")\n\nfig.colorbar(im, ax=axes, label=\"Objective Function Value\")\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "units/unit11-optim.html#various-considerations-in-using-the-python-functions",
    "href": "units/unit11-optim.html#various-considerations-in-using-the-python-functions",
    "title": "Optimization",
    "section": "Various considerations in using the Python functions",
    "text": "Various considerations in using the Python functions\nAs we’ve seen, initial values are important both for avoiding divergence (e.g., in N-R), for increasing speed of convergence, and for helping to avoid local optima. So it is well worth the time to try to figure out a good starting value or multiple starting values for a given problem.\nScaling can be important. One useful step is to make sure the problem is well-scaled, namely that a unit step in any parameter has a comparable change in the objective function, preferably approximately a unit change at the optimum. Basically if \\(x_{j}\\) is varying at \\(p\\) orders of magnitude smaller than the other \\(x\\)s, we want to reparameterize to \\(x_{j}^{*}=x_{j}\\cdot10^{p}\\) and then convert back to the original scale after finding the answer. Or we may want to work on the log scale for some variables, reparameterizing as \\(x_{j}^{*}=\\log(x_{j})\\).\nAs far as I can see one needs to do this manually with minimize in Python but optim() in R allows you to supply scaling information through the parscale component of the control argument.\nIf the function itself gives very large or small values near the solution, you may want to rescale the entire function to avoid calculations with very large or small numbers. This can avoid problems such as having apparent convergence because a gradient is near zero, simply because the scale of the function is small. In optim() in R, this can be controlled with the fnscale component of control.\nAlways consider your answer and make sure it makes sense, in particular that you haven’t ‘converged’ to an extreme value on the boundary of the space.\nVenables and Ripley suggest that it is often worth supplying analytic first derivatives rather than having a routine calculate numerical derivatives but not worth supplying analytic second derivatives. One possibility is using software such as Mathematica to do symbolic (i.e., analytic) differentiation and then writing code to implement the math of the result. Another is using software that can give derivatives using automatic differentiation such as PyTorch, jax and tensorflow. We saw an example of using Jax earlier in the unit.\nIn general for software development it’s obviously worth putting more time into figuring out the best optimization approach and supplying derivatives. For a one-off analysis, you can try a few different approaches and assess sensitivity.\nThe nice thing about likelihood optimization is that the asymptotic theory tells us that with large samples, the likelihood is approximately quadratic (i.e., the asymptotic normality of MLEs), which makes for a nice surface over which to do optimization. When optimizing with respect to variance components and other parameters that are non-negative, one approach to dealing with the constraints is to optimize with respect to the log of the parameter."
  },
  {
    "objectID": "units/unit11-optim.html#mm-algorithm",
    "href": "units/unit11-optim.html#mm-algorithm",
    "title": "Optimization",
    "section": "MM algorithm",
    "text": "MM algorithm\nThe MM algorithm is really more of a principle for constructing problem specific algorithms. MM stands for majorize-minorize. We’ll use the majorize part of it to minimize functions - the minorize part is the counterpart for maximizing functions.\nSuppose we want to minimize a convex function, \\(f(x)\\). The idea is to construct a majorizing function, at \\(x_{t}\\), which we’ll call \\(g\\). \\(g\\) majorizes \\(f\\) at \\(x_{t}\\) if \\(f(x_{t})=g(x_{t})\\) and \\(f(x)\\leq g(x)\\forall x\\).\nThe iterative algorithm is as follows. Given \\(x_{t}\\), construct a majorizing function \\(g_{t}(x).\\) Then minimize \\(g_{t}\\) w.r.t. \\(x\\) (or at least move downhill, such as with a modified Newton step) to find \\(x_{t+1}\\). Then we iterate, finding the next majorizing function, \\(g_{t+1}(x)\\). The algorithm is obviously guaranteed to go downhill, and ideally we use a function \\(g\\) that is easy to work with (i.e., to minimize or go downhill with respect to). Note that we haven’t done any matrix inversions or computed any derivatives of \\(f\\). Furthermore, the algorithm is numerically stable - it does not over- or undershoot the optimum. The downside is that convergence can be quite slow.\nThe tricky part is finding a good majorizing function. Basically one needs to gain some skill in working with inequalities. The Lange book has some discussion of this.\nAn example is for estimating regression coefficients for median regression (aka least absolute deviation regression), which minimizes \\(f(\\theta)=\\sum_{i=1}^{n}|y_{i}-z_{i}^{\\top}\\theta|=\\sum_{i=1}^{n}|r_{i}(\\theta)|\\). Note that \\(f(\\theta)\\) is convex because affine functions (in this case \\(y_{i}-z_{i}^{\\top}\\theta\\)) are convex, convex functions of affine functions are convex, and the summation preserves the convexity. We want to minimize \\[\\begin{aligned}\nf(\\theta) & = \\sum_{i=1}^{n}|r_{i}(\\theta)|\\\\\n& = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta)^{2}}\\end{aligned}\\]\nNext, \\(h(x)=\\sqrt{x}\\) is concave, so we can use the following (commonly-used) inequality, \\(h(x)\\leq h(y)+h^{\\prime}(y)(x-y)\\) which holds for any concave function, \\(h\\), and note that we have equality when \\(y=x\\). For \\(y=\\theta_{t}\\), the current value in the iterative optimization, we have: \\[\\begin{aligned}\nf(\\theta) & = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta)^{2}}\\\\\n& \\leq \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{r_{i}(\\theta)^{2}-r_{i}(\\theta_{t})^{2}}{2\\sqrt{r_{i}(\\theta_{t})^{2}}}\\\\\n& = g_{t}(\\theta)\\end{aligned}\\] where the term on the right of the second equation is our majorizing function \\(g(\\theta)\\) for the current \\(\\theta_{t}\\). We then have \\[\\begin{aligned}\ng_{t}(\\theta) & = \\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{1}{2}\\sum_{i=1}^{n}\\frac{r_{i}(\\theta)^{2}-r_{i}(\\theta_{t})^{2}}{2\\sqrt{r_{i}(\\theta_{t})^{2}}}\\\\\n& = \\frac{1}{2}\\sum_{i=1}^{n}\\sqrt{r_{i}(\\theta_{t})^{2}}+\\frac{1}{2}\\sum_{i=1}^{n}\\frac{r_{i}(\\theta)^{2}}{\\sqrt{r_{i}(\\theta_{t})^{2}}}\\end{aligned}\\] Our job in this iteration of the algorithm is to minimize \\(g\\) with respect to \\(\\theta\\) (recall that \\(\\theta_{t}\\) is a fixed value), so we can ignore the first sum, which doesn’t involve \\(\\theta\\). Minimizing the second sum can be seen as a weighted least squares problem, where the numerator is the usual sum of squared residuals and the weights are \\(w_{i}=\\frac{1}{\\sqrt{(y_{i}-z_{i}^{\\top}\\theta_{t})^{2}}}\\). Intuitively this makes sense: the weight is large when the magnitude of the residual is small this makes up for the fact that we are using least squares when we want to mimimize absolute deviations. So our update is: \\[\\theta_{t+1}=(Z^{\\top}W(\\theta_{t})Z)^{-1}Z^{\\top}W(\\theta_{t})Y,\\] where \\(W(\\theta_{t})\\) is a diagonal matrix with elements \\(w_{1},\\ldots,w_{n}.\\)\nAs usual, we want to think about what could go wrong numerically. If we have some very small magnitude residuals, they will get heavily upweighted in this procedure, which might cause instability in our optimization.\nFor an example of MM being used in practice for a real problem, see Jung et al. (2014): Biomarker Detection in Association Studies: Modeling SNPs Simultaneously via Logistic ANOVA, Journal of the American Statistical Association 109:1355."
  },
  {
    "objectID": "units/unit11-optim.html#expectation-maximization-em",
    "href": "units/unit11-optim.html#expectation-maximization-em",
    "title": "Optimization",
    "section": "Expectation-Maximization (EM)",
    "text": "Expectation-Maximization (EM)\nIt turns out the EM algorithm that many of you have heard about is a special case of MM. For our purpose here, we’ll consider maximization.\nThe EM algorithm is most readily motivated from a missing data perspective. Suppose you want to maximize \\(L(\\theta|x)=f(x;\\theta)\\) based on available data in a missing data context. Denote the complete data as \\(Y=(X,Z)\\) with \\(Z\\) is missing. As we’ll see, in many cases, \\(Z\\) is actually a set of latent variables that we introduce into the problem to formulate it so we can use EM. The canonical example is when \\(Z\\) are membership indicators in a mixture modeling context. (Note that in the case where you introduce \\(Z\\), that also means that one could also just directly maximize \\(L(\\theta|x)\\), which in many cases may work better than using the EM algorithm.)\nIn general, \\(\\log L(\\theta;x)\\) may be hard to optimize because it involves an integral over the missing data, \\(Z\\): \\[f(x;\\theta)=\\int f(x,z;\\theta)dz,\\] but the EM algorithm provides a recipe that makes the optimization straightforward for many problems.\nThe algorithm is as follows. Let \\(\\theta^{t}\\) be the current value of \\(\\theta\\). Then define \\[Q(\\theta|\\theta^{t})=E(\\log L(\\theta|Y)|x;\\theta^{t})\\].\nThat expectation is an expectation with respect to the conditional distribution, $ f(z|x; = ^t)$.\nThe algorithm is\n\nE step: Compute \\(Q(\\theta|\\theta^{t})\\), ideally calculating the expectation over the missing data in closed form. Note that \\(\\log L(\\theta|Y)\\) is a function of \\(\\theta\\) so \\(Q(\\theta|\\theta^{t})\\) will involve both \\(\\theta\\) and \\(\\theta^{t}\\).\nM step: Maximize \\(Q(\\theta|\\theta^{t})\\) with respect to \\(\\theta\\), finding \\(\\theta^{t+1}\\).\nContinue until convergence.\n\nIdeally both the E and M steps can be done analytically. When the M step cannot be done analytically, one can employ some of the numerical optimization tools we’ve already seen. When the E step cannot be done analytically, one standard approach is to estimate the expectation by Monte Carlo, which produces Monte Carlo EM (MCEM). The strategy is to draw from \\(z_{j}\\) from \\(f(z|x,\\theta^{t})\\) and approximate \\(Q\\) as a Monte Carlo average of \\(\\log f(x,z_{j};\\theta)\\), and then optimize over this approximation to the expectation. If one can’t draw in closed form from the conditional density, one strategy is to do a short MCMC to draw a (correlated) sample.\nEM can be show to increase the value of the function at each step using Jensen’s inequality (equivalent to the information inequality that holds with regard to the Kullback-Leibler divergence between two distributions) (Givens and Hoeting, p. 95, go through the details). Furthermore, one can show that it amounts, at each step, to maximizing a minorizing function for \\(\\log L(\\theta)\\) - the minorizing function (effectively \\(Q\\)) is tangent to \\(\\log L(\\theta)\\) at \\(\\theta^{t}\\) and lies below \\(\\log L(\\theta)\\).\nA standard example is a mixture model. (Here we’ll assume a mixture of normal distributions, but other distributions could be used.) Therefore we have \\[f(x;\\theta)=\\sum_{k=1}^{K}\\pi_{k}f_{k}(x;\\mu_{k},\\sigma_{k})\\] where we have \\(K\\) mixture components and \\(\\pi_{k}\\) are the (marginal) probabilities of being in each component. The complete parameter vector is \\(\\theta=\\{\\{\\pi_{k}\\},\\{\\mu_{k}\\},\\{\\sigma_{k}\\}\\}\\). Note that the likelihood is a complicated product (over observations) over the sum (over components), so maximization may be difficult. Furthermore, such likelihoods are well-known to be multimodal because of label switching.\nTo use EM, we take the group membership indicators for each observation as the missing data. For the \\(i\\)th observation, we have \\(z_{i}\\in\\{1,2,\\ldots,K\\}\\). Introducing these indicators “breaks the mixture”. If we know the memberships for all the observations, it’s often easy to estimate the parameters for each group based on the observations from that group. For example if the \\(\\{f_{k}\\}\\)’s were normal densities, then we can estimate the mean and variance of each normal density using the sample mean and sample variance of the \\(x_{i}\\)’s that belong to each mixture component. EM will give us a variation on this that uses “soft” (i.e., probabilistic) weighting.\nThe complete log likelihood given \\(z\\) and \\(x\\) is \\[\\log\\prod_{i}f(x_{i}|z_{i};\\theta)\\mbox{Pr}(Z_{i}=z_{i};\\theta)\\] which can be expressed as\n\n\\[\\begin{aligned}\n\\log L(\\theta|x,z) & = & \\sum_{i}\\log f(x_{i};\\mu_{z_{i}},\\sigma_{z_{i}})+\\log\\pi_{z_{i}}\\\\\n& = & \\sum_{i}\\sum_{k}I(z_{i}=k)(\\log f_{k}(x_{i};\\mu_{k},\\sigma_{k})+\\log\\pi_{k})\\end{aligned}\\] with \\(Q\\) equal to \\[Q(\\theta|\\theta^{t})=\\sum_{i}\\sum_{k}E(I(z_{i}=k)|x_{i};\\theta^{t})(\\log f_{k}(x_{i};\\mu_{k},\\sigma_{k})+\\log\\pi_{k})\\] where \\(E(I(z_{i}=k)|x_{i};\\theta^{t})\\) is equal to the probability that the \\(i\\)th observation is in the \\(k\\)th group given \\(x_{i}\\) and \\(\\theta_{t}\\), which is calculated from Bayes theorem as \\[p_{ik}^{t}=\\frac{\\pi_{k}^{t}f_{k}(x_{i};\\mu_{k}^{t},\\sigma_{k}^{t})}{\\sum_{j}\\pi_{j}^{t}f_{j}(x_{i};\\mu_{k}^{t},\\sigma_{k}^{t})}\\] We can now separately maximize \\(Q(\\theta|\\theta^{t})\\) with respect to \\(\\pi_{k}\\) and \\(\\mu_{k},\\sigma_{k}\\) to find \\(\\pi_{k}^{t+1}\\) and \\(\\mu_{k}^{t+1},\\sigma_{k}^{t+1}\\), since the expression is the sum of a term involving the parameters of the distributions and a term involving the mixture probabilities. In the latter case, if the \\(f_{k}\\) are normal distributions, you end up with a weighted sum of normal distributions, for which the estimators of the mean and variance parameters are the weighted mean of the observations and the weighted variance."
  },
  {
    "objectID": "units/unit11-optim.html#convex-optimization-convex-programming",
    "href": "units/unit11-optim.html#convex-optimization-convex-programming",
    "title": "Optimization",
    "section": "Convex optimization (convex programming)",
    "text": "Convex optimization (convex programming)\nConvex programming minimizes \\(f(x)\\) s.t. \\(h_{j}(x)\\leq0,\\,j=1,\\ldots,m\\) and \\(a_{i}^{\\top}x=b_{i},\\,i=1,\\ldots,q\\), where both \\(f\\) and the constraint functions are convex. Note that this includes more general equality constraints, as we can write \\(g(x)=b\\) as two inequalities \\(g(x)\\leq b\\) and \\(g(x)\\geq b\\). It also includes \\(h_{j}(x)\\geq b_{j}\\) by taking \\(-h_{j}(x)\\). Note that we can always have \\(h_{j}(x)\\leq b_{j}\\) and convert to the above form by subtracting \\(b_{j}\\) from each side (note that this preserves convexity). A vector \\(x\\) is said to be feasible, or in the feasible set, if all the constraints are satisfied for \\(x\\).\nThere are good algorithms for convex programming, and it’s possible to find solutions when we have hundreds or thousands of variables and constraints. It is often difficult to recognize if one has a convex program (i.e., if \\(f\\) and the constraint functions are convex), but there are many tricks to transform a problem into a convex program and many problems can be solved through convex programming. So the basic challenge is in recognizing or transforming a problem to one of convex optimization; once you’ve done that, you can rely on existing methods to find the solution.\nLinear programming, quadratic programming, second order cone programming and semidefinite programming are all special cases of convex programming. In general, these types of optimization are progressively more computationally complex.\nFirst let’s see some of the special cases and then discuss the more general problem."
  },
  {
    "objectID": "units/unit11-optim.html#linear-programming-linear-system-linear-constraints",
    "href": "units/unit11-optim.html#linear-programming-linear-system-linear-constraints",
    "title": "Optimization",
    "section": "Linear programming: Linear system, linear constraints",
    "text": "Linear programming: Linear system, linear constraints\nLinear programming seeks to minimize \\[f(x)=c^{\\top}x\\] subject to a system of \\(m\\) inequality constraints, \\(a_{i}^{\\top}x\\leq b_{i}\\) for \\(i=1,\\ldots,m\\), where \\(A\\) is of full row rank. This can also be written in terms of generalized inequality notation, \\(Ax\\preceq b\\). There are standard algorithms for solving linear programs, including the simplex method and interior point methods.\nNote that each equation in the set of equations \\(Ax=b\\) defines a hyperplane, so each inequality in \\(Ax\\preceq b\\) defines a half-space. Minimizing a linear function (presuming that the minimum exists) must mean that we push in the correct direction towards the boundaries formed by the hyperplanes, with the solution occuring at a corner (vertex) of the solid formed by the hyperplanes. The simplex algorithm starts with a feasible solution at a corner and moves along edges in directions that improve the objective function."
  },
  {
    "objectID": "units/unit11-optim.html#general-system-equality-constraints",
    "href": "units/unit11-optim.html#general-system-equality-constraints",
    "title": "Optimization",
    "section": "General system, equality constraints",
    "text": "General system, equality constraints\nSuppose we have an objective function \\(f(x)\\) and we have equality constraints, \\(Ax=b\\). We can manipulate this into an unconstrained problem. The null space of \\(A\\) is the set of \\(\\delta\\) s.t. \\(A\\delta=0\\). So if we start with a candidate \\(x_{c}\\) s.t. \\(Ax_{c}=b\\) (e.g., by using the pseudo inverse, \\(A^{+}b\\)), we can form all other candidates (a candidate is an \\(x\\) s.t. \\(Ax=b\\)) as \\(x=x_{c}+\\delta=x_{c}+Bz\\) where \\(B\\) is a set of column basis functions for the null space of \\(A\\) and \\(z\\in\\Re^{p-m}\\). Consider \\(h(z)=f(x_{c}+Bz)\\) and note that \\(h\\) is a function of \\(p-m\\) rather than \\(p\\) inputs. Namely, we are working in a reduced dimension space with no constraints. If we assume differentiability of \\(f\\), we can express \\(\\nabla h(z)=B^{\\top}\\nabla f(x_{c}+Bz)\\) and \\(H_{h}(z)=B^{\\top}H_{f}(x_{c}+Bz)B\\). Then we can use unconstrained methods to find the point at which \\(\\nabla h(z)=0\\).\nHow do we find \\(B\\)? One option is to use the \\(p-m\\) columns of \\(V\\) in the SVD of \\(A\\) that correspond to singular values that are zero. A second option is to take the QR decomposition of \\(A^{\\top}\\). Then \\(B\\) is the columns of \\(Q_{2}\\), where these are the columns of the (non-skinny) Q matrix corresponding to the rows of \\(R\\) that are zero.\nFor more general (nonlinear) equality constraints, \\(g_{i}(x)=b_{i}\\), \\(i=1,\\ldots,q\\), we can use the Lagrange multiplier approach to define a new objective function, \\[L(x,\\lambda)=f(x)+\\lambda^{\\top}(g(x)-b)\\] for which, if we set the derivative (with respect to both \\(x\\) and the Lagrange multiplier vector, \\(\\lambda\\)) equal to zero, we have a critical point of the original function and we respect the constraints.\nAn example occurs with quadratic programming, under the simplification of affine equality constraints (quadratic programming in general optimizes a quadratic function under affine inequality constraints - i.e., constraints of the form \\(Ax-b\\preceq0\\)). For example we might solve a least squares problem subject to linear equality constraints, \\(f(x)=\\frac{1}{2}x^{\\top}Qx+m^{\\top}x+c\\) s.t. \\(Ax=b\\), where \\(Q\\) is positive semi-definite. The Lagrange multiplier approach gives the objective function \\[L(x,\\lambda)=\\frac{1}{2}x^{\\top}Qx+m^{\\top}x+c+\\lambda^{\\top}(Ax-b)\\] and differentiating gives the equations \\[\\begin{aligned}\n\\frac{\\partial L(x,\\lambda)}{\\partial x} & =m+Qx+A^{\\top}\\lambda  =  0\\\\\n\\frac{\\partial L(x,\\lambda)}{\\partial\\lambda} & =Ax  =  b,\\end{aligned}\\] which gives us a system of equations that leads to the solution \\[\\left(\\begin{array}{c} x \\\\ \\lambda \\end{array} \\right) =\n\\left( \\begin{array}{cc} Q & A^{\\top} \\\\ A & 0 \\end{array}\\right)^{-1}\n\\left(\\begin{array}{c} -m \\\\ b \\end{array}\\right).\\label{eq:quadProg}\\]\nUsing known results for inverses of matrices split into blocks, one gets that \\(x^{*}=-Q^{-1}m+Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}(AQ^{-1}m+b)\\). This can be readily coded up using strategies from Unit 10."
  },
  {
    "objectID": "units/unit11-optim.html#the-dual-problem-optional",
    "href": "units/unit11-optim.html#the-dual-problem-optional",
    "title": "Optimization",
    "section": "The dual problem (optional)",
    "text": "The dual problem (optional)\nSometimes a reformulation of the problem eases the optimization. There are different kinds of dual problems, but we’ll just deal with the Lagrangian dual. Let \\(f(x)\\) be the function we want to minimize, under constraints \\(g_{i}(x)=0;\\,i=1,\\ldots,q\\) and \\(h_{j}(x)\\leq0;\\,j=1,\\ldots,m\\). Here I’ve explicitly written out the equality constraints to follow the notation in Lange. Consider the Langrangian, \\[L(x,\\lambda,\\mu)=f(x)+\\sum_{i}\\lambda_{i}g_{i}(x)+\\sum_{j}\\mu_{j}h_{j}(x).\\]\nSolving that can be shown to be equivalent to this optimization: \\[\\inf_{x}\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}L(x,\\lambda,\\mu)\\] where the supremum ensures that the constraints are satisfied because the Lagrangian is infinity if the constraints are not satisfied.\nLet’s consider interchanging the minimization and maximization. For \\(\\mu\\succeq0\\), one can show that \\[\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}\\inf_{x}L(x,\\lambda,\\mu)\\leq\\inf_{x}\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}L(x,\\lambda,\\mu),\\] because \\(\\inf_{x}L(x,\\lambda,\\mu)\\leq f(x^{*})\\) for the minimizing value \\(x^{*}\\) (p. 216 of the Boyd book). This gives us the Lagrange dual function: \\[d(\\lambda,\\mu)=\\inf_{x}L(x,\\lambda,\\mu),\\] and the Lagrange dual problem is to find the best lower bound: \\[\\sup_{\\lambda,\\mu:\\mu_{j}\\geq0}d(\\lambda,\\mu).\\]\nThe dual problem is always a convex optimization problem because \\(d(\\lambda,\\mu)\\) is concave (because \\(d(\\lambda,\\mu)\\) is a pointwise infimum of a family of affine functions of \\((\\lambda,\\mu)\\)). If the optima of the primal (original) problem and that of the dual do not coincide, there is said to be a “duality gap”. For convex programming, if certain conditions are satisfied (called constraint qualifications), then there is no duality gap, and one can solve the dual problem to solve the primal problem. Usually with the standard form of convex programming, there is no duality gap. Provided we can do the minimization over \\(x\\) in closed form we then maximize \\(d(\\lambda,\\mu)\\) w.r.t. the Lagrangian multipliers in a new constrained problem that is sometimes easier to solve, giving us \\((\\lambda^{*},\\mu^{*})\\).\nOne can show (p. 242 of the Boyd book) that \\(\\mu_{i}^{*}=0\\) unless the \\(i\\)th constraint is active at the optimum \\(x^{*}\\) and that \\(x^{*}\\) minimizes \\(L(x,\\lambda^{*},\\mu^{*})\\). So once one has \\((\\lambda^{*},\\mu^{*})\\), one is in the position of minimizing an unconstrained convex function. If \\(L(x,\\lambda^{*},\\mu^{*})\\) is strictly convex, then \\(x^{*}\\) is the unique optimum provided \\(x^{*}\\) satisfies the constraints, and no optimum exists if it does not.\nHere’s a simple example: suppose we want to minimize \\(x^{\\top}x\\) s.t. \\(Ax=b\\). The Lagrangian is \\(L(x,\\lambda)=x^{\\top}x+\\lambda^{\\top}(Ax-b)\\). Since \\(L(x,\\lambda)\\) is quadratic in \\(x\\), the infimum is found by setting \\(\\nabla_{x}L(x,\\lambda)=2x+A^{\\top}\\lambda=0\\), yielding \\(x=-\\frac{1}{2}A^{\\top}\\lambda\\). So the dual function is obtained by plugging this value of \\(x\\) into \\(L(x,\\lambda)\\), which gives \\[d(\\lambda)=-\\frac{1}{4}\\lambda^{\\top}AA^{\\top}\\lambda-b^{\\top}\\lambda,\\] which is concave quadratic. In this case we can solve the original constrained problem in terms of this unconstrained dual problem.\nAnother example is the primal and dual forms for finding the SVM classifier (see the Wikipedia article). In this algorithm, we want to develop a classifier using \\(n\\) pairs of \\(y\\in\\Re^{1}\\) and \\(x\\in\\Re^{p}\\). The dual form is easily derived because the minimization over \\(x\\) occurs in a function that is quadratic in \\(x\\). Expressing the problem in the primal form gives an optimization in \\(\\Re^{p}\\) while doing so in the dual form gives an optimization in \\(\\Re^{n}\\). So one reason to use the dual form would be if you have \\(n\\ll p\\)."
  },
  {
    "objectID": "units/unit11-optim.html#kkt-conditions-optional",
    "href": "units/unit11-optim.html#kkt-conditions-optional",
    "title": "Optimization",
    "section": "KKT conditions (optional)",
    "text": "KKT conditions (optional)\nKarush-Kuhn-Tucker (KKT) theory provides sufficient conditions under which a constrained optimization problem has a minimum, generalizing the Lagrange multiplier approach. The Lange and Boyd books have whole sections on this topic.\nSuppose that the function and the constraint functions are continuously differentiable near \\(x^{*}\\) and that we have the Lagrangian as before: \\[L(x,\\lambda,\\mu)=f(x)+\\sum_{i}\\lambda_{i}g_{i}(x)+\\sum_{j}\\mu_{j}h_{j}(x).\\]\nFor nonconvex problems, if \\(x^{*}\\) and \\((\\lambda^{*},\\mu^{*})\\) are the primal and dual optimal points and there is no duality gap, then the KKT conditions hold: \\[\\begin{aligned}\nh_{j}(x^{*}) & \\leq & 0\\\\\ng_{i}(x^{*}) & = & 0\\\\\n\\mu_{j}^{*} & \\geq & 0\\\\\n\\mu_{j}^{*}h_{j}(x^{*}) & = & 0\\\\\n\\nabla f(x^{*})+\\sum_{i}\\lambda_{i}^{*}\\nabla g_{i}(x^{*})+\\sum_{j}\\mu_{j}^{*}\\nabla h_{j}(x^{*}) & = & 0.\\end{aligned}\\]\nFor convex problems, we also have that if the KKT conditions hold, then \\(x^{*}\\) and \\((\\lambda^{*},\\mu^{*})\\) are primal and dual optimal and there is no duality gap.\nWe can consider this from a slightly different perspective, in this case requiring that the Lagrangian be twice differentiable.\nFirst we need a definition. A tangent direction, \\(w\\), with respect to \\(g(x)\\), is a vector for which \\(\\nabla g_{i}(x)^{\\top}w=0\\). If we are at a point, \\(x^{*}\\), at which the constraint is satisfied, \\(g_{i}(x^{*})=0\\), then we can move in the tangent direction (orthogonal to the gradient of the constraint function) (i.e., along the level curve) and still satisfy the constraint. This is the only kind of movement that is legitimate (gives us a feasible solution).\nIf the gradient of the Lagrangian with respect to \\(x\\) is equal to 0, \\[\\nabla f(x^{*})+\\sum_{i}\\lambda_{i}\\nabla g_{i}(x^{*})+\\sum_{j}\\mu_{j}\\nabla h_{j}(x^{*})=0,\\] and if \\(w^{\\top}H_{L}(x^{*},\\lambda,\\mu)w&gt;0\\) (with \\(H_{L}\\) being the Hessian of the Lagrangian) for all vectors \\(w\\) s.t. \\(\\nabla g(x^{*})^{\\top}w=0\\) and, for all active constraints,\\(\\nabla h(x^{*})^{\\top}w=0\\), then \\(x^{*}\\) is a local minimum. An active constraint is an inequality for which \\(h_{j}(x^{*})=0\\) (rather than \\(h_{j}(x^{*})&lt;0\\), in which case it is inactive). Basically we only need to worry about the inequality constraints when we are on the boundary, so the goal is to keep the constraints inactive.\nSome basic intuition is that we need positive definiteness only for directions that stay in the feasible region. That is, our only possible directions of movement (the tangent directions) keep us in the feasible region, and for these directions, we need the objective function to be increasing to have a minimum. If we were to move in a direction that goes outside the feasible region, it’s ok for the quadratic form involving the Hessian to be negative.\nMany algorithms for convex optimization can be interpreted as methods for solving the KKT conditions."
  },
  {
    "objectID": "units/unit11-optim.html#interior-point-methods",
    "href": "units/unit11-optim.html#interior-point-methods",
    "title": "Optimization",
    "section": "Interior-point methods",
    "text": "Interior-point methods\nWe’ll briefly discuss one of the standard methods for solving a convex optimization problem. The barrier method is one type of interior-point algorithm. It turns out that Newton’s method can be used to solve a constrained optimization problem, with twice-differentiable \\(f\\) and linear equality constraints. So the basic strategy of the barrier method is to turn the more complicated constraint problem into one with only linear equality constraints.\nRecall our previous notation, in which convex programming minimizes \\(f(x)\\) s.t. \\(h_{i}(x)\\leq0,\\,j=1,\\ldots,m\\) and \\(a_{i}^{\\top}x=b_{i},\\,i=1,\\ldots,q\\), where both \\(f\\) and the constraint functions are convex. The strategy begins with moving the inequality constraints into the objective function: \\[f(x)+\\sum_{j=1}^{m}I_{-}(h_{j}(x))\\] where \\(I_{-}(u)=0\\) if \\(u\\leq0\\) and \\(I_{-}(u)=\\infty\\) if \\(u&gt;0\\).\nThis is fine, but the new objective function is not differentiable so we can’t use a Newton-like approach. Instead, we approximate the indicator function with a logarithmic function, giving the new objective function \\[\\tilde{f}(x)=f(x)+\\sum_{j=1}^{m}-(1/t^{*})\\log(-h_{j}(x)),\\] which is convex and differentiable. The new term pushes down the value of the overall objective function when \\(x\\) approaches the boundary, nearing points for which the inequality constraints are not met. The \\(-\\sum(1/t^{*})\\log(-h_{j}(x))\\) term is called the log barrier, since it keeps the solution in the feasible set (i.e., the set where the inequality constraints are satisfied), provided we start at a point in the feasible set. Newton’s method with equality constraints (\\(Ax=b\\)) is then applied. The key thing is then to have \\(t^{*}\\) get larger (i.e., \\(t^{*}\\) is some increasing function of iteration time \\(t\\)) as the iterations proceed, which allows the solution to get closer to the boundary if that is indeed where the minimum lies.\nThe basic ideas behind Newton’s method with equality constraints are (1) start at a feasible point, \\(x_{0}\\), such that \\(Ax_{0}=b\\), and (2) make sure that each step is in a feasible direction, \\(A(x_{t+1}-x_{t})=0\\). To make sure the step is in a feasible direction we have to solve a linear system similar to that in the simplified quadratic programming problem: \\[\\left(\\begin{array}{c}\nx_{t+1}-x_{t}\\\\\n\\lambda\n\\end{array}\\right)=\\left(\\begin{array}{cc}\nH_{\\tilde{f}}(x_{t}) & A^{\\top}\\\\\nA & 0\n\\end{array}\\right)^{-1}\\left(\\begin{array}{c}\n-\\nabla\\tilde{f}(x_{t})\\\\\n0\n\\end{array}\\right),\\] which shouldn’t be surprising since the whole idea of Newton’s method is to substitute a quadratic approximation for the actual objective function."
  },
  {
    "objectID": "units/unit11-optim.html#software-for-constrained-and-convex-optimization",
    "href": "units/unit11-optim.html#software-for-constrained-and-convex-optimization",
    "title": "Optimization",
    "section": "Software for constrained and convex optimization",
    "text": "Software for constrained and convex optimization\nFor general convex optimization in Python see the cvxopt package. Some other resources to consider are\n\nMATLAB, in particular the fmincon() function, the CVX system, and MATLAB’s linear and quadratic programming abilities.\nThe CVXR package in R.\n\nI haven’t looked into CVXR in detail but given the developers include Stephen Boyd, who is a convex optimization guru, it’s worth checking out.\ncvxopt has specific solves (see help(cvxopt.solvers) for different kinds of convex optimization. A general purpose one is cvxopt.solvers.cp. Specifying the problem (the objective function, nonlinear constraints, and linear constraints) using the software is somewhat involved, so I haven’t worked out an example here."
  },
  {
    "objectID": "units/unit10-linalg.html",
    "href": "units/unit10-linalg.html",
    "title": "Numerical linear algebra",
    "section": "",
    "text": "PDF\nReferences:\nVideos (optional):\nThere are various videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to.\nIn working through how to compute something or understanding an algorithm, it can be very helpful to depict the matrices and vectors graphically. We’ll see this on the board in class."
  },
  {
    "objectID": "units/unit10-linalg.html#context",
    "href": "units/unit10-linalg.html#context",
    "title": "Numerical linear algebra",
    "section": "Context",
    "text": "Context\nMany statistical and machine learning methods involve linear algebra of some sort - at the very least matrix multiplication and very often some sort of matrix decomposition to fit models and do analysis: linear regression, various more sophisticated forms of regression, deep neural networks, principle components analysis (PCA) and the wide varieties of generalizations and variations on PCA, etc., etc."
  },
  {
    "objectID": "units/unit10-linalg.html#goals",
    "href": "units/unit10-linalg.html#goals",
    "title": "Numerical linear algebra",
    "section": "Goals",
    "text": "Goals\nHere’s what I’d like you to get out of this unit:\n\nHow to think about the computational order (number of computations involved) of a problem\nHow to choose a computational approach to a given linear algebra calculation you need to do.\nAn understanding of how issues with computer numbers (Unit 8) affect linear algebra calculations."
  },
  {
    "objectID": "units/unit10-linalg.html#key-principle",
    "href": "units/unit10-linalg.html#key-principle",
    "title": "Numerical linear algebra",
    "section": "Key principle",
    "text": "Key principle\nThe form of a mathematical expression and how it should be evaluated on a computer may be very different. Better computational approaches can increase speed and improve the numerical properties of the calculation.\n\nExample 1 (already seen in Unit 5): If \\(X\\) and \\(Y\\) are matrices and \\(z\\) is a vector, we should compute \\(X(Yz)\\) rather than \\((XY)z\\); the former is much more computationally efficient.\nExample 2: We do not compute \\((X^{\\top}X)^{-1}X^{\\top}Y\\) by computing \\(X^{\\top}X\\) and finding its inverse. In fact, perhaps more surprisingly, we may never actually form \\(X^{\\top}X\\) in some implementations.\nExample 3: Suppose I have a matrix \\(A\\), and I want to permute (switch) two rows. I can do this with a permutation matrix, \\(P\\), which is mostly zeroes. On a computer, in general I wouldn’t need to even change the values of \\(A\\) in memory in some cases (e.g., if I were to calculate \\(PAB\\)). Why not?"
  },
  {
    "objectID": "units/unit10-linalg.html#computational-complexity",
    "href": "units/unit10-linalg.html#computational-complexity",
    "title": "Numerical linear algebra",
    "section": "Computational complexity",
    "text": "Computational complexity\nWe can assess the computational complexity of a linear algebra calculation by counting the number multiplys/divides and the number of adds/subtracts. Sidenote: addition is a bit faster than multiplication, so some algorithms attempt to trade multiplication for addition.\nIn general we do not try to count the actual number of calculations, but just their order, though in some cases in this unit we’ll actually get a more exact count. In general, we denote this as \\(O(f(n))\\) which means that the number of calculations approaches \\(cf(n)\\) as \\(n\\to\\infty\\) (i.e., we know the calculation is approximately proportional to \\(f(n)\\)). Consider matrix multiplication, \\(AB\\), with matrices of size \\(a\\times b\\) and \\(b\\times c\\). Each column of the second matrix is multiplied by all the rows of the first. For any given inner product of a row by a column, we have \\(b\\) multiplies. We repeat these operations for each column and then for each row, so we have \\(abc\\) multiplies so \\(O(abc)\\) operations. We could count the additions as well, but there’s usually an addition for each multiply, so we can usually just count the multiplys and then say there are such and such {multiply and add}s. This is Monahan’s approach, but you may see other counting approaches where one counts the multiplys and the adds separately.\nFor two symmetric, \\(n\\times n\\) matrices, this is \\(O(n^{3})\\). Similarly, matrix factorization (e.g., the Cholesky decomposition) is \\(O(n^{3})\\) unless the matrix has special structure, such as being sparse. As matrices get large, the speed of calculations decreases drastically because of the scaling as \\(n^{3}\\) and memory use increases drastically. In terms of memory use, to hold the result of the multiply indicated above, we need to hold \\(ab+bc+ac\\) total elements, which for symmetric matrices sums to \\(3n^{2}\\). So for a matrix with \\(n=10000\\), we have \\(3\\cdot10000^{2}\\cdot8/1e9=2.4\\)Gb.\nWhen we have \\(O(n^{q})\\) this is known as polynomial time. Much worse is \\(O(b^{n})\\) (exponential time), while much better is \\(O(\\log n\\)) (log time). Computer scientists talk about NP-complete problems; these are essentially problems for which there is not a polynomial time algorithm - it turns out all such problems can be rewritten such that they are equivalent to one another.\nIn real calculations, it’s possible to have the actual time ordering of two approaches differ from what the order approximations tell us. For example, something that involves \\(n^{2}\\) operations may be faster than one that involves \\(1000(n\\log n+n)\\) even though the former is \\(O(n^{2})\\) and the latter \\(O(n\\log n)\\). The problem is that the constant, \\(c=1000\\), can matter (depending on how big \\(n\\) is), as can the extra calculations from the lower order term(s), in this case \\(1000n\\).\nA note on terminology: flops stands for both floating point operations (the number of operations required) and floating point operations per second, the speed of calculation."
  },
  {
    "objectID": "units/unit10-linalg.html#notation-and-dimensions",
    "href": "units/unit10-linalg.html#notation-and-dimensions",
    "title": "Numerical linear algebra",
    "section": "Notation and dimensions",
    "text": "Notation and dimensions\nI’ll try to use capital letters for matrices, \\(A\\), and lower-case for vectors, \\(x\\). Then \\(x_{i}\\) is the ith element of \\(x\\), \\(A_{ij}\\) is the \\(i\\)th row, \\(j\\)th column element, and \\(A_{\\cdot j}\\) is the \\(j\\)th column and \\(A_{i\\cdot}\\) the \\(i\\)th row. By default, we’ll consider a vector, \\(x\\), to be a one-column matrix, and \\(x^{\\top}\\) to be a one-row matrix. Some of the references given at the start of this Unit also use \\(a_{ij}\\) for \\(A_{ij}\\) and \\(a_{j}\\) for the \\(j\\)th column.\nThroughout, we’ll need to be careful that the matrices involved in an operation are conformable: for \\(A+B\\) both matrices need to be of the same dimension, while for \\(AB\\) the number of columns of \\(A\\) must match the number of rows of \\(B\\). Note that this allows for \\(B\\) to be a column vector, with only one column, \\(Ab\\). Just checking dimensions is a good way to catch many errors. Example: is \\(\\mbox{Cov}(Ax)=A\\mbox{Cov}(x)A^{\\top}\\) or \\(\\mbox{Cov}(Ax)=A^{\\top}\\mbox{Cov}(x)A\\)? Well, if \\(A\\) is \\(m\\times n\\), it must be the former, as the latter is not conformable.\nThe inner product of two vectors is \\(\\sum_{i}x_{i}y_{i}=x^{\\top}y\\equiv\\langle x,y\\rangle\\equiv x\\cdot y\\).\nThe outer product is \\(xy^{\\top}\\), which comes from all pairwise products of the elements.\nWhen the indices of summation should be obvious, I’ll sometimes leave them implicit. Ask me if it’s not clear."
  },
  {
    "objectID": "units/unit10-linalg.html#norms",
    "href": "units/unit10-linalg.html#norms",
    "title": "Numerical linear algebra",
    "section": "Norms",
    "text": "Norms\nFor a vector, \\(\\|x\\|_{p}=(\\sum_{i}|x_{i}|^{p})^{1/p}\\) and the standard (Euclidean) norm is \\(\\|x\\|_{2}=\\sqrt{\\sum x_{i}^{2}}=\\sqrt{x^{\\top}x}\\), just the length of the vector in Euclidean space, which we’ll refer to as \\(\\|x\\|\\), unless noted otherwise.\nOne commonly used norm for a matrix is the Frobenius norm, \\(\\|A\\|_{F}=(\\sum_{i,j}a_{ij}^{2})^{1/2}\\).\nIn this Unit, we’ll often make use of the induced matrix norm, which is defined relative to a corresponding vector norm, \\(\\|\\cdot\\|\\), as: \\[\\|A\\|=\\sup_{x\\ne0}\\frac{\\|Ax\\|}{\\|x\\|}\\] So we have \\[\\|A\\|_{2}=\\sup_{x\\ne0}\\frac{\\|Ax\\|_{2}}{\\|x\\|_{2}}=\\sup_{\\|x\\|_{2}=1}\\|Ax\\|_{2}\\] If you’re not familiar with the supremum (“sup” above), you can just think of it as taking the maximum. In the case of the 2-norm, the norm turns out to be the largest singular value in the singular value decomposition (SVD) of the matrix.\nWe can interpret the norm of a matrix as the most that the matrix can stretch a vector when multiplying by the vector (relative to the length of the vector).\nA property of any legitimate matrix norm (including the induced norm) is that \\(\\|AB\\|\\leq\\|A\\|\\|B\\|\\). Also recall that norms must obey the triangle inequality, \\(\\|A+B\\|\\leq\\|A\\|+\\|B\\|\\).\nA normalized vector is one with “length”, i.e., Euclidean norm, of one. We can easily normalize a vector: \\(\\tilde{x}=x/\\|x\\|\\)\nThe angle between two vectors is \\[\\theta=\\cos^{-1}\\left(\\frac{\\langle x,y\\rangle}{\\sqrt{\\langle x,x\\rangle\\langle y,y\\rangle}}\\right)\\]"
  },
  {
    "objectID": "units/unit10-linalg.html#orthogonality",
    "href": "units/unit10-linalg.html#orthogonality",
    "title": "Numerical linear algebra",
    "section": "Orthogonality",
    "text": "Orthogonality\nTwo vectors are orthogonal if \\(x^{\\top}y=0\\), in which case we say \\(x\\perp y\\). An orthogonal matrix is a square matrix in which all of the columns are orthogonal to each other and normalized. The same holds for the rows. Orthogonal matrices can be shown to have full rank. Furthermore if \\(A\\) is orthogonal, \\(A^{\\top}A=I\\), so \\(A^{-1}=A^{\\top}\\). Given all this, the determinant of orthogonal \\(A\\) is either 1 or -1. Finally the product of two orthogonal matrices, \\(A\\) and \\(B\\), is also orthogonal since \\((AB)^{\\top}AB=B^{\\top}A^{\\top}AB=B^{\\top}B=I\\).\n\nPermutations\nSometimes we make use of matrices that permute two rows (or two columns) of another matrix when multiplied. Such a matrix is known as an elementary permutation matrix and is an orthogonal matrix with a determinant of -1. You can multiply such matrices to get more general permutation matrices that are also orthogonal. If you premultiply by \\(P\\), you permute rows, and if you postmultiply by \\(P\\) you permute columns. Note that on a computer, you wouldn’t need to actually do the multiply (and if you did, you should use a sparse matrix routine), but rather one can often just rework index values that indicate where relevant pieces of the matrix are stored (more in the next section)."
  },
  {
    "objectID": "units/unit10-linalg.html#some-vector-and-matrix-properties",
    "href": "units/unit10-linalg.html#some-vector-and-matrix-properties",
    "title": "Numerical linear algebra",
    "section": "Some vector and matrix properties",
    "text": "Some vector and matrix properties\n\\(AB\\ne BA\\) but \\(A+B=B+A\\) and \\(A(BC)=(AB)C\\).\nIn Python, recall the syntax is\n\nA + B\n\n# Matrix multiplication\nnp.matmul(A, B)  \nA @ B        # alternative\nA.dot(B)     # not recommended by the NumPy docs\n\nA * B # Hadamard (direct) product\n\nYou don’t need the spaces, but they’re nice for code readability."
  },
  {
    "objectID": "units/unit10-linalg.html#trace-and-determinant-of-square-matrices",
    "href": "units/unit10-linalg.html#trace-and-determinant-of-square-matrices",
    "title": "Numerical linear algebra",
    "section": "Trace and determinant of square matrices",
    "text": "Trace and determinant of square matrices\nThe trace of a matrix is the sum of the diagonal elements. For square matrices, \\(\\mbox{tr}(A+B)=\\mbox{tr}(A)+\\mbox{tr}(B)\\), \\(\\mbox{tr}(A)=\\mbox{tr}(A^{\\top})\\).\nWe also have \\(\\mbox{tr}(ABC)=\\mbox{tr}(CAB)=\\mbox{tr}(BCA)\\) - basically you can move a matrix from the beginning to the end or end to beginning, provided they are conformable for this operation. This is helpful for a couple reasons:\n\nWe can find the ordering that reduces computation the most if the individual matrices are not square.\n\\(x^{\\top}Ax=\\mbox{tr}(x^{\\top}Ax)\\) since the quadratic form, \\(x^{\\top}Ax\\), is a scalar, and this is equal to \\(\\mbox{tr}(xx^{\\top}A)\\) where \\(xx^{\\top}A\\) is a matrix. It can be helpful to be able to go back and forth between a scalar and a trace in some statistical calculations.\n\nFor square matrices, the determinant exists and we have \\(|AB|=|A||B|\\) and therefore, \\(|A^{-1}|=1/|A|\\) since \\(|I|=|AA^{-1}|=1\\). Also \\(|A|=|A^{\\top}|\\), which can be seen using the QR decomposition for \\(A\\) and understanding properties of determinants of triangular matrices (in this case \\(R\\)) and orthogonal matrices (in this case \\(Q\\))."
  },
  {
    "objectID": "units/unit10-linalg.html#transposes-and-inverses",
    "href": "units/unit10-linalg.html#transposes-and-inverses",
    "title": "Numerical linear algebra",
    "section": "Transposes and inverses",
    "text": "Transposes and inverses\nFor square, invertible matrices, we have that \\((A^{-1})^{\\top}=(A^{\\top})^{-1}\\). Why? Since we have \\((AB)^{\\top}=B^{\\top}A^{\\top}\\), we have: \\[A^{\\top}(A^{-1})^{\\top}=(A^{-1}A)^{\\top}=I\\] so \\((A^{\\top})^{-1}=(A^{-1})^{\\top}\\).\nFor two invertible matrices, we have that \\((AB)^{-1} = B^{-1}A^{-1}\\) since \\(B^{-1}A^{-1} AB = I\\).\n\nOther matrix multiplications\nThe Hadamard or direct product is simply multiplication of the correspoding elements of two matrices by each other. In R this is simplyA * B.\nChallenge: How can I find \\(\\mbox{tr}(AB)\\) without using A %*% B ?\nThe Kronecker product is the product of each element of one matrix with the entire other matrix”\n\\[A\\otimes B=\\left(\\begin{array}{ccc}\nA_{11}B & \\cdots & A_{1m}B\\\\\n\\vdots & \\ddots & \\vdots\\\\\nA_{n1}B & \\cdots & A_{nm}B\n\\end{array}\\right)\\]\nThe inverse of a Kronecker product is the Kronecker product of the inverses,\n\\[ B^{-1} \\otimes A^{-1} \\]\nwhich is obviously quite a bit faster because the inverse (i.e., solving a system of equations) in this special case is \\(O(n^{3}+m^{3})\\) rather than the naive approach being \\(O((nm)^{3})\\)."
  },
  {
    "objectID": "units/unit10-linalg.html#matrix-decompositions",
    "href": "units/unit10-linalg.html#matrix-decompositions",
    "title": "Numerical linear algebra",
    "section": "Matrix decompositions",
    "text": "Matrix decompositions\nA matrix decomposition is a re-expression of a matrix, \\(A\\), in terms of a product of two or three other, simpler matrices, where the decomposition reveals structure or relationships present in the original matrix, \\(A\\). The “simpler” matrices may be simpler in various ways, including\n\nhaving fewer rows or columns;\nbeing diagonal, triangular or sparse in some way,\nbeing orthogonal matrices.\n\nIn addition, once you have a decomposition, computation is generally easier, because of the special structure of the simpler matrices.\nWe’ll see this in great detail in Section 3."
  },
  {
    "objectID": "units/unit10-linalg.html#linear-independence-rank-and-basis-vectors",
    "href": "units/unit10-linalg.html#linear-independence-rank-and-basis-vectors",
    "title": "Numerical linear algebra",
    "section": "Linear independence, rank, and basis vectors",
    "text": "Linear independence, rank, and basis vectors\nA set of vectors, \\(v_{1},\\ldots v_{n}\\), is linearly independent (LIN) when none of the vectors can be represented as a linear combination, \\(\\sum c_{i}v_{i}\\), of the others for scalars, \\(c_{1},\\ldots,c_{n}\\). If we have vectors of length \\(n\\), we can have at most \\(n\\) linearly independent vectors. The rank of a matrix is the number of linearly independent rows (or columns - it’s the same), and is at most the minimum of the number of rows and number of columns. We’ll generally think about it in terms of the dimension of the column space - so we can just think about the number of linearly independent columns.\nAny set of linearly independent vectors (say \\(v_{1},\\ldots,v_{n}\\)) span a space made up of all linear combinations of those vectors (\\(\\sum_{i=1}^{n}c_{i}v_{i}\\)). The spanning vectors are known as basis vectors. We can express a vector \\(y\\) that is in the space with respect to (as a linear combination of) basis vectors as \\(y=\\sum_{i}c_{i}v_{i}\\), where if the basis vectors are normalized and orthogonal, we can find the weights as \\(c_{i}=\\langle y,v_{i}\\rangle\\).\nConsider a regression context. We have \\(p\\) covariates (\\(p\\) columns in the design matrix, \\(X\\)), of which \\(q\\leq p\\) are linearly independent covariates. This means that \\(p-q\\) of the vectors can be written as linear combos of the \\(q\\) vectors. The space spanned by the covariate vectors is of dimension \\(q\\), rather than \\(p\\), and \\(X^{\\top}X\\) has \\(p-q\\) eigenvalues that are zero. The \\(q\\) LIN vectors are basis vectors for the space - we can represent any point in the space as a linear combination of the basis vectors. You can think of the basis vectors as being like the axes of the space, except that the basis vectors are not orthogonal. So it’s like denoting a point in \\(\\Re^{q}\\) as a set of \\(q\\) numbers telling us where on each of the axes we are - this is the same as a linear combination of axis-oriented vectors.\nWhen fitting a regression, if \\(n=p=q\\), a vector of \\(n\\) observations can be represented exactly as a linear combination of the \\(p\\) basis vectors, so there is no residual and we have a single unique (and exact) solution (e.g., with \\(n=p=2\\), the observations fall exactly on the simple linear regression line). If \\(n&lt;p\\), then we have at most \\(n\\) linearly independent covariates (the rank is at most \\(n\\)). In this case we have multiple possible solutions and the system is ill-determined (under-determined). Similarly, if \\(q&lt;p\\) and \\(n\\geq p\\), the rank is again less than \\(p\\) and we have multiple possible solutions. Of course we usually have \\(n&gt;p\\), so the system is overdetermined - there is no exact solution, but regression is all about finding solutions that minimize some criterion about the differences between the observations and linear combinations of the columns of the \\(X\\) matrix (such as least squares or penalized least squares). In standard regression, we project the observation vector onto the space spanned by the columns of the \\(X\\) matrix, so we find the point in the space closest to the observation vector."
  },
  {
    "objectID": "units/unit10-linalg.html#invertibility-singularity-rank-and-positive-definiteness",
    "href": "units/unit10-linalg.html#invertibility-singularity-rank-and-positive-definiteness",
    "title": "Numerical linear algebra",
    "section": "Invertibility, singularity, rank, and positive definiteness",
    "text": "Invertibility, singularity, rank, and positive definiteness\nFor square matrices, let’s consider how invertibility, singularity, rank and positive (or non-negative) definiteness relate.\nSquare matrices that are “regular” have an eigendecomposition, \\(A=\\Gamma\\Lambda\\Gamma^{-1}\\) where \\(\\Gamma\\) is a matrix with the eigenvectors as the columns and \\(\\Lambda\\) is a diagonal matrix of eigenvalues, \\(\\Lambda_{ii}=\\lambda_{i}\\). Symmetric matrices and matrices with unique eigenvalues are regular, as are some other matrices. The number of non-zero eigenvalues is the same as the rank of the matrix. Square matrices that have an inverse are also called nonsingular, and this is equivalent to having full rank. If the matrix is symmetric, the eigenvectors and eigenvalues are real and \\(\\Gamma\\) is orthogonal, so we have \\(A=\\Gamma\\Lambda\\Gamma^{\\top}\\). The determinant of the matrix is the product of the eigenvalues (why?), which is zero if it is less than full rank. Note that if none of the eigenvalues are zero then \\(A^{-1}=\\Gamma\\Lambda^{-1}\\Gamma^{\\top}\\).\nLet’s focus on symmetric matrices. The symmetric matrices that tend to arise in statistics are either positive definite (p.d.) or non-negative definite (n.n.d.). If a matrix is positive definite, then by definition \\(x^{\\top}Ax&gt;0\\) for any \\(x\\). Note that if \\(\\mbox{Cov}(y)=A\\) then \\(x^{\\top}Ax=x^{\\top}\\mbox{Cov}(y)x=\\mbox{Cov}(x^{\\top}y)=\\mbox{Var}(x^{\\top}y)\\) if so positive definiteness amounts to having linear combinations of random variables (with the elements of \\(x\\) here being the weights) having positive variance. So we must have that positive definite matrices are equivalent to variance-covariance matrices (I’ll just refer to this as a variance matrix or as a covariance matrix). If \\(A\\) is p.d. then it has all positive eigenvalues and it must have an inverse, though as we’ll see, from a numerical perspective, we may not be able to compute it if some of the eigenvalues are very close to zero. In Python, numpy.linalg.eig(A)[1] is \\(\\Gamma\\), with each column a vector, and numpy.linalg.eig(A)[0] contains the (unordered) eigenvalues.\nTo summarize, here are some of the various connections between mathematical and statistical properties of positive definite matrices:\n\\(A\\) positive definite \\(\\Leftrightarrow\\) \\(A\\) is a covariance matrix \\(\\Leftrightarrow\\) \\(x^{\\top}Ax&gt;0\\) \\(\\Leftrightarrow\\) \\(\\lambda_{i}&gt;0\\) (positive eigenvalues) \\(\\Rightarrow\\)\\(|A|&gt;0\\) \\(\\Rightarrow\\)\\(A\\) is invertible \\(\\Leftrightarrow\\) \\(A\\) is non singular \\(\\Leftrightarrow\\) \\(A\\) is full rank.\nAnd here are connections for positive semi-definite matrices:\n\\(A\\) positive semi-definite \\(\\Leftrightarrow\\) \\(A\\) is a constrained covariance matrix \\(\\Leftrightarrow\\) \\(x^{\\top}Ax\\geq0\\) and equal to 0 for some \\(x\\) \\(\\Leftrightarrow\\) \\(\\lambda_{i}\\geq 0\\) (non-negative eigenvalues), with at least one zero \\(\\Rightarrow\\) \\(|A|=0\\) \\(\\Leftrightarrow\\) \\(A\\) is not invertible \\(\\Leftrightarrow\\) \\(A\\) is singular \\(\\Leftrightarrow\\) \\(A\\) is not full rank."
  },
  {
    "objectID": "units/unit10-linalg.html#interpreting-an-eigendecomposition",
    "href": "units/unit10-linalg.html#interpreting-an-eigendecomposition",
    "title": "Numerical linear algebra",
    "section": "Interpreting an eigendecomposition",
    "text": "Interpreting an eigendecomposition\nLet’s interpret the eigendecomposition in a generative context as a way of generating random vectors. We can generate \\(y\\) s.t. \\(\\mbox{Cov}(y)=A\\) if we generate \\(y=\\Gamma\\Lambda^{1/2}z\\) where \\(\\mbox{Cov}(z)=I\\) and \\(\\Lambda^{1/2}\\) is formed by taking the square roots of the eigenvalues. So \\(\\sqrt{\\lambda_{i}}\\) is the standard deviation associated with the basis vector \\(\\Gamma_{\\cdot i}\\). That is, the \\(z\\)’s provide the weights on the basis vectors, with scaling based on the eigenvalues. So \\(y\\) is produced as a linear combination of eigenvectors as basis vectors, with the variance attributable to the basis vectors determined by the eigenvalues.\nTo go the other direction, we can project a vector \\(y\\) onto the space spanned by the eigenvectors: \\(w = (\\Gamma^{\\top}\\Gamma)^{-1}\\Gamma^{\\top}y = \\Gamma^{\\top}y = \\Lambda^{1/2}z\\), where the simplification of course comes from \\(\\Gamma\\) being orthogonal.\nIf \\(x^{\\top}Ax\\geq0\\) then \\(A\\) is nonnegative definite (also called positive semi-definite). In this case one or more eigenvalues can be zero. Let’s interpret this a bit more in the context of generating random vectors based on non-negative definite matrices, \\(y=\\Gamma\\Lambda^{1/2}z\\) where \\(\\mbox{Cov}(z)=I\\). Questions:\n\nWhat does it mean when one or more eigenvalue (i.e., \\(\\lambda_{i}=\\Lambda_{ii}\\)) is zero?\nSuppose I have an eigenvalue that is very small and I set it to zero? What will be the impact upon \\(y\\) and \\(\\mbox{Cov}(y)\\)?\nNow let’s consider the inverse of a covariance matrix, known as the precision matrix, \\(A^{-1}=\\Gamma\\Lambda^{-1}\\Gamma^{\\top}\\). What does it mean if a \\((\\Lambda^{-1})_{ii}\\) is very large? What if \\((\\Lambda^{-1})_{ii}\\) is very small?\n\nConsider an arbitrary \\(n\\times p\\) matrix, \\(X\\). Any crossproduct or sum of squares matrix, such as \\(X^{\\top}X\\) is positive definite (non-negative definite if \\(p&gt;n\\)). This makes sense as it’s just a scaling of an empirical covariance matrix."
  },
  {
    "objectID": "units/unit10-linalg.html#generalized-inverses-optional",
    "href": "units/unit10-linalg.html#generalized-inverses-optional",
    "title": "Numerical linear algebra",
    "section": "Generalized inverses (optional)",
    "text": "Generalized inverses (optional)\nSuppose I want to find \\(x\\) such that \\(Ax=b\\). Mathematically the answer (provided \\(A\\) is invertible, i.e. of full rank) is \\(x=A^{-1}b\\).\nGeneralized inverses arise in solving equations when \\(A\\) is not full rank. A generalized inverse is a matrix, \\(A^{-}\\) s.t. \\(AA^{-}A=A\\). The Moore-Penrose inverse (the pseudo-inverse), \\(A^{+}\\), is a (unique) generalized inverse that also satisfies some additional properties. \\(x=A^{+}b\\) is the solution to the linear system, \\(Ax=b\\), that has the shortest length for \\(x\\).\nWe can find the pseudo-inverse based on an eigendecomposition (or an SVD) as \\(\\Gamma\\Lambda^{+}\\Gamma^{\\top}\\). We obtain \\(\\Lambda^{+}\\) from \\(\\Lambda\\) as follows. For values \\(\\lambda_{i}&gt;0\\), compute \\(1/\\lambda_{i}\\). All other values are set to 0. Let’s interpret this statistically. Suppose we have a precision matrix with one or more zero eigenvalues and we want to find the covariance matrix. A zero eigenvalue means we have no precision, or infinite variance, for some linear combination (i.e., for some basis vector). We take the pseudo-inverse and assign that linear combination zero variance.\nLet’s consider a specific example. Autoregressive models are often used for smoothing (in time, in space, and in covariates). A first order autoregressive model for \\(y_{1},y_{2},\\ldots,y_{T}\\) has \\(E(y_{i}|y_{-i})=\\frac{1}{2}(y_{i-1}+y_{i+1})\\). Another way of writing the model is in time-order: \\(y_{i}=y_{i-1}+\\epsilon_{i}\\). A second order autoregressive model has \\(E(y_{i}|y_{-i})=\\frac{1}{6}(4y_{i-1}+4y_{i+1}-y_{i-2}-y_{i+2})\\). These constructions basically state that each value should be a smoothed version of its neighbors. One can figure out that the precision matrix for \\(y\\) in the first order model is \\[\\left(\\begin{array}{ccccc}\n\\ddots &  & \\vdots\\\\\n-1 & 2 & -1 & 0\\\\\n\\cdots & -1 & 2 & -1 & \\dots\\\\\n& 0 & -1 & 2 & -1\\\\\n&  & \\vdots &  & \\ddots\n\\end{array}\\right)\\] and in the second order model is\n\\[\\left( \\begin{array}{ccccccc} \\ddots &  &  & \\vdots \\\\ 1 & -4 & 6 & -4 & 1 \\\\ \\cdots & 1 & -4 & 6 & -4 & 1 & \\cdots \\\\  &  & 1 & -4 & 6 & -4 & 1 \\\\  &  &  & \\vdots \\end{array} \\right).\\]\nIf we look at the eigendecomposition of such matrices, we see that in the first order case, the eigenvalue corresponding to the constant eigenvector is zero.\n\nimport numpy as np\n\nprecMat = np.array([[1,-1,0,0,0],[-1,2,-1,0,0],[0,-1,2,-1,0],[0,0,-1,2,-1],[0,0,0,-1,1]])\ne = np.linalg.eig(precMat)\ne[0]        # 4th eigenvalue is numerically zero\n\narray([3.61803399e+00, 2.61803399e+00, 1.38196601e+00, 4.97762256e-17,\n       3.81966011e-01])\n\ne[1][:,3]   # constant eigenvector\n\narray([0.4472136, 0.4472136, 0.4472136, 0.4472136, 0.4472136])\n\n\nThis means we have no information about the overall level of \\(y\\). So how would we generate sample \\(y\\) vectors? We can’t put infinite variance on the constant basis vector and still generate samples. Instead we use the pseudo-inverse and assign ZERO variance to the constant basis vector. This corresponds to generating realizations under the constraint that \\(\\sum y_{i}\\) has no variation, i.e., \\(\\sum y_{i}=\\bar{y}=0\\) - you can see this by seeing that \\(\\mbox{Var}(\\Gamma_{\\cdot i}^{\\top}y)=0\\) when \\(\\lambda_{i}=0\\).\n\n# generate a realization\nevals = e[0]\nevals = 1/evals   # variances\nevals[3] = 0      # generalized inverse\ny = e[1] @ ((evals ** 0.5) * np.random.normal(size = 5))\ny.sum()\n\n-1.3322676295501878e-15\n\n\nIn the second order case, we have two non-identifiabilities: for the sum and for the linear component of the variation in \\(y\\) (linear in the indices of \\(y\\)).\nI could parameterize a statistical model as \\(\\mu+y\\) where \\(y\\) has covariance that is the generalized inverse discussed above. Then I allow for both a non-zero mean and for smooth variation governed by the autoregressive structure. In the second-order case, I would need to add a linear component as well, given the second non-identifiability."
  },
  {
    "objectID": "units/unit10-linalg.html#matrices-arising-in-regression",
    "href": "units/unit10-linalg.html#matrices-arising-in-regression",
    "title": "Numerical linear algebra",
    "section": "Matrices arising in regression",
    "text": "Matrices arising in regression\nIn regression, we work with \\(X^{\\top}X\\). Some properties of this matrix are that it is symmetric and non-negative definite (hence our use of \\((X^{\\top}X)^{-1}\\) in the OLS estimator). When is it not positive definite?\nFitted values are \\(X\\hat{\\beta}=X(X^{\\top}X)^{-1}X^{\\top}Y=HY\\). The “hat” matrix, \\(H\\), projects \\(Y\\) into the column space of \\(X\\). \\(H\\) is idempotent: \\(HH=H\\), which makes sense - once you’ve projected into the space, any subsequent projection just gives you the same thing back. \\(H\\) is singular. Why? Also, under what special circumstance would it not be singular?"
  },
  {
    "objectID": "units/unit10-linalg.html#storing-matrices",
    "href": "units/unit10-linalg.html#storing-matrices",
    "title": "Numerical linear algebra",
    "section": "Storing matrices",
    "text": "Storing matrices\nWe’ve discussed column-major and row-major storage of matrices. First, retrieval of matrix elements from memory is quickest when multiple elements are contiguous in memory. So in a column-major language (e.g., R, Fortran), it is best to work with values in a common column (or entire columns) while in a row-major language (e.g., Python, C) for values in a common row.\nIn some cases, one can save space (and potentially speed) by overwriting the output from a matrix calculation into the space occupied by an input. This occurs in some clever implementations of matrix factorizations."
  },
  {
    "objectID": "units/unit10-linalg.html#algorithms",
    "href": "units/unit10-linalg.html#algorithms",
    "title": "Numerical linear algebra",
    "section": "Algorithms",
    "text": "Algorithms\nGood algorithms can change the efficiency of an algorithm by one or more orders of magnitude, and many of the improvements in computational speed over recent decades have been in algorithms rather than in computer speed.\nMost matrix algebra calculations can be done in multiple ways. For example, we could compute \\(b=Ax\\) in either of the following ways, denoted here in pseudocode.\n\nStack the inner products of the rows of \\(A\\) with \\(x\\).\n\n\n        for(i=1:n){ \n            b_i = 0\n            for(j=1:m){\n                b_i = b_i + a_{ij} x_j\n            }\n        }\n\nTake the linear combination (based on \\(x\\)) of the columns of \\(A\\)\n\n\n        for(i=1:n){ \n            b_i = 0\n        }\n        for(j=1:m){\n            for(i = 1:n){\n                b_i = b_i + a_{ij} x_j  \n            }\n        }\nIn this case the two approaches involve the same number of operations but the first might be better for row-major matrices (so might be how we would implement in C) and the second for column-major (so might be how we would implement in Fortran).\nChallenge: check whether the first approach is faster in Python. (Write the code just doing the outer loop and doing the inner loop using vectorized calculation.)\n\nGeneral computational issues\nThe same caveats we discussed in terms of computer arithmetic hold naturally for linear algebra, since this involves arithmetic with many elements. Good implementations of algorithms are aware of the danger of catastrophic cancellation and of the possibility of dividing by zero or by values that are near zero."
  },
  {
    "objectID": "units/unit10-linalg.html#ill-conditioned-problems",
    "href": "units/unit10-linalg.html#ill-conditioned-problems",
    "title": "Numerical linear algebra",
    "section": "Ill-conditioned problems",
    "text": "Ill-conditioned problems\n\nBasics\nA problem is ill-conditioned if small changes to values in the computation result in large changes in the result. This is quantified by something called the condition number of a calculation. For different operations there are different condition numbers.\nIll-conditionedness arises most often in terms of matrix inversion, so the standard condition number is the “condition number with respect to inversion”, which when using the \\(L_{2}\\) norm is the ratio of the absolute values of the largest to smallest eigenvalue. Here’s an example: \\[A=\\left(\\begin{array}{cccc}\n10 & 7 & 8 & 7\\\\\n7 & 5 & 6 & 5\\\\\n8 & 6 & 10 & 9\\\\\n7 & 5 & 9 & 10\n\\end{array}\\right).\\] The solution of \\(Ax=b\\) for \\(b=(32,23,33,31)\\) is \\(x=(1,1,1,1)\\), while the solution for \\(b+\\delta b=(32.1,22.9,33.1,30.9)\\) is \\(x+\\delta x=(9.2,-12.6,4.5,-1.1)\\), where \\(\\delta\\) is notation for a perturbation to the vector or matrix.\n\ndef norm2(x):\n    return(np.sum(x**2) ** 0.5)\n\nA = np.array([[10,7,8,7],[7,5,6,5],[8,6,10,9],[7,5,9,10]])\nb = np.array([32,23,33,31])\nx = np.linalg.solve(A, b)\n\nbPerturbed = np.array([32.1, 22.9, 33.1, 30.9])\nxPerturbed = np.linalg.solve(A, bPerturbed)\n\ndelta_b = bPerturbed - b\ndelta_x = xPerturbed - x\n\nWhat’s going on? Some manipulations with inequalities involving the induced matrix norm (for any chosen vector norm, but we might as well just think about the Euclidean norm) (see Gentle-CS Sec. 5.1 or the derivation in class) give \\[\\frac{\\|\\delta x\\|}{\\|x\\|}\\leq\\|A\\|\\|A^{-1}\\|\\frac{\\|\\delta b\\|}{\\|b\\|}\\] where we define the condition number w.r.t. inversion as \\(\\mbox{cond}(A)\\equiv\\|A\\|\\|A^{-1}\\|\\). We’ll generally work with the \\(L_{2}\\) norm, and for a nonsingular square matrix the result is that the condition number is the ratio of the absolute values of the largest and smallest magnitude eigenvalues. This makes sense since \\(\\|A\\|_{2}\\) is the absolute value of the largest magnitude eigenvalue of \\(A\\) and \\(\\|A^{-1}\\|_{2}\\) that of the inverse of the absolute value of the smallest magnitude eigenvalue of \\(A\\).\nWe see in the code below that the large disparity in eigenvalues of \\(A\\) leads to an effect predictable from our inequality above, with the condition number helping us find an upper bound.\n\ne = np.linalg.eig(A)\nevals = e[0]\nprint(evals)\n\n## relative perturbation in x much bigger than in b\n\n[3.02886853e+01 3.85805746e+00 1.01500484e-02 8.43107150e-01]\n\nnorm2(delta_x) / norm2(x)\n\n8.19847546803699\n\nnorm2(delta_b) / norm2(b)\n\n## ratio of relative perturbations\n\n0.0033319453118976702\n\n(norm2(delta_x) / norm2(x)) / (norm2(delta_b) / norm2(b))\n\n## ratio of largest and smallest magnitude eigenvalues\n## confusingly evals[2] is the smallest, not evals[3]\n\n2460.567236431514\n\n(evals[0]/evals[2])\n\n2984.092701676269\n\n\nThe main use of these ideas for our purposes is in thinking about the numerical accuracy of a linear system solution (Gentle-NLA Sec 3.4). On a computer we have the system \\[(A+\\delta A)(x+\\delta x)=b+\\delta b\\] where the ‘perturbation’ is from the inaccuracy of computer numbers. Our exploration of computer numbers tells us that \\[\\frac{\\|\\delta b\\|}{\\|b\\|}\\approx10^{-p};\\,\\,\\,\\frac{\\|\\delta A\\|}{\\|A\\|}\\approx10^{-p}\\] where \\(p=16\\) for standard double precision floating points. Following Gentle, one gets the approximation\n\\[\\frac{\\|\\delta x\\|}{\\|x\\|}\\approx\\mbox{cond}(A)10^{-p},\\] so if \\(\\mbox{cond}(A)\\approx10^{t}\\), we have accuracy of order \\(10^{t-p}\\) instead of \\(10^{-p}\\). (Gentle cautions that this holds only if \\(10^{t-p}\\ll1\\)). So we can think of the condition number as giving us the number of digits of accuracy lost during a computation relative to the precision of numbers on the computer. E.g., a condition number of \\(10^{8}\\) means we lose 8 digits of accuracy relative to our original 16 on standard systems. One issue is that estimating the condition number is itself subject to numerical error and requires computation of \\(A^{-1}\\) (albeit not in the case of \\(L_{2}\\) norm with square, nonsingular \\(A\\)) but see Golub and van Loan (1996; p. 76-78) for an algorithm.\n\n\nImproving conditioning\nIll-conditioned problems in statistics often arise from collinearity of regressors. Often the best solution is not a numerical one, but re-thinking the modeling approach, as this generally indicates statistical issues beyond just the numerical difficulties.\nA general comment on improving conditioning is that we want to avoid large differences in the magnitudes of numbers involved in a calculation. In some contexts such as regression, we can center and scale the columns to avoid such differences - this will improve the condition of the problem. E.g., in simple quadratic regression with \\(x=\\{1990,\\ldots,2010\\}\\) (e.g., regressing on calendar years), we see that centering and scaling the matrix columns makes a huge difference on the condition number\n\nimport statsmodels.api as sm\n\nt1 = np.arange(1990, 2011)  # naive covariate\nX1 = np.column_stack((np.ones(21), t1, t1 ** 2))\n\nbeta = np.array([5, .1, .0001])\ny = X1 @ beta + np.random.normal(size = len(t1))\n\ne1 = np.linalg.eig(np.dot(X1.T, X1))\nnp.sort(e1[0])[::-1]\n\narray([3.36018564e+14, 7.69949736e+02, 2.24079720e-08])\n\nnp.linalg.cond(np.dot(X1.T, X1))  # built-in!\n\n4.653329826789808e+21\n\nsm.OLS(y, X1).fit().params\n\narray([ 1.60095209e+03, -1.52558839e+00,  5.13787887e-04])\n\nt2 = t1 - 2000              # centered\nX2 = np.column_stack((np.ones(21), t2, t2 ** 2))\ne2 = np.linalg.eig(np.dot(X2.T, X2))\nwith np.printoptions(suppress=True):\n    np.sort(e2[0])[::-1]\n\narray([50677.70427505,   770.        ,     9.29572495])\n\nt3 = t2/10                  # centered and scaled\nX3 = np.column_stack((np.ones(21), t3, t3 ** 2))\ne3 = np.linalg.eig(np.dot(X3.T, X3))\nwith np.printoptions(suppress=True):\n    np.sort(e3[0])[::-1]\n\narray([24.11293487,  7.7       ,  1.95366513])\n\n\nI haven’t shown the OLS results for the transformed version of the problem because the transformations modify the true parameter values, so there is a bit of arithmetic to be done, but you should be able to verify that the second and third approaches give reasonable answers.\nThe basic story is that simple strategies often solve the problem, and that you should be aware of the absolute and relative magnitudes involved in your calculations.\nOne rule of thumb is to try to work with numbers whose magnitude is around 1. We can often scale the values in our problem in order to do this. I.e., change the units of your variables. Instead of personal income in dollars, use personal income in thousands or hundreds of thousands of dollars."
  },
  {
    "objectID": "units/unit10-linalg.html#triangular-systems",
    "href": "units/unit10-linalg.html#triangular-systems",
    "title": "Numerical linear algebra",
    "section": "Triangular systems",
    "text": "Triangular systems\nAs a preface, let’s figure out how to solve \\(Ax=b\\) if \\(A\\) is upper triangular. The basic algorithm proceeds from the bottom up (and therefore is called a ‘backsolve’. We solve for \\(x_{n}\\) trivially, and then move upwards plugging in the known values of \\(x\\) and solving for the remaining unknown in each row (each equation).\n\n\\(x_{n}=b_{n}/A_{nn}\\)\nNow for \\(k&lt;n\\), use the already computed \\(\\{x_{n},x_{n-1},\\ldots,x_{k+1}\\}\\) to calculate \\(x_{k}=\\frac{b_{k}-\\sum_{j=k+1}^{n}x_{j}A_{kj}}{A_{kk}}\\).\nRepeat for all rows.\n\nHow many multiplies and adds are done? Solving lower triangular systems is very similar and involves the same number of calculations.\nIn Scipy, we can use linalg.solve_triangular to solve triangular systems. The trans argument indicates whether to solve \\(Ax=b\\) or \\(A^{\\top}x=b\\) (so one wouldn’t need to transpose before solving).\n\nimport scipy as sp\nnp.random.seed(1)\nn = 20\nX = np.random.normal(size = (n,n))\n## R has the `crossprod` function, which would be more efficient\n## than having to transpose, but numpy doesn't seem to have an equivalent.\nA = X.T @ X  # produce a positive def. matrix \n\nb = np.random.normal(size = n)\nL = np.linalg.cholesky(A) # L is lower-triangular\nU = L.T\n\nout1 = sp.linalg.solve_triangular(L, b, lower=True)\nout2 = np.linalg.inv(L) @ b\nnp.allclose(out1, out2)\n\nTrue\n\nout3 =  sp.linalg.solve_triangular(U, b, lower=False)\nout4 = np.linalg.inv(U) @ b\nnp.allclose(out1, out2)\n\nTrue\n\n\nTo reiterate the distinction between matrix inversion and solving a system of equations, when we write \\(U^{-1}b\\), what we mean on a computer is to carry out the above algorithm, not to find the inverse and then multiply.\nHere’s a good reason why.\n\nimport time\n\nnp.random.seed(1)\nn = 5000\nX = np.random.normal(size = (n,n))\n\n## R has the `crossprod` function, which would be more efficient\n## than having to transpose, but numpy doesn't seem to have an equivalent.\nA = X.T @ X\nb = np.random.normal(size = n)\nL = np.linalg.cholesky(A) # L is lower-triangular\n\nt0 = time.time()\nout1 = sp.linalg.solve_triangular(L, b, lower=True)\ntime.time() - t0\n\n0.032052040100097656\n\nt0 = time.time()\nout2 = np.linalg.inv(L) @ b\ntime.time() - t0\n\n4.630963087081909\n\n\nThat assumes you have \\(L\\), but we’ll see in a bit that even when one accounts for the creation of \\(L\\), you don’t want to invert matrices in order to solve systems of equations."
  },
  {
    "objectID": "units/unit10-linalg.html#gaussian-elimination-lu-decomposition",
    "href": "units/unit10-linalg.html#gaussian-elimination-lu-decomposition",
    "title": "Numerical linear algebra",
    "section": "Gaussian elimination (LU decomposition)",
    "text": "Gaussian elimination (LU decomposition)\nGaussian elimination is a standard way of directly computing a solution for \\(Ax=b\\). It is equivalent to the LU decomposition. LU is primarily done with square matrices, but not always. Also LU decompositions do exist for some singular matrices.\nThe idea of Gaussian elimination is to convert the problem to a triangular system. In class, we’ll walk through Gaussian elimination in detail and see how it relates to the LU decomposition. I’ll describe it more briefly here. Following what we learned in algebra when we have multiple equations, we preserve the solution, \\(x\\), when we add multiples of rows (i.e., add multiples of equations) together. This amounts to doing \\(L_{1}Ax=L_{1}b\\) for a lower-triangular matrix \\(L_{1}\\) that produces all zeroes in the first column of \\(L_{1}A\\) except for the first row. We proceed to zero out values below the diagonal for the other columns of \\(A\\). The result is \\(L_{n-1}\\cdots L_{1}Ax\\equiv Ux=L_{n-1}\\cdots L_{1}b\\equiv b^{*}\\) where \\(U\\) is upper triangular. This is the forward reduction step of Gaussian elimination. Then the backward elimination step solves \\(Ux=b^{*}\\).\nIf we’re just looking for the solution of the system, we don’t need the lower-triangular factor \\(L=(L_{n-1}\\cdots L_{1})^{-1}\\) in \\(A=LU\\), but it turns out to have a simple form that is computed as we go along, it is unit lower triangular and the values below the diagonal are the negative of the values below the diagonals in \\(L_{1},\\ldots,L_{n-1}\\) (note that each \\(L_{j}\\) has non-zeroes below the diagonal only in the \\(j\\)th column). As a side note related to storage, it turns out that as we proceed, we can store the elements of \\(L\\) and \\(U\\) in the original \\(A\\) matrix, except for the implicit 1s on the diagonal of \\(L\\).\nIn class, we’ll work out the computational complexity of the LU and see that it is \\(O(n^{3})\\).\nIf we look at help(np.linalg.solve) in Python, we see that it uses *_gesv*. A Google search indicates that this is a Lapack routine that does the LU decomposition with partial pivoting and row interchanges (see below on what these are), so numpy is using the algorithm we’ve just discussed.\nWe can also explicitly get the LU decomposition in Python with scipy.linalg.lu(), though for most use cases, what we want to do is solve a system of equations. (In R, one can’t easily get the explicit LU decomposition, though solve() in R does use the LU.\nOne additional complexity is that we want to avoid dividing by very small values to avoid introducing numerical inaccuracy (we would get large values that might overwhelm whatever they are being added to, and small errors in the divisor will have large effects on the result). This can be done on the fly by interchanging equations to use the equation (row) that produces the largest value to divide by. For example in the first step, we would switch the first equation (first row) for whichever of the remaining equations has the largest value in the first column. This is called partial pivoting. The divisors are called pivots. Complete pivoting also considers interchanging columns, and while theoretically better, partial pivoting is generally sufficient and requires fewer computations. Partial pivoting can be expressed as multiplying along the way by permutation matrices, \\(P_{1},\\ldots P_{n-1}\\) that switch rows. One can show with some work that based on pivoting, we have \\(PA=LU\\), where \\(P=P_{n-1}\\cdots P_{1}\\). In the demo code, we’ll see a toy example of the impact of pivoting.\nFinally \\(|PA|=|P||A|=|L||U|=|U|\\) (why?) so \\(|A|=|U|/|P|\\) and since the determinant of each permutation matrix, \\(P_{j}\\) is -1 (except when \\(P_{j}=I\\) because we don’t need to switch rows), we just need to multiply by minus one if there is an odd number of permutations. Or if we know the matrix is non-negative definite, we just take the absolute value of \\(|U|\\). So Gaussian elimination provides a fast stable way to find the determinant."
  },
  {
    "objectID": "units/unit10-linalg.html#when-would-we-explicitly-invert-a-matrix",
    "href": "units/unit10-linalg.html#when-would-we-explicitly-invert-a-matrix",
    "title": "Numerical linear algebra",
    "section": "When would we explicitly invert a matrix?",
    "text": "When would we explicitly invert a matrix?\nIn some cases (as we discussed in class) you actually need the inverse as the output (e.g., for estimating standard errors).\nBut if you are just needing the inverse to multiply it by a vector or another matrix, you’re better off using a decomposition. Basically, one can work out that the cost of computing the inverse and doing subsequent multiplication with it is rather more than computing the decomposition and doing subsequent multiplications with it, regardless of how many columns there are in the matrix you are multiplying by.\nWith numpy, that may mean computing the decomposition and then carrying out the steps to do the multiplication using the decomposition or using np.linalg.solve(A, B) which, as mentioned above, uses the LU behind the scenes. One would not do np.linalg.inv(A) @ B."
  },
  {
    "objectID": "units/unit10-linalg.html#cholesky-decomposition",
    "href": "units/unit10-linalg.html#cholesky-decomposition",
    "title": "Numerical linear algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\nWhen \\(A\\) is p.d., we can use the Cholesky decomposition to solve a system of equations. Positive definite matrices can be decomposed as \\(U^{\\top}U=A\\) where \\(U\\) is upper triangular. \\(U\\) is called a square root matrix and is unique (apart from the sign, which we fix by requiring the diagonals to be positive). One algorithm for computing \\(U\\) is:\n\n\\(U_{11}=\\sqrt{A_{11}}\\)\nFor \\(j=2,\\ldots,n\\), \\(U_{1j}=A_{1j}/U_{11}\\)\nFor \\(i=2,\\ldots,n\\),\n\n\\(U_{ii}=\\sqrt{A_{ii}-\\sum_{k=1}^{i-1}U_{ki}^{2}}\\)\nif \\(i&lt;n\\), then for \\(j=i+1,\\ldots,n\\): \\(U_{ij}=(A_{ij}-\\sum_{k=1}^{i-1}U_{ki}U_{kj})/U_{ii}\\)\n\n\nWe can then solve a system of equations as: \\(U^{-1}(U^{\\top-1}b)\\).\nConfusingly, while numpy’s cholesky gives \\(L = U^\\top\\), scipy’s cholesky can return either \\(L\\) or \\(U\\) but defaults to \\(U\\).\nHere are two ways we can use the Cholesky to solve a system of equations:\n\nU = sp.linalg.cholesky(A)\nsp.linalg.solve_triangular(U, \n          sp.linalg.solve_triangular(U, b, lower=False, trans='T'),\n          lower=False)\n\nU, lower = sp.linalg.cho_factor(A)\nsp.linalg.cho_solve((U, lower), b)\n\nThe Cholesky has some nice advantages over the LU: (1) while both are \\(O(n^{3})\\), the Cholesky involves only half as many computations, \\(n^{3}/6+O(n^{2})\\) and (2) the Cholesky factorization has only \\((n^{2}+n)/2\\) unique values compared to \\(n^{2}+n\\) for the LU. Of course the LU is more broadly applicable. The Cholesky does require computation of square roots, but it turns out this is not too intensive. There is also a method for finding the Cholesky without square roots.\n\nUses of the Cholesky\nThe standard algorithm for generating \\(y\\sim\\mathcal{N}(0,A)\\) is:\n\nL = sp.linalg.cholesky(A, lower=True)\ny = L @ np.random.normal(size = n)\n\nQuestion: where will most of the time in this two-step calculation be spent?\nIf a regression design matrix, \\(X\\), is full rank, then \\(X^{\\top}X\\) is positive definite, so we could find \\(\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}Y\\) using either the Cholesky or Gaussian elimination.\nHowever, for OLS, it turns out that the standard approach is to work with \\(X\\) using the QR decomposition rather than working with \\(X^{\\top}X\\); working with \\(X\\) is more numerically stable, though in most situations without extreme collinearity, either of the approaches will be fine.\nWe could also consider the GLS estimator, which accounts for dependence in the errors: \\(\\hat{\\beta}=(X^{\\top}\\Sigma^{-1}X)^{-1}X^{\\top}\\Sigma^{-1}Y\\)\nChallenge: write efficient R code to carry out the GLS solution using the Cholesky factorization.\n\n\nNumerical issues with eigendecompositions and Cholesky decompositions for positive definite matrices\nMonahan comments that in general Gaussian elimination and the Cholesky decomposition are very stable. However, in the Cholesky case, if the matrix is very ill-conditioned we can get \\(A_{ii}-\\sum_{k}U_{ki}^{2}\\) being negative and then the algorithm stops when we try to take the square root. In this case, the Cholesky decomposition does not exist numerically although it exists mathematically. It’s not all that hard to produce such a matrix, particularly when working with high-dimensional covariance matrices with large correlations.\n\nlocs = np.random.uniform(size = 100)\nrho = .1\ndists = np.abs(locs[:, np.newaxis] - locs)\nC = np.exp(-dists**2/rho**2)\ne = np.linalg.eig(C)\nnp.sort(e[0])[::-1][96:100]\n\narray([-4.16702596e-16+0.j, -5.38621518e-16+0.j, -6.26206506e-16+0.j,\n       -6.98611709e-16+0.j])\n\ntry:\n    L = np.linalg.cholesky(C)\nexcept Exception as error:\n    print(error)\n  \n\nMatrix is not positive definite\n\nvals = np.abs(e[0])\nnp.max(vals)/np.min(vals)\n\n3.135151918122864e+18\n\n\nI don’t see a way to use pivoting with the Cholesky in Python, but in R, one can do chol(C, pivot = TRUE).\nWe can think about the accuracy here as follows. Suppose we have a matrix whose diagonal elements (i.e., the variances) are order of magnitude 1 and that the true value of a \\(U_{ii}\\) is less than \\(1\\times10^{-16}\\). From the given \\(A_{ii}\\) we are subtracting \\(\\sum_{k}U_{ki}^{2}\\) and trying to calculate this very small number but we know that we can only represent the values \\(A_{ii}\\) and \\(\\sum_{k}U_{ki}^{2}\\) accurately to 16 places, so the difference is garbage starting in the 17th position and could well be negative. Now realize that \\(\\sum_{k}U_{ki}^{2}\\) is the result of a potentially large set of arithmetic operations, and is likely represented accurately to fewer than 16 places. Now if the true value of \\(U_{ii}\\) is smaller than the accuracy to which \\(\\sum_{k}U_{ki}^{2}\\) is represented, we can get a difference that is negative.\nNote that when the Cholesky fails, we can still compute an eigendecomposition, but we have negative numeric eigenvalues. Even if all the eigenvalues are numerically positive (or equivalently, we’re able to get the Cholesky), errors in small eigenvalues near machine precision could have large effects when we work with the inverse of the matrix. This is what happens when we have columns of the \\(X\\) matrix nearly collinear. We cannot statistically distinguish the effect of two (or more) covariates, and this plays out numerically in terms of unstable results.\nA strategy when working with mathematically but not numerically positive definite \\(A\\) is to set eigenvalues or singular values to zero when they get very small, which amounts to using a pseudo-inverse and setting to zero any linear combinations with very small variance. We can also use pivoting with the Cholesky and accumulate zeroes in the last \\(n-q\\) rows (for cases where we try to take the square root of a negative number), corresponding to the columns of \\(A\\) that are numerically linearly dependent."
  },
  {
    "objectID": "units/unit10-linalg.html#qr-decomposition",
    "href": "units/unit10-linalg.html#qr-decomposition",
    "title": "Numerical linear algebra",
    "section": "QR decomposition",
    "text": "QR decomposition\n\nIntroduction\nThe QR decomposition is available for any matrix, \\(X=QR\\), with \\(Q\\) orthogonal and \\(R\\) upper triangular. If \\(X\\) is non-square, \\(n\\times p\\) with \\(n&gt;p\\) then the leading \\(p\\) rows of \\(R\\) provide an upper triangular matrix (\\(R_{1}\\)) and the remaining rows are 0. (I’m using \\(p\\) because the QR is generally applied to design matrices in regression). In this case we really only need the first \\(p\\) columns of \\(Q\\), and we have \\(X=Q_{1}R_{1}\\), the ‘skinny’ QR (this is what R’s QR provides). For uniqueness, we can require the diagonals of \\(R\\) to be nonnegative, and then \\(R\\) will be the same as the upper-triangular Cholesky factor of \\(X^{\\top}X\\):\n\\[\\begin{aligned} X^{\\top}X & = & R^{\\top}Q^{\\top}QR \\\\ & = & R^{\\top}R\\end{aligned}.\\]\nThere are three standard approaches for computing the QR, using (1) reflections (Householder transformations), (2) rotations (Givens transformations), or (3) Gram-Schmidt orthogonalization (see below for details).\nFor \\(n\\times n\\) \\(X\\), the QR (for the Householder approach) requires \\(2n^{3}/3\\) flops, so QR is less efficient than LU or Cholesky.\nWe can also obtain the pseudo-inverse of \\(X\\) from the QR: \\(X^{+}=[R_{1}^{-1}\\,0]Q^{\\top}\\). In the case that \\(X\\) is not full-rank, there is a version of the QR that will work (involving pivoting) and we end up with some additional zeroes on the diagonal of \\(R_{1}\\).\n\n\nRegression and the QR\nOften QR is used to fit linear models, including in R. Consider the linear model in the form \\(Y=X\\beta+\\epsilon\\), finding \\(\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}Y\\). Let’s consider the skinny QR and note that \\(R^{\\top}\\) is invertible. Therefore, we can express the normal equations as\n\\[\n\\begin{aligned}\nX^{\\top}X\\beta & = & X^{\\top} Y \\\\\nR^{\\top}Q^{\\top}QR\\beta & = & R^{\\top}Q^{\\top} Y \\\\\nR \\beta & = & Q^{\\top} Y\n\\end{aligned}\n\\]\nand solving for \\(\\beta\\) is just a backsolve since \\(R\\) is upper-triangular. Furthermore the standard regression quantities, such as the hat matrix, the SSE, the residuals, etc. can be easily expressed in terms of \\(Q\\) and \\(R\\).\nWhy use the QR instead of the Cholesky on \\(X^{\\top}X\\)? The condition number of \\(X\\) is the square root of that of \\(X^{\\top}X\\), and the \\(QR\\) factorizes \\(X\\). Monahan has a discussion of the condition of the regression problem, but from a larger perspective, the situations where numerical accuracy is a concern are generally cases where the OLS estimators are not particularly helpful anyway (e.g., highly collinear predictors).\nWhat about computational order of the different approaches to least squares? The Cholesky is \\(np^{2}+\\frac{1}{3}p^{3}\\), an algorithm called sweeping is \\(np^{2}+p^{3}\\) , the Householder method for QR is \\(2np^{2}-\\frac{2}{3}p^{3}\\), and the modified Gram-Schmidt approach for QR is \\(2np^{2}\\). So if \\(n\\gg p\\) then Cholesky (and sweeping) are faster than the QR approaches. According to Monahan, modified Gram-Schmidt is most numerically stable and sweeping least. In general, regression is pretty quick unless \\(p\\) is large since it is linear in \\(n\\), so it may not be worth worrying too much about computational differences of the sort noted here.\n\n\nRegression and the QR in Python and R\nWe can get the Q and R matrices easily in Python.\n\nQ,R = np.linalg.qr(X)\n\nOne of the methods used by the statsmodel package in Python uses the QR to fit a regression.\nNote that by default in Python (and in R), you get the skinny QR, namely only the first \\(p\\) rows of \\(R\\) and the first \\(p\\) columns of \\(Q\\), where the latter form an orthonormal basis for the column space of \\(X\\). The remaining columns form an orthonormal basis for the null space of \\(X\\) (the space orthogonal to the column space of \\(X\\)). The analogy in regression is that we get the basis vectors for the regression, while adding the remaining columns gives us the full \\(n\\)-dimensional space of the observations.\nRegression in R uses the QR decomposition via qr(), which calls a Fortran function. qr() (and the Fortran functions that are called) is specifically designed to output quantities useful in fitting linear models.\nIn R, qr() returns the result as a list meant for use by other tools. R stores the \\(R\\) matrix in the upper triangle of \\$qr, while the lower triangle of $qr and $aux store the information for constructing \\(Q\\) (this relates to the Householder-related vectors \\(u\\) below). One can multiply by \\(Q\\) using qr.qy() and by \\(Q^{\\top}\\) using qr.qty(). If you want to extract \\(R\\) and \\(Q\\), the following will work:\n\nX.qr = qr(X)\nQ = qr.Q(X.qr)\nR = qr.R(X.qr) \n\nAs a side note, there are QR-based functions that provide regression-related quantities, such as qr.resid(), qr.fitted() and qr.coef(). These functions (and their Fortran counterparts) exist because one can work through the various regression quantities of interest and find their expressions in terms of \\(Q\\) and \\(R\\), with nice properties resulting from \\(Q\\) being orthogonal and \\(R\\) triangular.\n\n\nComputing the QR decomposition\nHere we’ll see some of the details of the different approaches to the QR, in part because they involve some concepts that may be useful in other contexts. I won’t expect you to see all of how this works, but please skim through this to get an idea of how things are done.\nOne approach involves reflections of vectors and a second rotations of vectors. Reflections and rotations are transformations that are performed by orthogonal matrices. The determinant of a reflection matrix is -1 and the determinant of a rotation matrix is 1. We’ll see some of the details in the demo code.\n\nQR Method 1: Reflections\nIf \\(u\\) and \\(v\\) are orthonormal vectors and \\(x\\) is in the space spanned by \\(u\\) and \\(v\\), \\(x=c_{1}u+c_{2}v\\), then \\(\\tilde{x}=-c_{1}u+c_{2}v\\) is a reflection (a Householder reflection) along the \\(u\\) dimension (since we are using the negative of that basis vector). We can think of this as reflecting across the plane perpendicular to \\(u\\). This extends simply to higher dimensions with orthonormal vectors, \\(u,v_{1},v_{2},\\ldots\\)\nSuppose we want to formulate the reflection in terms of a “Householder” matrix, \\(Q\\). It turns out that \\[Qx=\\tilde{x}\\] if \\(Q=I-2uu^{\\top}\\). \\(Q\\) has the following properties: (1) \\(Qu=-u\\), (2) \\(Qv=v\\) for \\(u^{\\top}v=0\\), (3) \\(Q\\) is orthogonal and symmetric.\nOne way to create the QR decomposition is by a series of Householder transformations that create an upper triangular \\(R\\) from \\(X\\):\n\\[\n\\begin{aligned}\nR & = & Q_{p}\\cdots Q_{1} X \\\\\nQ & = & (Q_{p}\\cdots Q_{1})^{\\top}\n\\end{aligned}\n\\]\nwhere we make use of the symmetry in defining \\(Q\\).\nBasically \\(Q_{1}\\) reflects the first column of \\(X\\) with respect to a carefully chosen \\(u\\), so that the result is all zeroes except for the first element. We want \\(Q_{1}x=\\tilde{x}=(||x||,0,\\ldots,0)\\). This can be achieved with \\(u=\\frac{x-\\tilde{x}}{||x-\\tilde{x}||}\\). Then \\(Q_{2}\\) makes the last \\(n-2\\) rows of the second column equal to zero. We’ll work through this a bit in class.\nIn the regression context, as we work through the individual transformations, \\(Q_{j}=I-2u_{j}u_{j}^{\\top}\\), we apply them to \\(X\\) and \\(Y\\) to create \\(R\\) (note this would not involve doing the full matrix multiplication - think about what calculations are actually needed) and \\(QY=Q^{\\top}Y\\), and then solve \\(R\\beta=Q^{\\top}Y\\). To find \\(\\mbox{Cov}(\\hat{\\beta})\\propto(X^{\\top}X)^{-1}=(R^{\\top}R)^{-1}=R^{-1}R^{-\\top}\\) we do need to invert \\(R\\), but it’s upper-triangular and of dimension \\(p\\times p\\). It turns out that \\(Q^{\\top}Y\\) can be partitioned into the first \\(p\\) and the last \\(n-p\\) elements, \\(z^{(1)}\\) and \\(z^{(2)}\\). The SSR is \\(\\|z^{(1)}\\|^{2}\\) and SSE is \\(\\|z^{(2)}\\|^{2}\\).\nFinal side note: if \\(X\\) is square (so \\(n=p)\\) you might wonder why we need \\(Q_{p}\\) since after \\(p-1\\) reflections, we don’t need to zero anything else out (since the last column of \\(R\\) has \\(n\\) non-zero elements). It turns out that if we go back to thinking about a Householder reflection in general, there is a lack of uniqueness in choosing \\(\\tilde{x}\\). It could either be \\((||x||,0,\\ldots,0)\\) or \\((-||x||,0,\\ldots,0)\\). For better numerical stability, one chooses from the two of those such that \\(x_{1}\\) is of the opposite sign to \\(\\tilde{x}_{1}\\), so that one avoids cancellation of numbers that may be of the same magnitude when doing \\(x-\\tilde{x}\\). The transformation \\(Q_{p}\\) is the last step of taking that approach of choosing the sign at each step. \\(Q_{p}\\) doesn’t zero anything out; it just basically just involves potentially setting \\(R_{pp}\\) to be \\(-R_{pp}\\). (To be honest, I’m not clear on why one would bother to do that last step, but that seems to be how it is presented in discussions of the Householder approach.) Of course in the case of \\(p&lt;n\\), we definitely need \\(Q_{p}\\) so that the last \\(n-p\\) rows of \\(R\\) are zero and we can then discard them when just using the skinny QR.\n\n\nQR Method 2: Rotations\nA Givens rotation matrix rotates a vector in a two-dimensional subspace to be axis oriented with respect to one of the two dimensions by changing the value of the other dimension. E.g. we can create \\(\\tilde{x}=(x_{1},\\ldots,\\tilde{x}_{p},\\ldots,0,\\ldots x_{n})\\) from \\(x=(x_{1,}\\ldots,x_{p},\\ldots,x_{q},\\ldots,x_{n})\\) using a matrix multiplication: \\(\\tilde{x}=Qx\\). \\(Q\\) is orthogonal but not symmetric.\nWe can use a series of Givens rotations to do the QR but unless it is done carefully, more computations are needed than with Householder reflections. The basic story is that we apply a series of Givens rotations to \\(X\\) such that we zero out the lower triangular elements.\n\\[\n\\begin{aligned}\nR & = & Q_{pn}\\cdots Q_{23}Q_{1n}\\cdots Q_{13}Q_{12} X \\\\\nQ & = & (Q_{pn}\\cdots Q_{12})^{\\top}\\end{aligned}.\n\\]\nNote that we create the \\(n-p\\) zero rows in \\(R\\) (because the calculations affect the upper triangle of \\(R\\)), but we can then ignore those rows and the corresponding columns of \\(Q\\).\n\n\nQR Method 3: Gram-Schmidt Orthogonalization\nGram-Schmidt involves finding a set of orthonormal vectors to span the same space as a set of LIN vectors, \\(x_{1},\\ldots,x_{p}\\). If we take the LIN vectors to be the columns of \\(X\\), so that we are discussing the column space of \\(X\\), then G-S yields the QR decomposition. Here’s the algorithm:\n\n\\(\\tilde{x}_{1}=\\frac{x_{1}}{\\|x_{1}\\|}\\) (normalize the first vector)\nOrthogonalize the remaining vectors with respect to \\(\\tilde{x}_{1}\\):\n\n\\(\\tilde{x}_{2}=\\frac{x_{2}-\\tilde{x}_{1}^{\\top}x_{2}\\tilde{x}_{1}}{\\|x_{2}-\\tilde{x}_{1}^{\\top}x_{2}\\tilde{x}_{1}\\|}\\), which orthogonalizes with respect to \\(\\tilde{x}_{1}\\) and normalizes. Note that \\(\\tilde{x}_{1}^{\\top}x_{2}\\tilde{x}_{1}=\\langle\\tilde{x}_{1},x_{2}\\rangle\\tilde{x}_{1}\\). So we are finding a scaling, \\(c\\tilde{x}_{1}\\), where \\(c\\) is based on the inner product, to remove the variation in the \\(x_{1}\\) direction from \\(x_{2}\\).\nFor \\(k&gt;2\\), find interim vectors, \\(x_{k}^{(2)}\\), by orthogonalizing with respect to \\(\\tilde{x}_{1}\\)\n\nProceed for \\(k=3,\\ldots\\), in turn orthogonalizing and normalizing the first of the remaining vectors w.r.t. \\(\\tilde{x}_{k-1}\\) and orthogonalizing the remaining vectors w.r.t. \\(\\tilde{x}_{k-1}\\) to get new interim vectors\n\nMathematically, we could instead orthogonalize \\(x_{2}\\) w.r.t. \\(\\tilde{x}_{1}\\), then orthogonalize \\(x_{3}\\) w.r.t. \\(\\{\\tilde{x}_{1},\\tilde{x}_{2}\\}\\), etc. The algorithm above is the modified G-S, and is known to be more numerically stable if the columns of \\(X\\) are close to collinear, giving vectors that are closer to orthogonal. The resulting \\(\\tilde{x}\\) vectors are the columns of \\(Q\\). The elements of \\(R\\) are obtained as we proceed: the diagonal values are the the normalization values in the denominators, while the off-diagonals are the inner products with the already-computed columns of \\(Q\\) that are computed as part of the numerators.\nAnother way to think about this is that \\(R=Q^{\\top}X\\), which is the same as regressing the columns of \\(X\\) on \\(Q,\\) since \\((Q^{\\top}Q)^{-1}Q^{\\top}X=Q^{\\top}X\\). By construction, the first column of \\(X\\) is a scaling of the first column of \\(Q\\), the second column of \\(X\\) is a linear combination of the first two columns of \\(Q\\), etc., so \\(R\\) being upper triangular makes sense.\n\n\n\nThe “tall-skinny” QR\nSuppose you have a very large regression problem, with \\(n\\) very large, and \\(n\\gg p\\). There is a variant of the QR, called the tall-skinny QR (see http://arxiv.org/pdf/0808.2664v1.pdf for details) that allows us to find the decomposition in a parallel fashion. The basic idea is to do a nested set of QR decompositions on blocks of rows of \\(X\\):\n\\[\nX  =  \\left( \\begin{array}{c}\nX_{0} \\\\\nX_{1} \\\\\nX_{2} \\\\\nX_{3}\n\\end{array}\n\\right) =\n\\left(\n\\begin{array}{c}\nQ_{0} R_{0} \\\\\nQ_{1} R_{1} \\\\\nQ_{2} R_{2} \\\\\nQ_{3} R_{3}\n\\end{array} \\right),\n\\]\nfollowed by ‘reduction’ steps (this can be done in a map-reduce context) that do the \\(QR\\) of pairs of the \\(R\\) factors: \\[\\left(\\begin{array}{c}\nR_{0}\\\\\nR_{1}\\\\\nR_{2}\\\\\nR_{3}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\left(\\begin{array}{c}\nR_{0}\\\\\nR_{1}\n\\end{array}\\right)\\\\\n\\left(\\begin{array}{c}\nR_{2}\\\\\nR_{3}\n\\end{array}\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\nQ_{01}R_{01}\\\\\nQ_{23}R_{23}\n\\end{array}\\right)\\] and \\[\\left(\\begin{array}{c}\nR_{01}\\\\\nR_{23}\n\\end{array}\\right)=Q_{0123}R_{0123}.\\]\nThe full decomposition is then\n\\[X=\\left( \\begin{array}{cccc} Q_{0} & 0 & 0 & 0 \\\\ 0 & Q_{1} & 0 & 0 \\\\ 0 & 0 & Q_{2} & 0 \\\\ 0 & 0 & 0 & Q_{3} \\end{array} \\right) \\left( \\begin{array}{cc} Q_{01} & 0 \\\\ 0 & Q_{23} \\end{array} \\right) Q_{0123} R_{0123} = QR.\\]\nThe computation can be done in parallel (in particular it can be done with map-reduce) and the \\(Q\\) matrix for big problems would generally not be computed explicitly but would be stored in its constituent pieces.\nAlternatively, there is a variant on the algorithm that processes the row-blocks of \\(X\\) serially, allowing you to do QR on a large tall-skinny matrix that you can’t fit in memory (or possibly even on disk). First you do \\(QR\\) on \\(X_{0}\\) to get \\(Q_{0}R_{0}\\). Then you stack \\(R_{0}\\) on top of \\(X_{1}\\) and do QR to get \\(R_{01}\\). Then stack \\(R_{01}\\) on top of \\(X_{2}\\) to get \\(R_{012}\\), etc."
  },
  {
    "objectID": "units/unit10-linalg.html#determinants",
    "href": "units/unit10-linalg.html#determinants",
    "title": "Numerical linear algebra",
    "section": "Determinants",
    "text": "Determinants\nThe absolute value of the determinant of a square matrix can be found from the product of the diagonals of the triangular matrix in any factorization that gives a triangular (including diagonal) matrix times an orthogonal matrix (or matrices) since the determinant of an orthogonal matrix is either one or minus one.\n\\(|A|=|QR|=|Q||R|=\\pm|R|\\)\n\\(|A^{\\top}A|=|(QR)^{\\top}QR|=|R^{\\top}R|=|R_{1}^{\\top}R_{1}|=|R_{1}|^{2}\\)\nIn Python, the following will do it (on the log scale).\n\nQ,R = qr(A)\nmagn = np.sum(np.log(np.abs(np.diag(R)))) \n\nAn alternative is the product of the diagonal elements of \\(D\\) (the singular values) in the SVD factorization, \\(A=UDV^{\\top}\\).\nFor non-negative definite matrices, we know the determinant is non-negative, so the uncertainty about the sign is not an issue. For positive definite matrices, a good approach is to use the product of the diagonal elements of the Cholesky decomposition.\nOne can also use the product of the eigenvalues: \\(|A|=|\\Gamma\\Lambda\\Gamma^{-1}|=|\\Gamma||\\Gamma^{-1}||\\Lambda|=|\\Lambda|\\)\n\nComputation\nComputing from any of these diagonal or triangular matrices as the product of the diagonals is prone to overflow and underflow, so we always work on the log scale as the sum of the log of the values. When some of these may be negative, we can always keep track of the number of negative values and take the log of the absolute values.\nOften we will have the factorization as a result of other parts of the computation, so we get the determinant for free.\nWe can use np.linalg.logdet() or (definitely not recommended) np.linalg.det() to calculate the determinant in Python. These functions use the LU decomposition."
  },
  {
    "objectID": "units/unit10-linalg.html#eigendecomposition",
    "href": "units/unit10-linalg.html#eigendecomposition",
    "title": "Numerical linear algebra",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\nThe eigendecomposition (spectral decomposition) is useful in considering convergence of algorithms and of course for statistical decompositions such as PCA. We think of decomposing the components of variation into orthogonal patterns (the eigenvectors) with variances (eigenvalues) associated with each pattern.\nSquare symmetric matrices have real eigenvectors and eigenvalues, with the factorization into orthogonal \\(\\Gamma\\) and diagonal \\(\\Lambda\\), \\(A=\\Gamma\\Lambda\\Gamma^{\\top}\\), where the eigenvalues on the diagonal of \\(\\Lambda\\) are ordered in decreasing value. Of course this is equivalent to the definition of an eigenvalue/eigenvector pair as a pair such that \\(Ax=\\lambda x\\) where \\(x\\) is the eigenvector and \\(\\lambda\\) is a scalar, the eigenvalue. The inverse of the eigendecomposition is simply \\(\\Gamma\\Lambda^{-1}\\Gamma^{\\top}\\). On a similar note, we can create a square root matrix, \\(\\Gamma\\Lambda^{1/2}\\), by taking the square roots of the eigenvalues.\nThe spectral radius of \\(A\\), denoted \\(\\rho(A)\\), is the maximum of the absolute values of the eigenvalues. As we saw when talking about ill-conditionedness, for symmetric matrices, this maximum is the induced norm, so we have \\(\\rho(A)=\\|A\\|_{2}\\). It turns out that \\(\\rho(A)\\leq\\|A\\|\\) for any induced matrix norm. The spectral radius comes up in determining the rate of convergence of some iterative algorithms.\n\nComputation\nThere are several methods for eigenvalues; a common one for doing the full eigendecomposition is the QR algorithm. The first step is to reduce \\(A\\) to upper Hessenburg form, which is an upper triangular matrix except that the first subdiagonal in the lower triangular part can be non-zero. For symmetric matrices, the result is actually tridiagonal. We can do the reduction using Householder reflections or Givens rotations. At this point the QR decomposition (using Givens rotations) is applied iteratively (to a version of the matrix in which the diagonals are shifted), and the result converges to a diagonal matrix, which provides the eigenvalues. It’s more work to get the eigenvectors, but they are obtained as a product of Householder matrices (required for the initial reduction) multiplied by the product of the \\(Q\\) matrices from the successive QR decompositions.\nWe won’t go into the algorithm in detail, but note that it involves manipulations and ideas we’ve seen already.\nIf only the largest (or the first few largest) eigenvalues and their eigenvectors are needed, which can come up in time series and Markov chain contexts, the problem is easier and can be solved by the power method. E.g., in a Markov chain context, steady state is reached through \\(x_{t}=A^{t}x_{0}\\). One can find the largest eigenvector by multiplying by \\(A\\) many times, normalizing at each step. \\(v^{(k)}=Az^{(k-1)}\\) and \\(z^{(k)}=v^{(k)}/\\|v^{(k)}\\|\\). There is an extension to find the \\(p\\) largest eigenvalues and their vectors. See the demo code in the qmd source file for an implementation (in R)."
  },
  {
    "objectID": "units/unit10-linalg.html#singular-value-decomposition",
    "href": "units/unit10-linalg.html#singular-value-decomposition",
    "title": "Numerical linear algebra",
    "section": "Singular value decomposition",
    "text": "Singular value decomposition\nLet’s consider an \\(n\\times m\\) matrix, \\(A\\), with \\(n\\geq m\\) (if \\(m&gt;n\\), we can always work with \\(A^{\\top})\\). This often is a matrix representing \\(m\\) features of \\(n\\) observations. We could have \\(n\\) documents and \\(m\\) words, or \\(n\\) gene expression levels and \\(m\\) experimental conditions, etc. \\(A\\) can always be decomposed as \\[A=UDV^{\\top}\\] where \\(U\\) and \\(V\\) are matrices with orthonormal columns (left and right eigenvectors) and \\(D\\) is diagonal with non-negative values (which correspond to eigenvalues in the case of square \\(A\\) and to squared eigenvalues of \\(A^{\\top}A\\)).\nThe SVD can be represented in more than one way. One representation is \\[A_{n\\times m}=U_{n\\times k}D_{k\\times k}V_{k\\times m}^{\\top}=\\sum_{j=1}^{k}D_{jj}u_{j}v_{j}^{\\top}\\] where \\(u_{j}\\) and \\(v_{j}\\) are the columns of \\(U\\) and \\(V\\) and where \\(k\\) is the rank of \\(A\\) (which is at most the minimum of \\(n\\) and \\(m\\) of course). The diagonal elements of \\(D\\) are the singular values.\nThat representation is as the sum of rank-one matrices (since each term is the scaled outer product of two vectors).\nIf \\(A\\) is positive semi-definite, the eigendecomposition is an SVD. Furthermore, \\(A^{\\top}A=VD^{2}V^{\\top}\\) and \\(AA^{\\top}=UD^{2}U^{\\top}\\), so we can find the eigendecomposition of such matrices using the SVD of \\(A\\) (for \\(AA^{\\top}\\) we need to fill out \\(U\\) to have \\(n\\) columns). Note that the squares of the singular values of \\(A\\) are the eigenvalues of \\(A^{\\top}A\\) and \\(AA^{\\top}\\).\nWe can also fill out the matrices to get \\[A=U_{n\\times n}D_{n\\times m}V_{m\\times m}^{\\top}\\] where the added rows and columns of \\(D\\) are zero with the upper left block the \\(D_{k\\times k}\\) from above.\n\nUses\nThe SVD is an excellent way to determine a matrix rank and to construct a pseudo-inverse (\\(A^{+}=VD^{+}U^{\\top})\\).\nWe can use the SVD to approximate \\(A\\) by taking \\(A\\approx\\tilde{A}=\\sum_{j=1}^{p}D_{jj}u_{j}v_{j}^{\\top}\\) for \\(p&lt;m\\). The Eckart-Minsky-Young theorem shows that the truncated SVD minimizes the Frobenius norm of \\(A-\\tilde{A}\\) over all possible rank-\\(p\\) approximations. As an example if we have a large image of dimension \\(n\\times m\\), we could hold a compressed version by a rank-\\(p\\) approximation using the SVD. The SVD is used a lot in clustering problems. For example, the Netflix prize was won based on a variant of SVD (in fact all of the top methods used variants on SVD, I believe).\nHere’s another way to think about the SVD in terms of transformations and bases. Applying the SVD to a vector, $ UDV^{}x$, carries out the following steps:\n\n\\(V^{\\top}x\\) expresses \\(x\\) in terms of weights for the columns of \\(V\\).\nMultiplying the result by \\(D\\) scales/stretches the weights.\nMultiplying by \\(U\\) produces the result, which is a weighted combination of columns of \\(U\\), spanning the column-space of \\(U\\).\n\nSo applying the SVD transforms \\(x\\) from the column space of \\(V\\) to the column space of \\(U\\).\n\n\nComputation\nThe basic algorithm (Golub-Reinsch) is similar to the QR method for the eigendecomposition. We use a series of Householder transformations on the left and right to reduce \\(A\\) to an upper bidiagonal matrix, \\(A^{(0)}\\). The post-multiplications (the transformations on the right) generate the zeros in the upper triangle. (An upper bidiagonal matrix is one with non-zeroes only on the diagonal and first subdiagonal above the diagonal). Then the algorithm produces a series of upper bidiagonal matrices, \\(A^{(0)}\\), \\(A^{(1)},\\) etc. that converge to a diagonal matrix, \\(D\\) . Each step is carried out by a sequence of Givens transformations:\n\\[\n\\begin{aligned}\nA^{(j+1)} & = & R_{m-2}^{\\top} R_{m-3}^{\\top} \\cdots R_{0}^{\\top} A^{(j)} T_{0} T_{1} \\cdots T_{m-2} \\\\\n& = & RA^{(j)} T\n\\end{aligned}.\n\\]\nThis eventually gives \\(A^{(...)}=D\\) and by construction, \\(U\\) (the product of the pre-multiplied Householder matrices and the \\(R\\) matrices) and \\(V\\) (the product of the post-multiplied Householder matrices and the \\(T\\) matrices) are orthogonal. The result is then transformed by a diagonal matrix to make the elements of \\(D\\) non-negative and by permutation matrices to order the elements of \\(D\\) in nonincreasing order.\n\n\nComputation for large tall-skinny matrices\nThe SVD can also be generated from a QR decomposition. Take \\(X=QR\\) and then do an SVD on the \\(R\\) matrix to get \\(X=QUDV^{\\top}=U^{*}DV^{\\top}\\). This is particularly helpful for the case when \\(X\\) is tall and skinny (suppose \\(X\\) is \\(n\\times p\\) with \\(n\\gg p\\)), because we can do the tall-skinny QR, and the resulting SVD on \\(R\\) is easy computationally if \\(p\\) is manageable."
  },
  {
    "objectID": "units/unit10-linalg.html#linear-algebra-in-python",
    "href": "units/unit10-linalg.html#linear-algebra-in-python",
    "title": "Numerical linear algebra",
    "section": "Linear algebra in Python",
    "text": "Linear algebra in Python\nSpeedups and storage savings can be obtained by working with matrices stored in special formats when the matrices have special structure. E.g., we might store a symmetric matrix as a full matrix but only use the upper or lower triangle. Banded matrices and block diagonal matrices are other common formats. Banded matrices are all zero except for \\(A_{i,i+c_{k}}\\) for some small number of integers, \\(c_{k}\\). Viewed as an image, these have bands. The bands are known as co-diagonals.\nNote that for many matrix decompositions, you can change whether all of the aspects of the decomposition are returned, or just some, which may speed calculations.\nScipy provides functionality for working with matrices in various ways, including the scipy.sparse module, which provides support for structured sparse matrices such as triangular and diagonal matrices as well as unstructured sparse matrices using various standard representations.\nSome useful packages in R for matrices are Matrix, spam, and bdsmatrix. Matrix can represent a variety of rectangular matrices, including triangular, orthogonal, diagonal, etc. and provides methods for various matrix calculations that are specific to the matrix type. spam handles general sparse matrices with fast matrix calculations, in particular a fast Cholesky decomposition. bdsmatrix focuses on block-diagonal matrices, which arise frequently in contexts where there is clustering that induces within-cluster correlation and cross-cluster independence.\nIn general, matrix operations in Python and R go to compiled C or Fortran code without much intermediate Python or R code, so they can actually be pretty efficient and are based on the best algorithms developed by numerical experts. The core libraries that are used are LAPACK and BLAS (the Linear Algebra PACKage and the Basic Linear Algebra Subroutines). As we’ve discussed in the parallelization unit, one way to speed up code that relies heavily on linear algebra is to make sure you have a BLAS library tuned to your machine. These include OpenBLAS (open source), Intel’s MKL, AMD’s ACML, and Apple’s vecLib.\nIf you use Conda, numpy will generally be linked against MKL or OpenBLAS (this will depend on the locations online of the packages being installed, i.e., the channel(s) used). With pip, numpy will generally be linked against OpenBLAS. it’s possible to install numpy so that it uses OpenBLAS. R can be linked to the shared object library file (.so file or .dylib on a Mac) for a fast BLAS. These BLAS libraries are also available in threaded versions that farm out the calculations across multiple cores or processors that share memory. More details are available in this SCF documentation.\nBLAS routines do vector operations (level 1), matrix-vector operations (level 2), and dense matrix-matrix operations (level 3). Often the name of the routine has as its first letter “d”, “s”, “c” to indicate the routine is double precision, single precision, or complex. LAPACK builds on BLAS to implement standard linear algebra routines such as eigendecomposition, solutions of linear systems, a variety of factorizations, etc."
  },
  {
    "objectID": "units/unit10-linalg.html#sparse-matrices",
    "href": "units/unit10-linalg.html#sparse-matrices",
    "title": "Numerical linear algebra",
    "section": "Sparse matrices",
    "text": "Sparse matrices\nAs an example of exploiting sparsity, we can use a standard format (CSR = compressed sparse row) in the Scipy sparse module:\nConsider the matrix to be row-major and store the non-zero elements in order in an array called data. Then create a array called indptr that stores the position of the first element of each row. Finally, have a array, indices that tells the column identity of each element.\n\nimport scipy.sparse as sparse\nmat = np.array([[0,0,1,0,10],[0,0,0,100,0],[0,0,0,0,0],[1000,0,0,0,0]])\nmat = sparse.csr_array(mat)\nmat.data\n\narray([   1,   10,  100, 1000])\n\nmat.indices  # column indices\n\narray([2, 4, 3, 0], dtype=int32)\n\nmat.indptr   # row pointers\n\n## Ideally don't first construct the dense matrix if it is large.\n\narray([0, 2, 3, 3, 4], dtype=int32)\n\nmat2 = sparse.csr_array((mat.data, mat.indices, mat.indptr))\nmat2.toarray()\n\narray([[   0,    0,    1,    0,   10],\n       [   0,    0,    0,  100,    0],\n       [   0,    0,    0,    0,    0],\n       [1000,    0,    0,    0,    0]])\n\n\nThat’s also how things are done in the spam package in R.\nWe can do a fast matrix multiply, \\(x = Ab\\), as follows in pseudo-code:\n    for(i in 1:nrows(A)){\n        x[i] = 0\n        # should also check that row is not empty...\n        for(j in (rowpointers[i]:(rowpointers[i+1]-1)) {\n            x[i] = x[i] + entries[j] * b[colindices[j]]\n        }   \n    }\nHow many computations have we done? Only \\(k\\) multiplies and \\(O(k)\\) additions where \\(k\\) is the number of non-zero elements of \\(A\\). Compare this to the usual \\(O(n^{2})\\) for dense multiplication.\nNote that for the Cholesky of a sparse matrix, if the sparsity pattern is fixed, but the entries change, one can precompute an optimal re-ordering that retains as much sparsity in \\(U\\) as possible. Then multiple Cholesky decompositions can be done more quickly as the entries change.\n\nBanded matrices\nSuppose we have a banded matrix \\(A\\) where the lower bandwidth is \\(p\\), namely \\(A_{ij}=0\\) for \\(i&gt;j+p\\) and the upper bandwidth is \\(q\\) (\\(A_{ij}=0\\) for \\(j&gt;i+q\\)). An alternative to reducing to \\(Ux=b^{*}\\) is to compute \\(A=LU\\) and then do two solutions, \\(U^{-1}(L^{-1}b)\\). One can show that the computational complexity of the LU factorization is \\(O(npq)\\) for banded matrices, while solving the two triangular systems is \\(O(np+nq)\\), so for small \\(p\\) and \\(q\\), the speedup can be dramatic.\nBanded matrices come up in time series analysis. E.g., moving average (MA) models produce banded covariance structures because the covariance is zero after a certain number of lags."
  },
  {
    "objectID": "units/unit10-linalg.html#low-rank-updates-optional",
    "href": "units/unit10-linalg.html#low-rank-updates-optional",
    "title": "Numerical linear algebra",
    "section": "Low rank updates (optional)",
    "text": "Low rank updates (optional)\nA transformation of the form \\(A-uv^{\\top}\\) is a rank-one update because \\(uv^{\\top}\\) is of rank one.\nMore generally a low rank update of \\(A\\) is \\(\\tilde{A}=A-UV^{\\top}\\) where \\(U\\) and \\(V\\) are \\(n\\times m\\) with \\(n\\geq m\\). The Sherman-Morrison-Woodbury formula tells us that \\[\\tilde{A}^{-1}=A^{-1}+A^{-1}U(I_{m}-V^{\\top}A^{-1}U)^{-1}V^{\\top}A^{-1}\\] so if we know \\(x_{0}=A^{-1}b\\), then the solution to \\(\\tilde{A}x=b\\) is \\(x+A^{-1}U(I_{m}-V^{\\top}A^{-1}U)^{-1}V^{\\top}x\\). Provided \\(m\\) is not too large, and particularly if we already have a factorization of \\(A\\), then \\(A^{-1}U\\) is not too bad computationally, and \\(I_{m}-V^{\\top}A^{-1}U\\) is \\(m\\times m\\). As a result \\(A^{-1}(U(\\cdots)^{-1}V^{\\top}x)\\) isn’t too bad.\nThis also comes up in working with precision matrices in Bayesian problems where we may have \\(A^{-1}\\) but not \\(A\\) (we often add precision matrices to find conditional normal distributions). An alternative expression for the formula is \\(\\tilde{A}=A+UCV^{\\top}\\), and the identity tells us \\[\\tilde{A}^{-1}=A^{-1}-A^{-1}U(C^{-1}+V^{\\top}A^{-1}U)^{-1}V^{\\top}A^{-1}\\]\nBasically Sherman-Morrison-Woodbury gives us matrix identities that we can use in combination with our knowledge of smart ways of solving systems of equations."
  },
  {
    "objectID": "units/unit5-programming.html",
    "href": "units/unit5-programming.html",
    "title": "Programming concepts",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "units/unit5-programming.html#string-processing-and-regular-expressions-in-python",
    "href": "units/unit5-programming.html#string-processing-and-regular-expressions-in-python",
    "title": "Programming concepts",
    "section": "String processing and regular expressions in Python",
    "text": "String processing and regular expressions in Python\nHere we’ll see functionality for working with strings in Python, focusing on regular expressions with the re package. This will augment our consideration of regular expressions in the shell, in particular by seeing how we can replace patterns in addition to finding them.\nThe re package provides Perl-style regular expressions, but it doesn’t seem to support named character classes such as [:digit:]. Instead use classes such as \\d and [0-9].\nIn Python, you apply a matching function and then query the result to get information about what was matched and where in the string.\n\ntext = \"Here's my number: 919-543-3300.\"\nm = re.search(\"\\d+\", text)\nm\n\n&lt;re.Match object; span=(18, 21), match='919'&gt;\n\nm.group()\n\n'919'\n\nm.start()\n\n18\n\nm.end()\n\n21\n\nm.span()\n\n(18, 21)\n\n\nNotice that that showed us only the first match.\nWe can instead use findall to get all the matches.\n\nre.findall(\"\\d+\", text)\n\n['919', '543', '3300']\n\n\nThis is equivalent to:\n\npattern = re.compile(\"\\d+\")\nre.findall(pattern, text)\n\n['919', '543', '3300']\n\n\nThe compile can be omitted and will be done implicitly, but is a good idea to do explicitly if you have a complex regex pattern that you will use repeatedly (e.g. on every line in a file). It is also a reminder that regular expressions is a separate language, that can be compiled into a program. The compilation results in an object that relies on finite state machines to match the pattern.\nTo ignore case, do the following:\n\ntext = \"That cat in the Hat\"\nre.findall(\"hat\", text, re.IGNORECASE)\n\n['hat', 'Hat']\n\n\nThere are several other regex flags (also called compilation flags) that can control the behavior of the matching engine in interesting ways (check out re.VERBOSE and re.MULTILINE for instance).\nWe can of course use list comprehension to work with multiple strings. But we need to be careful to check whether a match was found.\n\ndef return_group(pattern, txt):\n    m = re.search(pattern, txt)\n    if m:\n       return m.group()\n    else:\n       return None\n\ntext = [\"Here's my number: 919-543-3300.\", \"hi John, good to meet you\",\n        \"They bought 731 bananas\", \"Please call 1.919.554.3800\"]\n[return_group(\"\\d+\", str) for str in text]\n\n['919', None, '731', '1']\n\n\nWe can replace matching substrings with re.sub.\n\ntext = \"Here's my number: 919-543-3300.\"\nre.sub(\"\\d\", \"Z\", text   )\n\n\"Here's my number: ZZZ-ZZZ-ZZZZ.\"\n\n\nRecall that we can search for location-specific matches in relation to the start and end of a string.\n\ntext = \"hats are all that are important to a hatter.\"\nre.findall(\"^hat\\w+\", text)\n\n['hats']\n\n\nRecall that we can search based on repetitions (as already demonstrated with the \\w+ just above).\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 1.919.554.3800.\"\nre.findall(\"\\d{3}[-.]\\d{3}[-.]\\d{4}\", text)\n\n['919-543-3300', '919.554.3800']\n\n\nNext let’s consider grouping using ().\nHere’s a basic example of using grouping via parentheses with the OR operator.\n\ntext = \"At the site http://www.ibm.com. Some other text. ftp://ibm.com\"\nre.search(\"(http|ftp):\\\\/\\\\/\", text).group()\n\n'http://'\n\n\nHowever, if we want to find all the matches and try to use findall, we see that it returns only the captured groups when grouping operators are present, as discussed a bit in help(re.findall), so we’d need to add an additional grouping operator to capture the full pattern when using findall:\n\nre.findall(\"(http|ftp):\\\\/\\\\/\", text)  \n\n['http', 'ftp']\n\nre.findall(\"((http|ftp):\\\\/\\\\/)\", text) \n\n[('http://', 'http'), ('ftp://', 'ftp')]\n\n\nWhen you are searching for all occurrences of a pattern in a large text object, it may be beneficial to use finditer:\n\nit = re.finditer(\"(http|ftp):\\\\/\\\\/\", text)  # http or ftp followed by ://\n\nfor match in it:\n    match.span()\n\n(12, 19)\n(49, 55)\n\n\nThis method behaves lazily and returns an iterator that gives us one match at a time, and only scans for the next match when we ask for it. This is similar to the behavior we saw with pandas.read_csv(chunksize = n)\nAs another example, the phone number detection problem could have been done a bit more compactly (as well as more generally to allow for an initial “1-” or “1.”) as:\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 1.919.554.3800.\"\nre.findall(\"((1[-.])?(\\d{3}[-.]){1,2}\\d{4})\", text)\n\n[('919-543-3300', '', '543-'), ('1.919.554.3800', '1.', '554.')]\n\n\nQuestion: the above regex would actually match something that is not a valid phone number. What can go wrong?\nGroups are also used when we need to reference back to a detected pattern when doing a replacement. This is why they are sometimes referred to as “capturing groups”. For example, here we’ll find any numbers and add underscores before and after them:\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 919.554.3800.\"\nre.sub(\"([0-9]+)\", \"_\\\\1_\", text)\n\n\"Here's my number: _919_-_543_-_3300_. They bought _731_ bananas. Please call _919_._554_._3800_.\"\n\n\nHere we’ll remove commas not used as field separators.\n\ntext = '\"H4NY07011\",\"ACKERMAN, GARY L.\",\"H\",\"$13,242\",,,'\nre.sub(\"([^\\\",]),\", \"\\\\1\", text)\n\n'\"H4NY07011\",\"ACKERMAN GARY L.\",\"H\",\"$13242\",,,'\n\n\nHow does that work? Consider that “[^\\\",]” matches a character that is not a quote and not a comma. The regex is such a character followed by a comma, with the matched character saved in \\\\1 because of the grouping operator.\nGroups can also be given names, instead of having to refer to them by their numbers, but we will not demonstrate this here.\n\nChallenge: Suppose a text string has dates in the form “Aug-3”, “May-9”, etc. and I want them in the form “3 Aug”, “9 May”, etc. How would I do this regex?\n\nFinally, let’s consider where a match ends when there is ambiguity.\nAs a simple example consider that if we try this search, we match as many digits as possible, rather than returning the first “9” as satisfying the request for “one or more” digits.\n\ntext = \"See the 998 balloons.\"\nre.findall(\"\\d+\", text)\n\n['998']\n\n\nThat behavior is called greedy matching, and it’s the default. That example also shows why it is the default. What would happen if it were not the default?\nHowever, sometimes greedy matching doesn’t get us what we want.\nConsider this attempt to remove multiple html tags from a string.\n\ntext = \"Do an internship &lt;b&gt; in place &lt;/b&gt; of &lt;b&gt; one &lt;/b&gt; course.\"\nre.sub(\"&lt;.*&gt;\", \"\", text)\n\n'Do an internship  course.'\n\n\nNotice what happens because of greedy matching.\nOne way to avoid greedy matching is to use a ? after the repetition specifier.\n\nre.sub(\"&lt;.*?&gt;\", \"\", text)\n\n'Do an internship  in place  of  one  course.'\n\n\nHowever, that syntax is a bit frustrating because ? is also used to indicate 0 or 1 repetitions, making the regex a bit hard to read/understand.\n\nChallenge: Suppose I want to strip out HTML tags but without using the ? to avoid greedy matching. How can I be more careful in constructing my regex?"
  },
  {
    "objectID": "units/unit5-programming.html#special-characters-in-python",
    "href": "units/unit5-programming.html#special-characters-in-python",
    "title": "Programming concepts",
    "section": "Special characters in Python",
    "text": "Special characters in Python\nRecall that when characters are used for special purposes, we need to ‘escape’ them if we want them interpreted as the actual (literal) character. In what follows, I show this in Python, but similar manipulations are sometimes needed in the shell and in R.\nThis can get particularly confusing in Python as the backslash is also used to input special characters such as newline (\\n) or tab (\\t).\nHere are some examples of using special characters.\n\ntmp = \"Harry said, \\\"Hi\\\"\"\nprint(tmp)   # prints out without a newline -- this is hard to show in rendered doc\n\nHarry said, \"Hi\"\n\ntmp = \"Harry said, \\\"Hi\\\".\\n\"\nprint(tmp)      # prints out with the newline -- hard to show in rendered doc\n\nHarry said, \"Hi\".\n\ntmp = [\"azar\", \"foo\", \"hello\\tthere\\n\"]\nprint(tmp[2])\n\nhello   there\n\nre.search(\"[\\tZ]\", tmp[2])   ## search for a tab or a 'Z'\n\n&lt;re.Match object; span=(5, 6), match='\\t'&gt;\n\n\nHere are some examples of using various special characters in regex syntax.\n\n## Search for characters that are not 'z'\n## (using ^ as regular expression syntax)\nre.search(\"[^z]\", \"zero\")\n## Use list comprehension to show results for various input strings:\n\n&lt;re.Match object; span=(1, 2), match='e'&gt;\n\n[print(st + \":\\t\", re.search(\"[^z]\", st))\n    for st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]]\n\n## Search for either a '^' (as a regular character) or a 'z':\n\na^2:     &lt;re.Match object; span=(0, 1), match='a'&gt;\n93:  &lt;re.Match object; span=(0, 1), match='9'&gt;\nzzz:     None\nzit:     &lt;re.Match object; span=(1, 2), match='i'&gt;\nazar:    &lt;re.Match object; span=(0, 1), match='a'&gt;\n[None, None, None, None, None]\n\n[print(st + \":\\t\", re.search(\"[\\^z]\", st))\n    for st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]]\n\n## Search for exactly three characters\n## (using . as regular expression syntax)\n\na^2:     &lt;re.Match object; span=(1, 2), match='^'&gt;\n93:  None\nzzz:     &lt;re.Match object; span=(0, 1), match='z'&gt;\nzit:     &lt;re.Match object; span=(0, 1), match='z'&gt;\nazar:    &lt;re.Match object; span=(1, 2), match='z'&gt;\n[None, None, None, None, None]\n\n[print(st + \":\\t\", re.search(\"^.{3}$\", st))\n    for st in [\"abc\", \"1234\", \"def\"]]\n\n## Search for a period (as a regular character)\n\nabc:     &lt;re.Match object; span=(0, 3), match='abc'&gt;\n1234:    None\ndef:     &lt;re.Match object; span=(0, 3), match='def'&gt;\n[None, None, None]\n\n[print(st + \":\\t\", re.search(\"\\.\", st)) for st in [\"3.9\", \"27\", \"4.2\"]]\n\n3.9:     &lt;re.Match object; span=(1, 2), match='.'&gt;\n27:  None\n4.2:     &lt;re.Match object; span=(1, 2), match='.'&gt;\n[None, None, None]\n\n\n\nChallenge Explain why we use a single backslash to get a newline and double backslash to write out a Windows path in the examples here:\n\n\n## Suppose we want to use a \\ in our string:\nprint(\"hello\\nagain\")\n\nhello\nagain\n\nprint(\"hello\\\\nagain\")\n\nhello\\nagain\n\nprint(\"My Windows path is: C:\\\\Users\\\\nadal.\")\n\nMy Windows path is: C:\\Users\\nadal.\n\n\nAnother way to achieve this effect if your string does not contain any special characters is to prefix your string literal with an r for “raw”:\n\nprint(r\"My Windows path is: C:\\Users\\nadal.\")\n\nMy Windows path is: C:\\Users\\nadal.\n\n\nAdvanced note: Searching for an actual backslash gets even more complicated (lookup backslash plague or baskslash hell), because we need to pass two backslashes as the regular expression, so that a literal backslash is searched for. However, to pass two backslashes, we need to escape each of them with a backslash so Python doesn’t treat each backslash as part of a special character. So that’s four backslashes to search for a single backslash! Yikes. One rule of thumb is just to keep entering backslashes until things work!\n\n## Search for an actual backslash\ntmp = \"something \\ other\\n\"\nprint(tmp)  # notice the escaping of the literal backslash\n\nsomething \\ other\n\nre.search(\"\\\\\\\\\", tmp)\n\n&lt;re.Match object; span=(10, 11), match='\\\\'&gt;\n\ntry:\n    re.search(\"\\\\\", tmp)\nexcept Exception as error:\n    print(error)\n\nbad escape (end of pattern) at position 0\n\n\nAgain here you can use “raw” strings, at the price of losing the ability to use any special characters:\n\n## Search for an actual backslash\nre.search(r\"\\\\\", tmp)\n\n&lt;re.Match object; span=(10, 11), match='\\\\'&gt;\n\n\nThis tells python to treat this sting literal without any escaping, but does this does not apply to the regex engine (or else we would have used a single backslash). So yes. This can be quite confusing.\n\nWarning Be careful when cutting and pasting from documents that are not text files as you may paste in something that looks like a single or double quote, but which R cannot interpret as a quote because it’s some other ASCII quote character. If you paste in a ” from PDF, it will not be interpreted as a standard R double quote mark.\n\nSimilar things come up in the shell and in R, but in the shell you often don’t need as many backslashes. E.g. you could do this to look for a literal backslash character.\n\necho \"hello\" &gt; file.txt\necho \"with a \\ there\" &gt;&gt; file.txt\ngrep '\\\\' file.txt\n\n## Or without regular expressions\ngrep -F \"\\\" file.txt"
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-the-operating-system",
    "href": "units/unit5-programming.html#interacting-with-the-operating-system",
    "title": "Programming concepts",
    "section": "Interacting with the operating system",
    "text": "Interacting with the operating system\nScripting languages allow one to interact with the operating system in various ways. Most allow you to call out to the shell to run arbitrary shell code and save results within your session.\nI’ll assume everyone knows about the following functions/functionality for interacting with the filesystem and file in Python: os.getcwd, os.chdir, import, pickle.dump, pickle.load\nAlso in IPython there is additional functionality/syntax.\nHere are a variety of tools for interacting with the operating system:\n\nTo run UNIX commands from within Python, use subprocess.run(), as follows, noting that we can save the result of a system call to an R object:\n\nimport subprocess, io\nsubprocess.run([\"ls\", \"-al\"])   ## results apparently not shown when compiled...\n\nCompletedProcess(args=['ls', '-al'], returncode=0)\n\nfiles = subprocess.run([\"ls\", \"-al\"], capture_output = True)\nfiles.stdout\n\nb'total 3056\\ndrwxr-sr-x 13 paciorek scfstaff      79 Oct  2 11:38 .\\ndrwxr-sr-x 13 paciorek scfstaff      52 Oct  2 11:38 ..\\n-rw-r--r--  1 paciorek scfstaff  117142 Sep  7 18:13 chatgpt-regex-numbers.png\\n-rw-r--r--  1 paciorek scfstaff     176 Aug 29 13:46 debug_code.py\\n-rw-r--r--  1 paciorek scfstaff     216 Jul 20 08:08 debug_code.py~\\n-rw-r--r--  1 paciorek scfstaff      42 Aug 29 13:46 dummyfun.py\\n-rw-r--r--  1 paciorek scfstaff  175396 Aug 29 13:46 exampleGraphic.png\\n-rw-r--r--  1 paciorek scfstaff 1036183 Aug 29 13:46 file.txt\\n-rw-r--r--  1 paciorek scfstaff      59 Aug 29 13:46 foo.py\\n-rw-r--r--  1 paciorek scfstaff   20260 Jul 26 11:45 graph.png\\n-rw-r--r--  1 paciorek scfstaff     200 Aug 24 08:27 html.tips\\n-rw-r--r--  1 paciorek scfstaff      45 Aug 24 07:54 html.tips~\\ndrwxr-sr-x  2 paciorek scfstaff       3 Aug 28 16:39 .ipynb_checkpoints\\n-rw-r--r--  1 paciorek scfstaff    2464 Jul 26 11:45 linked-list.png\\n-rw-r--r--  1 paciorek scfstaff      98 Sep 21 11:37 local.py\\n-rw-r--r--  1 paciorek scfstaff      79 Aug 29 13:46 mymod.py\\ndrwxr-sr-x  4 paciorek scfstaff       8 Sep 15 17:26 mypkg\\n-rw-r--r--  1 paciorek scfstaff     401 Aug 29 13:46 mysqrt.py\\n-rw-r--r--  1 paciorek scfstaff   63998 Aug 18 11:19 normalized_example.png\\ndrwxr-sr-x  2 paciorek scfstaff      13 Oct  2 11:37 __pycache__\\n-rw-r--r--  1 paciorek scfstaff     223 Jul  9 14:10 q.py\\n-rw-r--r--  1 paciorek scfstaff     590 Jul 19 18:19 run_no_break2.py\\n-rw-r--r--  1 paciorek scfstaff     588 Jul 19 18:14 run_no_break2.py~\\n-rw-r--r--  1 paciorek scfstaff     381 Jul 19 17:50 run_no_break_full.py~\\n-rw-r--r--  1 paciorek scfstaff     573 Aug 29 13:46 run_no_break.py\\n-rw-r--r--  1 paciorek scfstaff     656 Jul 19 17:55 run_no_break.py~\\n-rw-r--r--  1 paciorek scfstaff     591 Aug 29 13:46 run_with_break.py\\n-rw-r--r--  1 paciorek scfstaff     656 Jul 19 17:53 run_with_break.py~\\n-rw-r--r--  1 paciorek scfstaff     385 Aug 29 13:46 stratified.py\\n-rw-r--r--  1 paciorek scfstaff     586 Jul 19 17:18 stratified.py~\\n-rw-r--r--  1 paciorek scfstaff     385 Jul 19 17:20 stratified_with_break.py~\\n-rw-r--r--  1 paciorek scfstaff      33 Jul 19 17:05 test2.py\\n-rw-r--r--  1 paciorek scfstaff      79 Sep 15 11:46 test3.py\\n-rw-r--r--  1 paciorek scfstaff      25 Jul 19 17:04 test.py~\\n-rw-r--r--  1 paciorek scfstaff     404 Aug 29 09:29 test.qmd\\n-rw-r--r--  1 paciorek scfstaff     354 Aug 28 16:38 test.qmd~\\n-rw-r--r--  1 paciorek scfstaff      66 Aug 29 13:46 test_scope.py\\n-rw-r--r--  1 paciorek scfstaff      18 Sep  8 14:57 test.txt\\n-rw-r--r--  1 paciorek scfstaff      10 Aug 31 08:01 tmp2.txt\\n-rw-r--r--  1 paciorek scfstaff       2 Aug 25 13:29 tmp3.txt\\n-rw-r--r--  1 paciorek scfstaff      57 Sep  8 07:53 tmp.qmd\\n-rw-r--r--  1 paciorek scfstaff      55 Sep  8 07:52 tmp.qmd~\\n-rw-r--r--  1 paciorek scfstaff      14 Aug 31 08:22 tmp.txt\\n-rw-r--r--  1 paciorek scfstaff       4 Aug 31 08:01 tmp.txt~\\n-rw-r--r--  1 paciorek scfstaff    9357 Jul 26 11:45 tree.png\\ndrwxr-sr-x  4 paciorek scfstaff       4 Jul 26 12:26 unit10-linalg_cache\\n-rw-r--r--  1 paciorek scfstaff   81631 Aug 29 13:46 unit10-linalg.qmd\\n-rw-r--r--  1 paciorek scfstaff   78863 Jul  5 15:41 unit10-linalg.qmd~\\n-rw-r--r--  1 paciorek scfstaff    9509 Aug 29 13:46 unit1-intro.md\\n-rw-r--r--  1 paciorek scfstaff    8908 Jul 26 07:47 unit1-intro.md~\\n-rw-r--r--  1 paciorek scfstaff   56222 Aug 31 08:00 unit2-dataTech.qmd\\n-rw-r--r--  1 paciorek scfstaff   52630 Jun  8 13:16 unit2-dataTech.qmd~\\n-rw-r--r--  1 paciorek scfstaff   18297 Aug 31 09:10 unit3-bash.qmd\\n-rw-r--r--  1 paciorek scfstaff   12674 Aug 26 12:07 unit3-bash.qmd~\\n-rw-r--r--  1 paciorek scfstaff   12674 Jul 26 11:50 unit3-bash.Rmd~\\n-rw-r--r--  1 paciorek scfstaff    3927 Aug 29 13:46 unit3-bash.sh\\n-rw-r--r--  1 paciorek scfstaff   41222 Sep  6 18:05 unit4-goodPractices.qmd\\n-rw-r--r--  1 paciorek scfstaff   16432 Jul 18 16:17 unit4-goodPractices.qmd~\\ndrwxr-sr-x  4 paciorek scfstaff       4 Jul 26 12:29 unit5-programming_cache\\ndrwxr-sr-x  5 paciorek scfstaff       5 Oct  2 11:37 unit5-programming_files\\n-rw-r--r--  1 paciorek scfstaff  127531 Oct  2 11:35 unit5-programming.qmd\\n-rw-r--r--  1 paciorek scfstaff  128126 Oct  2 11:38 unit5-programming.rmarkdown\\n-rw-r--r--  1 paciorek scfstaff  251605 Oct  2 11:38 unit5-programming.tex\\ndrwxr-sr-x  4 paciorek scfstaff       4 Jul 24 17:08 unit6-parallel_cache\\ndrwxr-sr-x  3 paciorek scfstaff       3 Oct  2 08:44 unit6-parallel_files\\n-rw-r--r--  1 paciorek scfstaff   50875 Oct  2 08:41 unit6-parallel.qmd\\n-rw-r--r--  1 paciorek scfstaff   45558 Jul 26 09:33 unit6-parallel.qmd~\\ndrwxr-sr-x  4 paciorek scfstaff       4 Jul 27 09:40 unit7-bigData_cache\\n-rw-r--r--  1 paciorek scfstaff   56433 Oct  2 09:55 unit7-bigData.qmd\\n-rw-r--r--  1 paciorek scfstaff   69916 Jul 26 17:52 unit7-bigData.qmd~\\ndrwxr-sr-x  4 paciorek scfstaff       4 May 25 17:14 unit8-numbers_cache\\ndrwxr-sr-x  3 paciorek scfstaff       3 Jul 26 12:27 unit8-numbers_files\\n-rw-r--r--  1 paciorek scfstaff   29633 Aug 29 13:46 unit8-numbers.qmd\\n-rw-r--r--  1 paciorek scfstaff   29174 May 24 12:22 unit8-numbers.qmd~\\n-rw-r--r--  1 paciorek scfstaff   42193 Aug 29 13:46 unit9-sim.qmd\\n-rw-r--r--  1 paciorek scfstaff   42193 Jul 10 10:58 unit9-sim.qmd~\\n-rw-r--r--  1 paciorek scfstaff      72 Aug 28 16:39 Untitled.ipynb\\n-rw-r--r--  1 paciorek scfstaff     142 Aug 29 13:46 vec_orig.py\\n-rw-r--r--  1 paciorek scfstaff     555 Oct  2 11:37 vec.pyc\\n'\n\nwith io.BytesIO(files.stdout) as stream:  # create a file-like object\n     content = stream.readlines()\ncontent[2:4]\n\n[b'drwxr-sr-x 13 paciorek scfstaff      52 Oct  2 11:38 ..\\n', b'-rw-r--r--  1 paciorek scfstaff  117142 Sep  7 18:13 chatgpt-regex-numbers.png\\n']\n\n\nThere are also a bunch of functions that will do specific queries of the filesystem, including\n\nos.path.exists(\"unit2-dataTech.qmd\")\n\nTrue\n\nos.listdir(\"../data\")\n\n['coop.txt.gz', 'test.db', 'cpds.csv', 'IPs.RData', 'airline.csv', 'stackoverflow-2016.db', 'airline.parquet', 'hivSequ.csv', 'RTADataSub.csv', 'stackoverflow-2021.db', 'precip.txt', 'precipData.txt']\n\n\nThere are some tools for dealing with differences between operating systems. os.path.join is a nice example:\n\nos.listdir(os.path.join(\"..\", \"data\"))\n\n['coop.txt.gz', 'test.db', 'cpds.csv', 'IPs.RData', 'airline.csv', 'stackoverflow-2016.db', 'airline.parquet', 'hivSequ.csv', 'RTADataSub.csv', 'stackoverflow-2021.db', 'precip.txt', 'precipData.txt']\n\n\nIt’s best if you can to write your code, as shown here with os.path.join, in a way that is agnostic to the underlying operating system.\nTo get some info on the system you’re running on:\n\nimport platform\nplatform.system()\n\n'Linux'\n\nos.uname()\n\nposix.uname_result(sysname='Linux', nodename='smeagol', release='5.15.0-73-generic', version='#80-Ubuntu SMP Mon May 15 15:18:26 UTC 2023', machine='x86_64')\n\nplatform.python_version()\n\n'3.11.0'\n\n\nTo retrieve environment variables:\n\nos.environ['PATH']\n\n'/system/linux/mambaforge-3.11/bin:/usr/local/linux/mambaforge-3.11/condabin:/system/linux/mambaforge-3.11/condabin:/system/linux/mambaforge-3.11/bin:/system/linux/mambaforge-3.11/condabin:/system/linux/mambaforge-3.11/bin:/system/linux/julia-1.8.5/bin:/system/linux/mambaforge-3.11/bin:/accounts/vis/paciorek/bin:/system/linux/bin:/usr/local/bin:/usr/bin:/usr/sbin:/usr/lib/rstudio-server/bin:/accounts/vis/paciorek/.local/bin'\n\n\nYou can have an Python script act as a shell script (like running a bash shell script) as follows.\n\nWrite your Python code in a text file, say example.py\nAs the first line of the file, include #!/usr/bin/python (like #!/bin/bash in a bash shell file, as seen in Unit 2) or for more portability across machines, include #!/usr/bin/env python.\nMake the Python code file executable with chmod: chmod ugo+x example.py.\nRun the script from the command line: ./example.py\n\nIf you want to pass arguments into your script, you can do so with the argparse package.\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('-y', '--year', default=2002,\n                    help='year to download')\nparser.add_argument('-m', '--month', default=None,\n                    help='month to download')\nargs = parse.parse_args()\nargs.year\nyear = int(args.year)\n\nNow we can run it as follows in the shell:\n\n./example.py 2004 January\n\nUse Ctrl-C to interrupt execution. This will generally back out gracefully, returning you to a state as if the command had not been started. Note that if Python is exceeding the amount of memory available, there can be a long delay. This can be frustrating, particularly since a primary reason you would want to interrupt is when Python runs out of memory."
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-external-code",
    "href": "units/unit5-programming.html#interacting-with-external-code",
    "title": "Programming concepts",
    "section": "Interacting with external code",
    "text": "Interacting with external code\nScripting languages such as R, Python, and Julia allow you to call out to “external code”, which often means C or C++ (but also Fortran, Java and other languages).\nCalling out to external code is particularly important in languages like R and Python that are often much slower than compiled code and less important in a fast language like Julia (which uses Just-In-Time compilation – more on that later).\nIn fact, the predecessor language to R, which was called ‘S’ was developed specifically (at AT&T’s Bell Labs in the 1970s and 1980s) as an interactive wrapper around Fortran, the numerical programming language most commonly used at the time (and still widely relied on today in various legacy codes).\nIn Python, one can directly call out to C or C++ code or one can use Cython to interact with C. With Cython, one can:\n\nHave Cython automatically translate Python code to C, if you provide type definitions for your variables.\nDefine C functions that can be called from your Python code.\n\nIn R, one can call directly out to C or C++ code using .Call or one can use the Rcpp package. Rcpp is specifically designed to be able to write C++ code that feels somewhat like writing R code and where it is very easy to pass data between R and C++."
  },
  {
    "objectID": "units/unit5-programming.html#modules",
    "href": "units/unit5-programming.html#modules",
    "title": "Programming concepts",
    "section": "Modules",
    "text": "Modules\nA module is a collection of related code in a file with the extension .py. The code can include functions, classes, and variables, as well as runnable code. To access the objects in the module, you need to import the module.\nHere we’ll create mymod.py from the shell, but of course usually one would create it in an editor.\n\ncat &lt;&lt; EOF &gt; mymod.py\nx = 7\nrange = 3\ndef myfun(x):\n    print(\"The arg is: \", str(x), \".\", sep = '')\nEOF\n\n\nimport mymod\nprint(mymod.x)\n\n7\n\nmymod.myfun(7)\n\nThe arg is: 7."
  },
  {
    "objectID": "units/unit5-programming.html#the-import-statement",
    "href": "units/unit5-programming.html#the-import-statement",
    "title": "Programming concepts",
    "section": "The import statement",
    "text": "The import statement\nThe import statement allows one to get access to code in a module. Importantly it associates the names of the objects in the module with a name accessible in the scope in which it was imported (i.e., the current context). The mapping of names (references) to objects is called a namespace. We discuss scopes and namespaces in more detail later.\n\ndel mymod\ntry:           # Check if mymod is in scope\n    mymod.x\nexcept Exception as error:\n    print(error)\n\nname 'mymod' is not defined\n\ny = 3\n\nimport mymod\nmymod         # essentially a dictionary in the current (global) scope\n\n&lt;module 'mymod' from '/accounts/vis/paciorek/teaching/243fall23/stat243-fall-2023/units/mymod.py'&gt;\n\nx             # not a name in the current (global) scope\n\nError: NameError: name 'x' is not defined\n\nrange         # a builtin, not from the module\n\n&lt;class 'range'&gt;\n\nmymod.x\n\n7\n\ndir(mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n\nmymod.x\n\n7\n\nmymod.range\n\n3\n\n\nSo y and mymod are in the global namespace and range and x are in the module namespace of mymod. You can access the built-in range function from the global namespace but it turns out it’s actually in the built-ins scope (more later).\nNote the usefulness of distinguishing the objects in a module from those in the global namespace. We’ll discuss this more in a bit.\nThat said, we can make an object defined in a module directly accessible in the current scope (adding it to the global namespace in this example) at which point it is distinct from the object in the module:\n\nfrom mymod import x\nx   # now part of global namespace\n\n7\n\ndir()\n\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'content', 'files', 'io', 'it', 'm', 'match', 'math', 'mymod', 'os', 'pattern', 'platform', 'r', 're', 'return_group', 'stream', 'subprocess', 'sys', 'text', 'time', 'tmp', 'x', 'y']\n\nmymod.x = 5\nx = 3\nmymod.x\n\n5\n\nx\n\n3\n\n\nBut in general we wouldn’t want to use from to import objects in that fashion because we could introduce name conflicts and we reduce modularity.\nThat said, it can be tedious to always have to type the module name (and in many cases there are multiple submodule names you’d also need to type).\n\nimport mymod as m\nm.x\n\n5"
  },
  {
    "objectID": "units/unit5-programming.html#packages",
    "href": "units/unit5-programming.html#packages",
    "title": "Programming concepts",
    "section": "Packages",
    "text": "Packages\nA package is a directory containing one or more modules and with a file named __init__.py that is called when a package is imported and serves to initialize the package.\nLet’s create a basic package.\n\nmkdir mypkg\n\ncat &lt;&lt; EOF &gt; mypkg/__init__.py\n## Make objects from mymod.py available as mypkg.foo\nfrom mymod import *\n\nprint(\"Welcome to my package.\")\nEOF\n\ncat &lt;&lt; EOF &gt; mypkg/mymod.py\nx = 7\n\ndef myfun(val):\n    print(\"The arg is: \", str(val), \".\", sep = '')\nEOF\n\nNote that if there were other modules, we could have imported from those as well.\nNow we can use the objects from the module without having to know that it was in a particular module (because of how __init__.py was set up).\n\nimport mypkg\n\nWelcome to my package.\n\nmypkg.x\n\n7\n\nmypkg.myfun(7)\n\nThe arg is: 7.\n\n\nNote, one can set __all__ in an __init__.py to define what is imported, which makes clear what is publicly available and hides what is considered internal.\n\nSubpackages\nPackages can also have modules in nested directories, achieving additional modularity via subpackages. A package can automatically import the subpackages via the main __init__.py or require the user to import them manually, e.g., import mypkg.mysubpkg.\n\nmkdir mypkg/mysubpkg\n\ncat &lt;&lt; EOF &gt; mypkg/mysubpkg/__init__.py\nfrom values import *\nprint(\"Welcome to my package's subpackage.\")\nEOF\n\ncat &lt;&lt; EOF &gt; mypkg/mysubpkg/values.py\nx = 999\nb = 7\nd = 9\nEOF\n\n\nimport mypkg.mysubpkg     ## Note that __init__.py is invoked\n\nWelcome to my package's subpackage.\n\nmypkg.mysubpkg.b\n\n7\n\n\nNote that a given __init__.py is invoked when importing anything nested within the directory containing the __init__.py.\nIf we wanted to automatically import the subpackage we would add import mypkg.mysubpkg to mypkg/__init__.py.\nOne would generally not import the items from mysubpkg directly into the mypkg namespace but there may be cases one would do something like this. For example np.linspace is actually found numpy/core/function_base.py, but we don’t need to refer to numpy.core.linspace."
  },
  {
    "objectID": "units/unit5-programming.html#installing-packages",
    "href": "units/unit5-programming.html#installing-packages",
    "title": "Programming concepts",
    "section": "Installing packages",
    "text": "Installing packages\nIf a package is on PyPI or available through Conda but not on your system, you can install it easily (usually). You don’t need root permission on a machine to install a package, though you may need to use pip install --user or set up a new Conda environment.\nPackages often depend on other packages. In general, if one package depends on another, pip or conda will generally install the dependency automatically.\nOne advantage of Conda is that it can also install non-Python packages on which a Python package depends, whereas with pip you sometimes need to install a system package to satisfy a dependency.\nIt’s not uncommon to run into a case where conda has trouble installing a package because of version inconsistencies amongst the dependencies. mamba is a drop-in replacement for conda and often does a better job of this “dependency resolution”. We use mamba by default on the SCF.\n\nReproducibility and package management\nFor reproducibility, it’s important to know the versions of the packages you use (and the version of Python). pip and conda make it easy to do this. You can create a requirements file that captures the packages you are currently using (and, critically, their versions) and then install exactly that set of packages (and versions) based on that requirements file.\npip freeze &gt; requirements.txt\npip install -r requirements.txt\n\nconda env export &gt; environment.yml\nconda env create -f environment.yml\nConda is a general package manager. You can use it to manage Python packages but lots of other software as well, including R and Julia.\nConda environments provide an additional layer of modularity/reproducibility, allowing you to set up a fully reproducible environment for your computation. Here (by explicitly giving python=3.11) the Python 3.11 executable and all packages you install in the environment are fully independent of whatever Python executables are installed on the system.\nconda create -n myenv python=3.11\nsource activate myenv\nconda install numpy\nSide note: if you use conda activate rather than source activate, Conda will prompt you to run conda init, which will make changes to your ~/.bashrc that, for one, activate the Conda base environment automatically when a shell is started. This may be fine, but it’s helpful to be aware.\n\n\nPackage locations\nPackages in Python (and in R, Julia, etc.) may be installed in various places on the filesystem, and it sometimes it is helpful (e.g., if you end up with multiple versions of a package installed on your system) to be able to figure out where on the filesystem the package is being loaded from. If you run pkgname.__file__, you’ll be able to see where the imported package is installed. pkname.__version__ will show the version of the package (as will pip list or conda list, for all packages).\nsys.path shows where Python looks for packages on your system.\n\n\nSource vs. binary packages\nThe difference between a source package and a binary package is that the source package has the raw Python (and C/C++ and Fortran, in some cases) code as text files, while the binary package has all the non-Python code in a binary/non-text format, with the C/C++ and Fortran code already having been compiled.\nIf you install a package from source, C/C++/Fortran code will be compiled on your system (if the package has such code). That should mean the compiled code will work on your system, but requires you to have a compiler available and things properly configured. A binary package doesn’t need to be compiled on your system, but in some cases the code may not run on your system because it was compiled in such a way that is not compatible with your system.\nPython wheels are a binary package format for Python packages. Wheels for some packages will vary by platform (i.e., operating system) so that the package will install correctly on the system where it is being installed."
  },
  {
    "objectID": "units/unit5-programming.html#data-structures",
    "href": "units/unit5-programming.html#data-structures",
    "title": "Programming concepts",
    "section": "Data structures",
    "text": "Data structures\nPlease see the data structures section of Unit 2 for some general discussion of data structures.\nWe’ll also see more complicated data structures when we consider objects in the section on object-oriented programming."
  },
  {
    "objectID": "units/unit5-programming.html#types-and-classes",
    "href": "units/unit5-programming.html#types-and-classes",
    "title": "Programming concepts",
    "section": "Types and classes",
    "text": "Types and classes\n\nOverview and static vs. dynamic typing\nThe term ‘type’ refers to how a given piece of information is stored and what operations can be done with the information.\n‘Primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., boolean, integer, numeric (real-valued, aka double or floating point), character, pointer (aka address, reference).\nIn compiled languages like C and C++, one has to define the type of each variable. Such languages are statically typed. Interpreted (or scripting) languages such as Python and R have dynamic types. One can associate different types of information with a given variable name at different times and without declaring the type of the variable:\n\nx = 'hello'\nprint(x)\n\nhello\n\nx = 7\nx*3\n\n21\n\n\nIn contrast in a language like C, one has to declare a variable based on its type before using it:\n\ndouble y;\ndouble x = 3.1;\ny = x * 7.1;\n\nDynamic typing can be quite helpful from the perspective of quick implementation and avoiding tedious type definitions and problems from minor inconsistencies between types (e.g., multiplying an integer by a real-valued number). But static typing has some critical advantages from the perspective of software development, including:\n\nprotecting against errors from mismatched values and unexpected user inputs, and\ngenerally much faster execution because the type of a variable does not need to be checked when the code is run.\n\nMore complex types in Python (and in R) often use references (pointers, aka addresses) to the actual locations of the data. We’ll see this in detail when we discuss Memory.\n\n\nTypes in Python\nYou should be familiar with the important built-in data types in Python, most importantly lists, tuples, and dictionaries, as well as basic scalar types such as integers, floats, and strings.\nLet’s look at the type of various built-in data structures in Python and in numpy, which provides important types for numerical computing.\n\nx = 3\ntype(x)\n\n&lt;class 'int'&gt;\n\nx = 3.0\ntype(x)\n\n&lt;class 'float'&gt;\n\nx = 'abc'\ntype(x)\n\n&lt;class 'str'&gt;\n\nx = False\ntype(x)\n\n&lt;class 'bool'&gt;\n\nx = [3, 3.0, 'abc']\ntype(x)\n\n&lt;class 'list'&gt;\n\nimport numpy as np\n\nx = np.array([3, 5, 7])  ## array of integers\ntype(x)\n\n&lt;class 'numpy.ndarray'&gt;\n\ntype(x[0])\n\n&lt;class 'numpy.int64'&gt;\n\nx = np.random.normal(size = 3) # array of floats (aka 'doubles')\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.random.normal(size = (3,4)) # multi-dimensional array\ntype(x)\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nSometimes numpy may modify a type to make things easier for you, which often works well, but you may want to control it yourself to be sure:\n\nx = np.array([3, 5, 7.3])\nx\n\narray([3. , 5. , 7.3])\n\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.array([3.0, 5.0, 7.0]) # Force use of floats (either `3.0` or `3.`).\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.array([3, 5, 7], dtype = 'float64')\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\n\nThis can come up when working on a GPU, where the default is usually 32-bit (4-byte) numbers instead of 64-bit (8-byte) numbers. ### Composite objects\nMany objects can be composite (e.g., a list of dictionaries or a dictionary of lists, tuples, and strings).\n\nmydict = {'a': 3, 'b': 7}\nmylist = [3, 5, 7]\n\nmylist[1] = mydict\nmylist\n\n[3, {'a': 3, 'b': 7}, 7]\n\nmydict['a'] = mylist\n\n\n\nMutable objects\nMost objects in Python can be modified in place (i.e., modifying only some of the object), but tuples, strings, and sets are immutable:\n\nx = (3,5,7)\ntry:\n    x[1] = 4\nexcept Exception as error:\n    print(error)\n\n'tuple' object does not support item assignment\n\ns = 'abc'\ns[1]\n\n'b'\n\ntry:\n    s[1] = 'y'\nexcept Exception as error:\n    print(error)\n\n'str' object does not support item assignment\n\n\n\n\nConverting between types\nThis also goes by the term coercion and casting. Casting often needs to be done explicitly in compiled languages and somewhat less so in interpreted languages like Python.\nWe can cast (coerce) between different basic types:\n\ny = str(x[0])\ny\n\n'3'\n\ny = int(x[0])\ntype(y)\n\n&lt;class 'int'&gt;\n\n\nSome common conversions are converting numbers that are being interpreted as strings into actual numbers and converting between booleans and numeric values.\nIn some cases Python will automatically do conversions behind the scenes in a smart way (or occasionally not so smart way). Consider these attempts/examples of implicit coercion:\n\nx = np.array([False, True, True])\nx.sum()         # What do you think is going to happen?\n\n2\n\nx = np.random.normal(size = 5)\ntry:\n    x[3] = 'hat'    # What do you think is going to happen?\nexcept Exception as error:\n    print(error)\n  \n\ncould not convert string to float: 'hat'\n\nmyArray = [1, 3, 5, 9, 4, 7]\n# myArray[2.0]    # What do you think is going to happen?\n# myArray[2.73]   # What do you think is going to happen?\n\nR is less strict and will do conversions in some cases that Python won’t:\n\nx &lt;- rnorm(5)\nx[2.0]\n\n[1] -0.3507278\n\nx[2.73]\n\n[1] -0.3507278\n\n\nWhat are the advantages and disadvantages of the different behaviors of Python and R?\n\n\nDataframes\nHopefully you’re also familiar with the Pandas dataframe type.\nPandas picked up the idea of dataframes from R and functionality is similar in many ways to what you can do with R’s dplyr package.\ndplyr and pandas provide a lot of functionality for the “split-apply-combine” framework of working with “rectangular” data.\nOften analyses are done in a stratified fashion - the same operation or analysis is done on subsets of the data set. The subsets might be different time points, different locations, different hospitals, different people, etc.\nThe split-apply-combine framework is intended to operate in this kind of context: - first one splits the dataset by one or more variables, - then one does something to each subset, and - then one combines the results.\nsplit-apply-combine is also closely related to the famous Map-Reduce framework underlying big data tools such as Hadoop and Spark.\nIt’s also very similar to standard SQL queries involving filtering, grouping, and aggregation.\n\n\nPython object protocols\nThere are a number of broad categories of kinds of objects: mapping, number, sequence, iterator. These are called object protocols.\nAll objects that fall in a given category share key characteristics. For example sequence objects have a notion of “next”, while iterator objects have a notion of “stopping”.\nIf you implement your own class that falls into one of these categories, it should follow the relevant protocol by providing the required methods. For example a container class that supports iteration should provide the __iter__ and __next__ methods.\nHere we see that tuples are iterable containers:\n\nmytuple = (\"apple\", \"banana\", \"cherry\")\n\nfor item in mytuple:\n    print(item)\n\napple\nbanana\ncherry\n\nmyit = iter(mytuple)\n\nprint(next(myit))\n\napple\n\nprint(next(myit))\n\nbanana\n\nmyit.__next__()\n\n'cherry'\n\nx = zip(['clinton', 'bush', 'obama', 'trump'], ['Dem', 'Rep', 'Dem', 'Rep'])\nnext(x)\n\n('clinton', 'Dem')\n\nnext(x)\n\n('bush', 'Rep')\n\n\nWe can also go from an iterable object to a standard list:\n\nr = range(5)\nr\n\nrange(0, 5)\n\nlist(r)\n\n[0, 1, 2, 3, 4]"
  },
  {
    "objectID": "units/unit5-programming.html#principles",
    "href": "units/unit5-programming.html#principles",
    "title": "Programming concepts",
    "section": "Principles",
    "text": "Principles\nSome of the standard concepts in object-oriented programming include encapsulation, inheritance, polymorphism, and abstraction.\nEncapsulation involves preventing direct access to internal data in an object from outside the object. Instead the class is designed so that access (reading or writing) happens through the interface set up by the programmer (e.g., ‘getter’ and ‘setter’ methods). However, Python actually doesn’t really enforce the notion of internal or private information.\nInheritance allows one class to be based on another class, adding more specialized features. For example in the statsmodels package, the OLS class inherits from the WLS class.\nPolymorphism allows for different behavior of an object or function depending on the context. A polymorphic function behaves differently depending on the input types. For example, think of a print function or an addition operator behaving differently depending on the type of the input argument(s). A polymorphic object is one that can belong to different classes (e.g., based on inheritance), and a given method name can be used with any of the classes. An example would be having a base or super class called ‘algorithm’ and various specific machine learning algorithms inheriting from that class. All of the classes might have a ‘predict’ method.\nAbstraction involves hiding the details of how something is done (e.g., via the method of a class), giving the user an interface to provide inputs and get outputs. By making the actual computation a black box, the programmer can modify the internals without changing how a user uses the system.\nClasses generally have constructors that initialize objects of the class and destructors that remove objects."
  },
  {
    "objectID": "units/unit5-programming.html#classes-in-python",
    "href": "units/unit5-programming.html#classes-in-python",
    "title": "Programming concepts",
    "section": "Classes in Python",
    "text": "Classes in Python\nPython provides a pretty standard approach to writing object-oriented code focused on classes.\nOur example is to create a class for working with random time series. Each object of the class has specific parameter values that control the stochastic behavior of the time series. With a given object we can simulate one or more time series (realizations).\nHere’s the initial definition of the class with methods and fields.\n\nimport numpy as np\n\nclass tsSimClass:\n    '''\n    Class definition for time series simulators\n    '''\n    def __init__(self, times, mean = 0, corParam = 1):\n        ## add assertions that corParam is numeric, length 1 and times is np array\n        self._times = times\n        self.n = len(times)\n        self.mean = mean\n        self.corParam = corParam\n        self._currentU = False\n        self._calcMats()\n    def __str__(self):    # 'print' method\n        return f\"An object of class `tsSimClass` with {self.n} time points.\"\n    def __len__(self):\n        return self.n\n    def setTimes(self, newTimes):\n        self._times = newTimes\n        self._calcMats()\n    def getTimes(self):\n        return self._times\n    def simulate(self):\n        if not self._currentU:    \n            self._calcMats()\n        ## analogous to mu+sigma*z for generating N(mu, sigma^2)\n        return self.mean + np.dot(self.U.T, np.random.normal(size = self.n))\n    def _calcMats(self):\n        ## calculates correlation matrix and Cholesky factor\n        lagMat = np.abs(self._times[:, np.newaxis] - self._times)\n        corMat = np.exp(-lagMat ** 2 / self.corParam ** 2)\n        self.U = np.linalg.cholesky(corMat)\n        print(\"Done updating correlation matrix and Cholesky factor.\")\n        self._currentU = True\n\nNow let’s see how we would use the class.\n\nmyts = tsSimClass(np.arange(1, 101), 2, 1)\n\nDone updating correlation matrix and Cholesky factor.\n\nprint(myts)\n\nAn object of class `tsSimClass` with 100 time points.\n\nnp.random.seed(1)\n## here's a simulated time series\ny1 = myts.simulate()\n\nimport matplotlib.pyplot as plt\nplt.plot(myts.getTimes(), y1, '-')\nplt.xlabel('time')\nplt.ylabel('process values')\n## simulate a second series\ny2 = myts.simulate()\nplt.plot(myts.getTimes(), y2, '--')\nplt.show()\n\n\n\n\nWe could set up a different object that has different parameter values. That new simulated time series is less wiggly because the corParam value is larger than before.\n\nmyts2 = tsSimClass(np.arange(1, 101), 2, 4)\n\nDone updating correlation matrix and Cholesky factor.\n\nnp.random.seed(1)\n## here's a simulated time series with a different value of\n## the correlation parameter (corParam)\ny3 = myts2.simulate()\n\nplt.plot(myts2.getTimes(), y3, '-', color = 'red')\nplt.xlabel('time')\nplt.ylabel('process values')\nplt.show()\n\n\n\n\n\nCopies and references\nNext let’s think about when copies are made. In the next example mytsRef is a copy of myts in the sense that both names point to the same underlying object. But no data were copied when the assignment to mytsRef was done.\n\nmytsRef = myts\n## 'mytsRef' and 'myts' are names for the same underlying object\nimport copy\nmytsFullCopy = copy.deepcopy(myts)\n\n## Now let's change the values of a field\nmyts.setTimes(np.arange(1,1001,10))\n\nDone updating correlation matrix and Cholesky factor.\n\nmyts.getTimes()[0:4] \n\narray([ 1, 11, 21, 31])\n\nmytsRef.getTimes()[0:4] # the same as `myts`\n\narray([ 1, 11, 21, 31])\n\nmytsFullCopy.getTimes()[0:4] # different from `myts`\n\narray([1, 2, 3, 4])\n\n\nIn contrast mytsFullCopy is a reference to a different object, and all the data from myts had to be copied over to mytsFullCopy. This takes additional memory (and time), but is also safer, as it avoids the possibility that the user might modify myts and not realize that they were also affecting mytsRef. We’ll discuss this more when we discuss copying in the section on memory use.\n\n\nEncapsulation\nThose of you familiar with OOP will probably be familiar with the idea of public and private fields and methods.\nWhy have private fields (i.e., encapsulation)? The use of private fields shields them from modification by users. Python doesn’t really provide this functionality but by convention, attributes whose name starts with _ are considered private. In this case, we don’t want users to modify the times field. Why is this important? In this example, the correlation matrix and the Cholesky factor U are both functions of the vector of times. So we don’t want to allow a user to directly modify times. If they did, it would leave the fields of the object in inconsistent states. Instead we want them to use setTimes, which correctly keeps all the fields in the object internally consistent (by calling _calcMats). It also allows us to improve efficiency by controlling when computationally expensive operations are carried out.\nIn a module, objects that start with _ are a weak form of private attributes. Users can access them, but from foo import * does not import them.\n\n\nChallenge\n\nChallenge\nHow would you get Python to quit immediately, without asking for any more information, when you simply type q (no parentheses!) instead of quit()? There are actually a couple ways to do this. (Hint: you can do this by understanding what happens when you type q and how to exploit the characteristics of Python classes.)\n\n\n\nInheritance\nInheritance can be a powerful way to reduce code duplication and keep your code organized in a logical (nested) fashion. Special cases can be simple extensions of more general classes.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named '{self.name}' of age {self.age}.\"\n      def color(self):\n          return \"unknown\"\n\nclass GrizzlyBear(Bear):\n      def __init__(self, name, age, num_people_killed = 0):\n          super().__init__(name, age)\n          self.num_people_killed = num_people_killed\n      def color(self):\n          return \"brown\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n\nA bear named 'Yogi the Bear' of age 23.\n\nyog.color()\n\n'unknown'\n\nnum399 = GrizzlyBear(\"Jackson Hole Grizzly 399\", 35)\nprint(num399)\n\nA bear named 'Jackson Hole Grizzly 399' of age 35.\n\nnum399.color()\n\n'brown'\n\nnum399.num_people_killed\n\n0\n\n\nHere the GrizzlyBear class has additional fields/methods beyond those inherited from the base class (the Bear class), i.e., num_people_killed (since grizzly bears are much more dangerous than some other kinds of bears), and perhaps additional or modified methods. Python uses the methods specific to the GrizzlyBear class if present before falling back to methods of the Bear class if not present in the GrizzlyBear class.\nThe above is an example of polymorphism. Instances of the GrizzlyBear class are polymorphic because they can have behavior from both the GrizzlyBear and Bear classes. The color method is polymorphic in that it can be used for both classes but is defined to behave differently depending on the class.\nMore relevant examples of inheritance in Python and R include how regression models are handled. E.g., in Python’s statsmodels, the OLS class inherits from the WLS class."
  },
  {
    "objectID": "units/unit5-programming.html#attributes",
    "href": "units/unit5-programming.html#attributes",
    "title": "Programming concepts",
    "section": "Attributes",
    "text": "Attributes\nBoth fields and methods are attributes.\nWe saw the notion of attributes when looking at HTML and XML, where the information was stored as key-value pairs that in many cases had additional information in the form of attributes.\n\nClass attributes vs. instance attributes\nHere count is a class attribute while name and age are instance attributes.\n\nclass Bear:\n      count = 0\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n          Bear.count += 1\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog.count\n\n1\n\nsmoke = Bear(\"Smoky the Bear\", 77)\nsmoke.count\n\n2\n\n\nThe class attribute allows us to manipulate information relating to all instances of the class, as seen here where we keep track of the number of bears that have been created.\n\n\nAdding attributes\nIt turns out we can add instance attributes on the fly in some cases, which is a bit disconcerting in some ways.\n\nyog.bizarre = 7\nyog.bizarre\n\n7\n\ndef foo(x):\n    print(x)\n\nfoo.bizarre = 3\nfoo.bizarre\n\n3"
  },
  {
    "objectID": "units/unit5-programming.html#generic-function-oop",
    "href": "units/unit5-programming.html#generic-function-oop",
    "title": "Programming concepts",
    "section": "Generic function OOP",
    "text": "Generic function OOP\nLet’s consider the len function in Python. It seems to work magically on various kinds of objects.\n\n\nx = [3, 5, 7]\nlen(x)\n\n3\n\nx = np.random.normal(size = 5)\nlen(x)\n\n5\n\nx = {'a': 2, 'b': 3}\nlen(x)\n\n2\n\n\nSuppose you were writing the len function. What would you have to do to make it work as it did above? What would happen if a user wants to use len with a class that they define?\nInstead, Python implements the len function by calling the __len__ method of the class that the argument belongs to.\n\nx = {'a': 2, 'b': 3}\nlen(x)\n\n2\n\nx.__len__()\n\n2\n\n\n__len__ is a dunder method (a “Double-UNDERscore” method), which we’ll discuss more in a bit.\nSomething similar occurs with operators:\n\nx = 3\nx + 5\n\n8\n\nx = 'abc'\nx + 'xyz'\n\n'abcxyz'\n\nx.__add__('xyz')\n\n'abcxyz'\n\n\nThis use of generic functions is convenient in that it allows us to work with a variety of kinds of objects using familiar functions.\nThe use of such generic functions and operators is similar in spirit to function or method overloading in C++ and Java. It is also how the (very) old S3 system in R works. And it’s a key part of the (fairly) new Julia language.\n\nWhy use generic functions?\nThe Python developers could have written len as a regular function with a bunch of if statements so that it can handle different kinds of input objects.\nThis has some disadvantages:\n\nWe need to write the code that does the checking.\nFurthermore, all the code for the different cases all lives inside one potentially very long function, unless we create class-specific helper functions.\nMost importantly, len will only work for existing classes. And users can’t easily extend it for new classes that they create because they don’t control the len (built-in) function. So a user could not add the additional conditions/classes in a big if-else statement. The generic function approach makes the system extensible – we can build our own new functionality on top of what is already in Python.\n\n\nThe print function\nLike len, print is a generic function, with various class-specific methods.\nWe can write a print method for our own class by defining the __str__ method as well as a __repr__ method giving what to display when the name of an object is typed.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n\n&lt;__main__.Bear object at 0x7f899a374410&gt;\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __repr__(self):\n          return f\"Bear(name={self.name}, age={self.age})\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)   # Invokes __str__\n\nA bear named Yogi the Bear of age 23.\n\nyog          # Invokes __repr__\n\nBear(name=Yogi the Bear, age=23)\n\n\n\n\n\nMultiple dispatch OOP\nThe dispatch system involved in len and + involves only the first argument to the function (or operator). In contrast, Julia emphasizes the importance of multiple dispatch as particularly important for mathematical computation. With multiple dispatch, the specific method can be chosen based on more than one argument.\nIn R, the old (but still used in some contexts) S4 system in R and the new R7 system both provide for multiple dispatch.\nAs a very simple example unrelated to any specific language, multiple dispatch would allow one to do the following with the addition operator:\n3 + 7    # 10\n3 + 'a'  # '3a'\n'hi' +  ' there'  # 'hi there'\nThe idea of having the behavior of an operator or function adapt to the type of the input(s) is one aspect of polymorphism."
  },
  {
    "objectID": "units/unit5-programming.html#the-python-object-model-and-dunder-methods.",
    "href": "units/unit5-programming.html#the-python-object-model-and-dunder-methods.",
    "title": "Programming concepts",
    "section": "The Python object model and dunder methods.",
    "text": "The Python object model and dunder methods.\nNow that we’ve seen the basics of classes, as well as generic function OOP, we’re in a good position to understand the Python object model.\nObjects are dictionaries that provide a mapping from attribute names to their values, either fields or methods.\ndunder methods are special methods that Python will invoke when various functions are called on instances of the class or other standard operations are invoked. They allow classes to interact with Python’s built-ins.\nHere are some important dunder methods:\n\n__init__ is the constructor (initialization) function that is called when the class name is invoked (e.g., Bear(...))\n__len__ is called by len()\n__str__ is called by print()\n__repr__ is called when an object’s name is invoked\n__call__ is called if the instance is invoked as a function call (e.g., yog() in the Bear case)\n__add__ is called by the + operator.\n\nLet’s see an example of defining a dunder method for the Bear class.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __add__(self, value):\n          self.age += value\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog + 12\nprint(yog)\n\nA bear named Yogi the Bear of age 35.\n\n\nMost of the things we work with in Python are objects. Functions are also objects, as are classes.\n\ntype(len)\n\n&lt;class 'builtin_function_or_method'&gt;\n\ndef foo(x):\n    print(x)\n\ntype(foo)\n\n&lt;class 'function'&gt;\n\ntype(Bear)\n\n&lt;class 'type'&gt;"
  },
  {
    "objectID": "units/unit5-programming.html#functional-programming-in-python",
    "href": "units/unit5-programming.html#functional-programming-in-python",
    "title": "Programming concepts",
    "section": "Functional programming (in Python)",
    "text": "Functional programming (in Python)\n\nOverview of functional programming\nFunctional programming is an approach to programming that emphasizes the use of modular, self-contained functions. Such functions should operate only on arguments provided to them (avoiding global variables), and produce no side effects, although in some cases there are good reasons for making an exception. Another aspect of functional programming is that functions are considered ‘first-class’ citizens in that they can be passed as arguments to another function, returned as the result of a function, and assigned to variables. In other words, a function can be treated as any other variable.\nIn many cases (including Python and R), anonymous functions (also called ‘lambda functions’) can be created on-the-fly for use in various circumstances.\nOne can do functional programming in Python by focusing on writing modular, self-contained functions rather than classes. And functions are first-class citizens. However, there are aspects of Python that do not align with the principles mentioned above.\n\nPython’s pass-by-reference behavior causes functions to potentially have the important side effects of modifying arguments that are mutable (e.g., lists and numpy arrays but not tuples) if the programmer is not careful about not modifying arguments within functions.\nSome operations are carried out by statements (e.g., import, def) rather than functions.\n\nIn contrast, R functions have pass-by-value behavior, which is more consistent with a pure functional programming approach.\n\n\nThe principle of no side effects\nBefore we discuss Python further, let’s consider how R behaves in more detail as R conforms more strictly to a functional programming perspective.\nMost functions available in R (and ideally functions that you write as well) operate by taking in arguments and producing output that is then (presumably) used subsequently. The functions generally don’t have any effect on the state of your R environment/session other than the output they produce.\nAn important reason for this (plus for not using global variables) is that it means that it is easy for people using the language to understand what code does. Every function can be treated a black box – you don’t need to understand what happens in the function or worry that the function might do something unexpected (such as changing the value of one of your variables). The result of running code is simply the result of a composition of functions, as in mathematical function composition.\nOne aspect of this is that R uses a pass-by-value approach to function arguments. In R (but not Python), when you pass an object in as an argument and then modify it in the function, you are modifying a local copy of the variable that exists in the context (the frame) of the function and is deleted when the function call finishes:\n\nx &lt;- 1:3\nmyfun &lt;- function(x) {\n      x[2] &lt;- 7\n      print(x)\n      return(x)\n}\n\nnew_x &lt;- myfun(x)\n\n[1] 1 7 3\n\nx   # unmodified\n\n[1] 1 2 3\n\n\nIn contrast, Python uses a pass-by-reference approach, seen here:\n\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return x\n\nnew_x = myfun(x)\nx   # modified!\n\narray([1, 7, 3])\n\n\nAnd actually, given the pass-by-reference behavior, we would probably use a version of myfun that looks like this:\n\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return None\n\nmyfun(x)\nx   # modified!\n\narray([1, 7, 3])\n\n\nNote how easy it would be for a Python programmer to violate the ‘no side effects’ principle. In fact to avoid it, we need to do some additional work in terms of making a copy of x to a new location in memory before modifying it in the function.\n\nx = np.array([1,2,3])\ndef myfun(x):\n  y = x.copy()\n  y[1] = 7\n  return y\n\nnew_x = myfun(x)\nx   # no side effects!\n\narray([1, 2, 3])\n\n\nMore on pass-by-value vs. pass-by-reference later.\nEven in R, there are some (necessary) exceptions to the idea of no side effects, such as par() and plot().\n\n\nFunctions are first-class objects\nEverything in Python is an object, including functions and classes. We can assign functions to variables in the same way we assign numeric and other values.\nWhen we make an assignment we associate a name (a ‘reference’) with an object in memory. Python can find the object by using the name to look up the object in the namespace.\n\nx = 3\ntype(x)\n\n&lt;class 'int'&gt;\n\ntry:\n    x(3)  # x is not a function (yet)\nexcept Exception as error:\n    print(error)\n\n'int' object is not callable\n\ndef x(val):\n    return pow(val, 2)\n\nx(3)\n\n9\n\ntype(x)\n\n&lt;class 'function'&gt;\n\n\nWe can call a function based on the text name of the function.\n\nfunction = getattr(np, \"mean\")\nfunction(np.array([1,2,3]))\n\n2.0\n\n\nWe can also pass a function into another function as the actual function object. This is an important aspect of functional programming. We can do it with our own function or (as we’ll see shortly) with various built-in functions, such as map.\n\ndef apply_fun(fun, a):\n    return fun(a)\n\napply_fun(round, 3.5)\n\n4\n\n\nA function that takes a function as an argument, returns a function as a result, or both is known as a higher-order function.\n\n\nWhich operations are function calls?\nPython provides various statements that are not formal function calls but allow one to modify the current Python session:\n\nimport: import modules or packages\ndef: define functions or classes\nreturn: return results from a function\ndel: remove an object\n\nOperators are examples of generic function OOP, where the appropriate method of the class of the first object that is part of the operation is called.\n\nx = np.array([0,1,2])\nx - 1\n\narray([-1,  0,  1])\n\nx.__sub__(1)\n\narray([-1,  0,  1])\n\nx\n\narray([0, 1, 2])\n\n\nNote that the use of the operator does not modify the object.\n(Note that you can use return(x) and del(x) but behind the scenes the Python interpreter is intepreting those as return x and del x.)\n\n\nMap operations\nA map operation takes a function and runs the function on each element of some collection of items, analogous to a mathematical map. This kind of operation is very commonly used in programming, particularly functional programming, and often makes for clean, concise, and readable code.\nPython provides a variety of map-type functions: map (a built-in) and pandas.apply. These are examples of higher-order functions – functions that take a function as an argument. Another map-type operation is list comprehension, shown here:\n\nx = [1,2,3]\ny = [pow(val, 2) for val in x]\ny\n\n[1, 4, 9]\n\n\nIn Python, map is run on the elements of an iterable object. Such objects include lists as well as the result of range() and other functions that produce iterables.\n\nx = [1.0, -2.7, 3.5, -5.1]\nlist(map(abs, x))\n\n[1.0, 2.7, 3.5, 5.1]\n\nlist(map(pow, x, [2,2,2,2]))\n\n[1.0, 7.290000000000001, 12.25, 26.009999999999998]\n\n\nOr we can use lambda functions to define a function on the fly:\n\nx = [1.0, -2.7, 3.5, -5.1]\nresult = list(map(lambda vals: vals * 2, x))\n\nIf you need to pass another argument to the function you can use a lambda function as above or functools.partial:\n\nfrom functools import partial\n\n# Create a new round function with 'ndigits' argument pre-set\nround3 = partial(round, ndigits = 3)\n\n# Apply the function to a list of numbers\nlist(map(round3, [32.134234, 7.1, 343.7775]))\n\n[32.134, 7.1, 343.777]\n\n\nLet’s compare using a map-style operation (with Pandas) to using a for loop to run a stratified analysis for a generic example (this code won’t run because the variables don’t exist):\n\n# stratification \nsubsets = df.groupby('grouping_variable')\n\n# map using pandas.apply: one line, easy to understand\nresults = subsets.apply(analysis_function)\n\n# for loop: needs storage set up and multiple lines\nresults &lt;- []\nfor _,subset in subsets:   # iterate over the key-value pairs (the subsets)\n  results.append(analysis_function(subset))\n\nMap operations are also at the heart of the famous MapReduce framework, used in Hadoop and Spark for big data processing."
  },
  {
    "objectID": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "href": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "title": "Programming concepts",
    "section": "Function evaluation, frames, and the call stack",
    "text": "Function evaluation, frames, and the call stack\n\nOverview\nWhen we run code, we end up calling functions inside of other function calls. This leads to a nested series of function calls. The series of calls is the call stack. The stack operates like a stack of cafeteria trays - when a function is called, it is added to the stack (pushed) and when it finishes, it is removed (popped).\nUnderstanding the series of calls is important when reading error messages and debugging. In Python, when an error occurs, the call stack is shown, which has the advantage of giving the complete history of what led to the error and the disadvantage of producing often very verbose output that can be hard to understand. (In contrast, in R, only the function in which the error occurs is shown, but you can see the full call stack by invoking traceback().)\nWhat happens when an Python function is evaluated?\n\nThe user-provided function arguments are evaluated in the calling scope and the results are matched to the argument names in the function definition.\nA new frame containing a new namespace is created to store information related to the function call and placed on the stack. Assignment to the argument names is done in the namespace, including any default arguments.\nThe function is evaluated in the (new) local scope. Any look-up of variables not found in the local scope (using the namespace that was created) is done using the lexical scoping rules to look in the series of enclosing scopes (if any exist), then in the global/module scope, and then in the built-ins scope.\nWhen the function finishes, the return value is passed back to the calling scope and the frame is taken off the stack. The namespace is removed, unless the namespace is the enclosing scope for an existing namespace.\n\nI’m not expecting you to fully understand that previous paragraph and all the terms in it yet. We’ll see all the details as we proceed through this Unit.\n\n\nFrames and the call stack\nPython keeps track of the call stack. Each function call is associated with a frame that has a namespace that contains the local variables for that function call.\nThere are a bunch of functions that let us query what frames are on the stack and access objects in particular frames of interest. This gives us the ability to work with objects in the frame from which a function was called.\nWe can use functions from the traceback package to query the call stack.\n\nimport traceback\n\ndef function_a():\n    function_b()\n\ndef function_b():\n    function_c()\n\ndef function_c():\n    traceback.print_stack()\n\nfunction_a()\n\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"&lt;string&gt;\", line 2, in function_a\n  File \"&lt;string&gt;\", line 2, in function_b\n  File \"&lt;string&gt;\", line 2, in function_c"
  },
  {
    "objectID": "units/unit5-programming.html#function-inputs-and-outputs",
    "href": "units/unit5-programming.html#function-inputs-and-outputs",
    "title": "Programming concepts",
    "section": "Function inputs and outputs",
    "text": "Function inputs and outputs\n\nArguments\nYou can see the arguments (and any default values) for a function using the help system.\nLet’s create an example function:\n\ndef add(x, y, z=1, absol=False):\n    if absol:\n        return(abs(x+y+z))\n    else:\n        return(x+y+z)\n\nWhen using a function, there are some rules that must be followed.\nArguments without defaults are required.\nArguments can be specified by position (based on the order of the inputs) or by name (keyword), using name=value, with positional arguments appearing first.\n\nadd(3, 5)\n\n9\n\nadd(3, 5, 7)\n\n15\n\nadd(3, 5, absol=True, z=-5)\n\n3\n\nadd(z=-5, x=3, y=5)\n\n3\n\ntry:\n    add(3)\nexcept Exception as error:\n    print(error)\n\nadd() missing 1 required positional argument: 'y'\n\n\nHere’s another error related to positional vs. keyword arguments.\n\nadd(z=-5, 3, 5)  ## Can't trap `SyntaxError` with `try`\n# SyntaxError: positional argument follows keyword argument  \n\nFunctions may have unspecified arguments, which are designated using *args. (‘args’ is a convention - you can call it something else). Unspecified arguments occurring at the beginning of the argument list are generally a collection of like objects that will be manipulated (consider print).\nHere’s an example where we see that we can manipulate args, which is a tuple, as desired.\n\ndef sum_args(*args):\n    print(args[2])\n    total = sum(args)\n    return total\n\nresult = sum_args(1, 2, 3, 4, 5)\n\n3\n\nprint(result)  # Output: 15\n\n15\n\n\nThis syntax also comes in handy for some existing functions, such as os.path.join, which can take either an arbitrary number of inputs or a list.\n\nos.path.join('a','b','c')\n\n'a/b/c'\n\nx = ['a','b','c']\nos.path.join(*x)\n\n'a/b/c'\n\n\n\n\nFunction outputs\nreturn x will specify x as the output of the function. return can occur anywhere in the function, and allows the function to exit as soon as it is done.\nWe can return multiple outputs using return - the return value will then be a tuple.\n\ndef f(x):\n    if x &lt; 0:\n        return -x**2\n    else:\n        res = x^2\n        return x, res\n\n\nf(-3)\n\n-9\n\nf(3)\n\n(3, 1)\n\nout1,out2 = f(3)\n\nIf you want a function to be invoked for its side effects, you can omit return or explicitly have return None or simply return."
  },
  {
    "objectID": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "href": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "title": "Programming concepts",
    "section": "Pass by value vs. pass by reference",
    "text": "Pass by value vs. pass by reference\nWhen talking about programming languages, one often distinguishes pass-by-value and pass-by-reference.\nPass-by-value means that when a function is called with one or more arguments, a copy is made of each argument and the function operates on those copies. In pass-by-value, changes to an argument made within a function do not affect the value of the argument in the calling environment.\nPass-by-reference means that the arguments are not copied, but rather that information is passed allowing the function to find and modify the original value of the objects passed into the function. In pass-by-reference changes inside a function can affect the object outside of the function.\nPass-by-value is elegant and modular in that functions do not have side effects - the effect of the function occurs only through the return value of the function. However, it can be inefficient in terms of the amount of computation and of memory used. In contrast, pass-by-reference is more efficient, but also more dangerous and less modular. It’s more difficult to reason about code that uses pass-by-reference because effects of calling a function can be hidden inside the function. Thus pass-by-value is directly related to functional programming.\nArrays and other non-scalar objects in Python are pass-by-reference (but note that tuples are immutable, so one could not modify a tuple that is passed as an argument).\n\ndef myfun(x):\n    x[1] = 99\n\ny = [0, 1, 2]\nz = myfun(y)\ntype(z)\n\n&lt;class 'NoneType'&gt;\n\ny\n\n[0, 99, 2]\n\n\nLet’s see what operations cause arguments modified in a function to affect state outside of the function:\n\ndef myfun(f_scalar, f_x, f_x_new, f_x_newid, f_x_copy):\n\n    f_scalar = 99                 # global input unaffected\n    f_x[0] = 99                   # global input MODIFIED\n    f_x_new = [99,2,3]            # global input unaffected\n\n    newx = f_x_newid\n    newx[0] = 99                  # global input MODIFIED\n\n    xcopy = f_x_copy.copy() \n    xcopy[0] = 99                 # global input unaffected\n\n\nscalar = 1\nx = [1,2,3]\nx_new = np.array([1,2,3])\nx_newid = np.array([1,2,3])\nx_copy = np.array([1,2,3])\n\n\nmyfun(scalar, x, x_new, x_newid, x_copy)\n\nHere are the cases where state is preserved:\n\nscalar\n\n1\n\nx_new\n\narray([1, 2, 3])\n\nx_copy\n\narray([1, 2, 3])\n\n\nAnd here are the cases where state is modified:\n\nx\n\n[99, 2, 3]\n\nx_newid\n\narray([99,  2,  3])\n\n\nBasically if you replace the reference (object name) then the state outside the function is preserved. That’s because a new local variable in the function scope is created. However in the ` If you modify part of the object, state is not preserved.\nThe same behavior occurs with other mutable objects such as numpy arrays.\n\nPointers\nTo put pass-by-value vs. pass-by-reference in a broader context, I want to briefly discuss the idea of a pointer, common in compiled languages such as C.\n\nint x = 3;\nint* ptr;\nptr = &x;\n*ptr * 7; // returns 21\n\n\nThe int* declares ptr to be a pointer to (the address of) the integer x.\nThe &x gets the address where x is stored.\n*ptr dereferences ptr, returning the value in that address (which is 3 since ptr is the address of x.\n\nArrays in C are really pointers to a block of memory:\n\nint x[10];\n\nIn this case x will be the address of the first element of the vector. We can access the first element as x[0] or *x.\nWhy have we gone into this? In C, you can pass a pointer as an argument to a function. The result is that only the scalar address is copied and not the entire object, and inside the function, one can modify the original object, with the new value persisting on exit from the function. For example in the following example one passes in the address of an object and that object is then modified in place, affecting its value when the function call finishes.\n\nint myCal(int* ptr){\n    *ptr = *ptr + *ptr;\n}\n\nmyCal(&x)  # x itself will be modified\n\nSo Python behaves similarly to the use of pointers in C."
  },
  {
    "objectID": "units/unit5-programming.html#namespaces-and-scopes",
    "href": "units/unit5-programming.html#namespaces-and-scopes",
    "title": "Programming concepts",
    "section": "Namespaces and scopes",
    "text": "Namespaces and scopes\nAs discussed here in the Python docs, a namespace is a mapping from names to objects that allows Python to find objects by name via clear rules that enforce modularity and avoid name conflicts.\nNamespaces are created and removed through the course of executing Python code. When a function is run, a namespace for the local variables in the function is created, and then deleted when the function finishes executing. Separate function calls (including recursive calls) have separate namespaces.\nScope is closely related concept – a scope determines what namespaces are accessible from a given place in one’s code. Scopes are nested and determine where and in what order Python searches the various namespaces for objects.\nNote that the ideas of namespaces and scopes are relevant in most other languages, though the details of how they work can differ.\nThese ideas are very important for modularity, isolating the names of objects to avoid conflicts.\nThis allows you to use the same name in different modules or submodules, as well as different packages using the same name.\nOf course to make the objects in a module or package available we need to use import.\nConsider what happens if you have two modules that both use x and you import x using from.\n\nfrom mypkg.mymod import x\nfrom mypkg.mysubpkg import x\nx  # which x is used?\n\nWe’ve added x twice to the namespace of the global scope. Are both available? Did one ‘overwrite’ the other? How do I access the other one?\nThis is much better:\n\nimport mypkg\nmypkg.x\n\n7\n\nimport mypkg.mysubpkg\nmypkg.mysubpkg.x\n\n999\n\n\nSide note: notice that import mypkg causes the name mypkg itself to be in the current (global) scope.\nWe can see the objects in a given namespace/scope using dir().\n\nxyz = 7\ndir()\n\n['Bear', 'GrizzlyBear', '__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'apply_fun', 'content', 'copy', 'f', 'files', 'foo', 'function', 'function_a', 'function_b', 'function_c', 'io', 'it', 'item', 'm', 'match', 'math', 'myArray', 'mydict', 'myfun', 'myit', 'mylist', 'mymod', 'mypkg', 'myts', 'myts2', 'mytsFullCopy', 'mytsRef', 'mytuple', 'new_x', 'np', 'num399', 'os', 'out1', 'out2', 'partial', 'pattern', 'platform', 'plt', 'r', 're', 'result', 'return_group', 'round3', 's', 'scalar', 'smoke', 'stream', 'subprocess', 'sum_args', 'sys', 'text', 'time', 'tmp', 'traceback', 'tsSimClass', 'x', 'x_copy', 'x_new', 'x_newid', 'xyz', 'y', 'y1', 'y2', 'y3', 'yog', 'z']\n\nimport mypkg\ndir(mypkg)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'myfun', 'mymod', 'mysubpkg', 'x']\n\nimport mypkg.mymod\ndir(mypkg.mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'x']\n\nimport builtins\ndir(builtins)\n\n['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '_', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip']\n\n\nHere are the key scopes to be aware of, in order (“LEGB”) of how the namespaces are searched:\n\nLocal scope: objects available within function (or class method).\nnon-local (Enclosing) scope: objects available from functions enclosing a given function (we’ll talk about this more later; this relates to lexical scoping).\nGlobal (aka ‘module’) scope: objects available in the module in which the function is defined (which may simply be the default global scope when you start the Python interpreter). This is also the local scope if the code is not executing inside a function.\nBuilt-ins scope: objects provided by Python through the built-ins module but available from anywhere.\n\nNote that import adds the name of the imported module to the namespace of the current (i.e., local) scope.\nWe can see the local and global namespaces using locals() and globals().\n\ncat local.py\n\ngx = 7\n\ndef myfun(z):\n    y = z*3\n    print(\"local: \", locals())\n    print(\"global: \", globals())\n\n\nRun the following code to see what is in the different namespaces:\n\nimport local\n\ngx = 99\nlocal.myfun(3)\n\nStrangely (for me being more used to R, where package namespaces are locked), we can add an object to a namespace created from a module or package:\n\nmymod.x = 33\ndir(mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n\nimport numpy as np\nnp.x = 33\n'x' in dir(np)\n\nTrue\n\n\nAs more motivation, consider this example.\nSuppose we have this code in a module named test_scope.py:\n\ncat test_scope.py\n\nmagic_number = 7\n\ndef myfun(val):\n    return(val * magic_number)\n\n\nNow suppose we also define magic_number in the scope in which myfun is called from.\n\nimport test_scope\nmagic_number = 900\ntest_scope.myfun(3)\n\n21\n\n\nWe see that Python uses magic_number from the module. What would be bad about using magic_number from the global scope of the Python session rather than the global scope of the module? Consider a case where instead of using the test_scope.py module we were using code from a package.\n\nLexical scoping (enclosing scopes)\nIn this section, we seek to understand what happens in the following circumstance. Namely, where does Python get the value for the object x?\n\ndef f(y):\n  return(x + y)\n\nf(3)\n\nVariables in the enclosing scope are available within a function. The enclosing scope is the scope in which a function is defined, not the scope from which a function is called.\nThis approach is called lexical scoping. R and many other languages also use lexical scoping.\nThe behavior of basing lookup on where functions are defined rather than where they are called from extends the local-global scoping discussed in the previous section, with similar motivation.\nLet’s dig deeper to understand where Python looks for non-local variables, illustrating lexical scoping:\n\n## Case 1\nx = 3\ndef f2():\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nf() # what will happen?\n\n## Case 2\nx = 3\ndef f2()\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 3\nx = 3\ndef f():\n    def f2():\n        print(x)        \n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 4\nx = 3\ndef f():\n    def f2():\n        print(x)\n    f2()\n\nx = 100\nf() # what will happen?\n\nHere’s a tricky example:\n\ny = 100\ndef fun_constructor():\n    y = 10\n    def g(x):\n            return(x + y)     \n    return(g)\n\n## fun_constructor() creates functions\nmyfun = fun_constructor()\nmyfun(3)\n\nLet’s work through this:\n\nWhat is the enclosing scope for the function g()?\nWhich y does g() use?\nWhere is myfun defined (this is tricky – how does myfun relate to g)?\nWhat is the enclosing scope for myfun()?\nWhen fun_constructor() finishes, does its namespace disappear? What would happen if it did?\nWhat does myfun use for y?\n\nWe can use the inspect package to see information about the closure.\n\nimport inspect\ninspect.getclosurevars(myfun)\n\nClosureVars(nonlocals={}, globals={'copy': &lt;module 'copy' from '/system/linux/mambaforge-3.11/lib/python3.11/copy.py'&gt;}, builtins={}, unbound=set())\n\n\n(Note that I haven’t fully investigated the use of inspect, but it looks like it has a lot of useful tools.)\nBe careful when using variables from non-local scopes as the value of that variable may well not be what you expect it to be. In general one wants to think carefully before using variables that are taken from outside the local scope, but in some cases it can be useful.\nNext we’ll see some ways of accessing variables outside of the local scope.\n\n\nGlobal and non-local variables\nWe can create and modify global variables and variables in the enclosing scope using global and nonlocal respectively. Note that global is in the context of the current module so this could be a variable in your current Python session if you’re working with functions defined in that session or a global variable in a module or package.\n\ndel x\n\ndef myfun():\n    global x\n    x = 7\n\nmyfun()\nprint(x)\n\n7\n\nx = 9\nmyfun()\nprint(x)\n\n7\n\n\n\ndef outer_function():\n    x = 10  # Outer variable\n    def inner_function():\n        nonlocal x\n        x = 20  # Modifying the outer variable\n    print(x)  # Output: 10\n    inner_function()\n    print(x)  # Output: 20\n\nouter_function()\n\n10\n20\n\n\nIn R, one can do similar things using the global assignment operator &lt;&lt;-.\n\n\nClosures\nOne way to associate data with functions is to use a closure. This is a functional programming way to achieve something like an OOP class. This Wikipedia entry nicely summarizes the idea, which is a general functional programming idea and not specific to Python.\nUsing a closure involves creating one (or more functions) within a function call and returning the function(s) as the output. When one executes the original function (the constructor), the new function(s) is created and returned and one can then call that function(s). The function then can access objects in the enclosing scope (the scope of the constructor) and can use nonlocal to assign into the enclosing scope, to which the function (or the multiple functions) have access. The nice thing about this compared to using a global variable is that the data in the closure is bound up with the function(s) and is protected from being changed by the user.\n\nx = np.random.normal(size = 5)\ndef scaler_constructor(input):\n    data = input\n    def g(param):\n            return(param * data) \n    return(g)\n\nscaler = scaler_constructor(x)\ndel x # to demonstrate we no longer need x\nscaler(3)\n\narray([ 0.5081473 ,  2.22166935, -2.86110181, -0.79865552,  0.09784364])\n\nscaler(6)\n\narray([ 1.0162946 ,  4.44333871, -5.72220361, -1.59731104,  0.19568728])\n\n\nSo calling scaler(3) multiplies 3 by the value of data stored in the closure (the namespace of the enclosing scope) of the function scaler.\nNote that it can be hard to see the memory use involved in the closure.\nHere’s a more realistic example. There are other ways you could do this, but this is slick:\n\ndef make_container(n):\n    x = np.zeros(n)\n    i = 0\n    def store(value = None):\n        nonlocal x, i\n        if value is None:\n            return x\n        else:\n            x[i] = value\n            i += 1\n    return store\n\n\nnboot = 20\nbootmeans = make_container(nboot)\n\nimport pandas as pd\niris = pd.read_csv('https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/io/data/csv/iris.csv')\ndata = iris['SepalLength']\n\nfor i in range(nboot): \n    bootmeans(np.mean(np.random.choice(data, size = len(data), replace = True)))\n\n\nbootmeans()\n\narray([5.874     , 5.95533333, 5.86      , 5.754     , 5.77466667,\n       5.81733333, 5.902     , 5.79933333, 5.842     , 5.81933333,\n       5.854     , 5.97      , 5.84      , 5.80133333, 5.99333333,\n       5.88133333, 5.83133333, 5.84533333, 5.91466667, 5.79666667])\n\nbootmeans.__closure__\n\n(&lt;cell at 0x7f899a295de0: int object at 0x7f89ab261968&gt;, &lt;cell at 0x7f899a296170: numpy.ndarray object at 0x7f899bc29350&gt;)"
  },
  {
    "objectID": "units/unit5-programming.html#decorators",
    "href": "units/unit5-programming.html#decorators",
    "title": "Programming concepts",
    "section": "Decorators",
    "text": "Decorators\nNow that we’ve seen function generators, it’s straightforward to discuss decorators.\nA decorator is a wrapper around a function that extends the functionality of the function without actually modifying the function.\nWe can create a simple decorator “manually” like this:\n\ndef verbosity_wrapper(myfun):\n    def wrapper(*args, **kwargs):\n        print(f\"Starting {myfun.__name__}.\")\n        output = myfun(*args, **kwargs)\n        print(f\"Finishing {myfun.__name__}.\")\n        return output\n    return wrapper\n    \nverbose_rnorm = verbosity_wrapper(np.random.normal)\n\nx = verbose_rnorm(size = 5)\n\nStarting normal.\nFinishing normal.\n\nx\n\narray([ 0.07202449, -0.67672674,  0.98248139, -1.65748762,  0.81795808])\n\n\nPython provides syntax that helps you create decorators with less work (this is an example of the general idea of syntactic sugar).\nWe can easily apply our decorator defined above to a function as follows. Now the function name refers to the wrapped version of the function.\n\n@verbosity_wrapper\ndef myfun(x):\n    return x\n\ny = myfun(7)\n\nStarting myfun.\nFinishing myfun.\n\ny\n\n7\n\n\nOur decorator doesn’t do anything useful, but hopefully you can imagine that the idea of being able to have more control over the operation of functions could be useful. For example we could set up a timing wrapper so that when we run a function, we get a report on how long it took to run the function. Or using the idea of a closure, we could keep a running count of the number of times a function has been called.\nOne real-world example of using decorators is in setting up functions to run in parallel in dask, which we’ll discuss in Unit 7."
  },
  {
    "objectID": "units/unit5-programming.html#overview-2",
    "href": "units/unit5-programming.html#overview-2",
    "title": "Programming concepts",
    "section": "Overview",
    "text": "Overview\nThe main things to remember when thinking about memory use are: (1) numeric vectors take 8 bytes per element and (2) we need to keep track of when large objects are created, including local variables in the frames of functions.\n\nx = np.random.normal(size = 5)\nx.itemsize # 8 bytes\n\n8\n\nx.nbytes\n\n40\n\n\n\nAllocating and freeing memory\nUnlike compiled languages like C, in Python we do not need to explicitly allocate storage for objects. (However, we will see that there are times that we do want to allocate storage in advance, rather than successively concatenating onto a larger object.)\nPython automatically manages memory, releasing memory back to the operating system when it’s not needed via a process called garbage collection. Very occasionally you may want to remove large objects as soon as they are not needed. del does not actually free up memory, it just disassociates the name from the memory used to store the object. In general Python will quickly clean up such objects without a reference (i.e., a name), so there is generally no need to call gc.collect() to force the garbage collection.\nIn a language like C in which the user allocates and frees up memory, memory leaks are a major cause of bugs. Basically if you are looping and you allocate memory at each iteration and forget to free it, the memory use builds up inexorably and eventually the machine runs out of memory. In Python, with automatic garbage collection, this is generally not an issue, but occasionally memory leaks could occur.\n\n\nThe heap and the stack\nThe heap is the memory that is available for dynamically creating new objects while a program is executing, e.g., if you create a new object in Python or call new in C++. When more memory is needed the program can request more from the operating system. When objects are removed in Python, Python will handle the garbage collection of releasing that memory.\nThe stack is the memory used for local variables when a function is called.\nThere’s a nice discussion of this on this Stack Overflow thread."
  },
  {
    "objectID": "units/unit5-programming.html#monitoring-memory-use",
    "href": "units/unit5-programming.html#monitoring-memory-use",
    "title": "Programming concepts",
    "section": "Monitoring memory use",
    "text": "Monitoring memory use\n\nMonitoring overall memory use on a UNIX-style computer\nTo understand how much memory is available on your computer, one needs to have a clear understanding of disk caching. The operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around than if it had to read the information from disk. While the cached information is using memory, that same memory is immediately available to other processes, so the memory is available even though it is “in use”.\nWe can see this via free -h (the -h is for ‘human-readable’, i.e. show in GB (G)) on Linux machine.\n          total used free shared buff/cache available \n    Mem:   251G 998M 221G   2.6G        29G      247G \n    Swap:  7.6G 210M 7.4G\nYou’ll generally be interested in the Mem row. (See below for some comments on Swap.) The shared column is complicated and probably won’t be of use to you. The buff/cache column shows how much space is used for disk caching and related purposes but is actually available. Hence the available column is the sum of the free and buff/cache columns (more or less). In this case only about 1 GB is in use (indicated in the used column).\ntop (Linux or Mac) and vmstat (on Linux) both show overall memory use, but remember that the amount actually available to you is the amount free plus any buff/cache usage. Here is some example output from vmstat:\n\n    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- \n    r b   swpd      free   buff    cache si so bi bo in cs us sy id wa st \n    1 0 215140 231655120 677944 30660296  0  0  1  2  0  0 18  0 82  0  0\nIt shows 232 GB free and 31 GB used for cache and therefore available, for a total of 263 GB available.\nHere are some example lines from top:\n    KiB Mem : 26413715+total, 23180236+free, 999704 used, 31335072 buff/cache \n    KiB Swap:  7999484 total,  7784336 free, 215148 used. 25953483+avail Mem\nWe see that this machine has 264 GB RAM (the total column in the Mem row), with 259.5 GB available (232 GB free plus 31 GB buff/cache as seen in the Mem row). (I realize the numbers don’t quite add up for reasons I don’t fully understand, but we probably don’t need to worry about that degree of exactness.) Only 1 GB is in use.\nSwap is essentially the reverse of disk caching. It is disk space that is used for memory when the machine runs out of physical memory. You never want your machine to be using swap for memory because your jobs will slow to a crawl. As seen above, the swap line in both free and top shows 8 GB swap space, with very little in use, as desired.\n\n\nMonitoring memory use in Python\nThere are a number of ways to see how much memory is being used. When Python is actively executing statements, you can use top from the UNIX shell.\nIn Python, we can call out to the system to get the info we want:\n\nimport psutil\n\n# Get memory information\nmemory_info = psutil.Process().memory_info()\n\n# Print the memory usage\nprint(\"Memory usage:\", memory_info.rss/10**6, \" Mb.\")\n\n# Let's turn that into a function for later use:\n\nMemory usage: 470.765568  Mb.\n\ndef mem_used():\n    print(\"Memory usage:\", psutil.Process().memory_info().rss/10**6, \" Mb.\")\n\nWe can see the size of an object (in bytes) with sys.getsizeof().\n\nmy_list = [1, 2, 3, 4, 5]\nsys.getsizeof(my_list)\n\n104\n\nx = np.random.normal(size = 10**7) # should use about 80 Mb\nsys.getsizeof(x)\n\n80000112\n\n\nHowever, we need to be careful about objects that refer to other objects:\n\ny = [3, x]\nsys.getsizeof(y)  # Whoops!\n\n72\n\n\nHere’s a trick where we serialize the object, as if to export it, and then see how long the binary representation is.\n\nimport pickle\nser_object = pickle.dumps(y)\nsys.getsizeof(ser_object)\n\n80000201\n\n\nThere are also some flags that one can start python with that allow one to see information about memory use and allocation. See man python. You could also look into the memory_profiler or pympler packages."
  },
  {
    "objectID": "units/unit5-programming.html#how-memory-is-used-in-python",
    "href": "units/unit5-programming.html#how-memory-is-used-in-python",
    "title": "Programming concepts",
    "section": "How memory is used in Python",
    "text": "How memory is used in Python\n\nTwo key tools: id and is\nWe can use the id function to see where in memory an object is stored and is to see if two object are actually the same objects in memory. It’s particularly useful for understanding storage and memory use for complicated data structures. We’ll also see that they can be handy tools for seeing where copies are made and where they are not.\n\nx = np.random.normal(size = 10**7)\nid(x)\n\n140228665313968\n\nsys.getsizeof(x)\n\n80000112\n\ny = x\nid(y)\n\n140228665313968\n\nx is y\n\nTrue\n\nsys.getsizeof(y)\n\n80000112\n\nz = x.copy()\nid(z)\n\n140228667485872\n\nsys.getsizeof(z)\n\n80000112\n\n\n\n\nMemory use in specific circumstances\n\nHow lists are stored\nHere we can use id to determine how the overall list is stored as well as the elements of the list.\n\nnums = np.random.normal(size = 5)\nobj = [nums, nums, np.random.normal(size = 5), ['adfs']]\n\nid(nums)\n\n140228665322032\n\nid(obj)\n\n140228665946048\n\nid(obj[0])\n\n140228665322032\n\nid(obj[1])\n\n140228665322032\n\nid(obj[2])\n\n140228665675856\n\nid(obj[3])\n\n140228665966656\n\nid(obj[3][0])\n\n140228665970736\n\nobj[0] is obj[1]\n\nTrue\n\nobj[0] is obj[2]\n\nFalse\n\n\nWhat do we notice?\n\nThe list itself appears to be a array of references (pointers) to the component elements.\nEach element has its own address.\nTwo elements of a list can use the same memory (see the first two elements here, whose contents are at the same memory address).\nA list element can use the same memory as another object (or part of another object).\n\n\n\nHow character strings are stored.\nSimilar tricks are used for storing strings (and also integers). We’ll explore this in a problem on PS4.\n\n\nModifying elements in place\nWhat do this simple experiment tell us?\n\nx = np.random.normal(size = 5)\nid(x)\n\n140228665321552\n\nx[2] = 3.5\nid(x)\n\n140228665321552\n\n\nIt makes some sense that modifying elements of an object here doesn’t cause a copy – if it did, working with large objects would be very difficult.\n\n\n\nWhen are copies made?\nLet’s try to understand when Python uses additional memory for objects, and how it knows when it can delete memory. We’ll use large objects so that we can use free or top to see how memory use by the Python process changes.\n\nx = np.random.normal(size = 10**8)\nid(x)\n\n140228665675952\n\ny = x\nid(y)\n\n140228665675952\n\nx = np.random.normal(size = 10**8)\nid(x)\n\n140228665321552\n\n\nOnly if we re-assign x to reference a different object does additional memory get used.\n\nHow does Python know when it can free up memory?\nPython keeps track of how many names refer to an object and only removes memory when there are no remaining references to an object.\n\nimport sys\n\nx = np.random.normal(size = 10**8)\ny = x\nsys.getrefcount(y)\n\n3\n\ndel x\nsys.getrefcount(y)\n\n2\n\ndel y\n\nWe can see the number of references using sys.getrefcount. Confusingly, the number is one higher than we’d expect, because it includes the temporary reference from passing the object as the argument to getrefcount.\n\nx = np.random.normal(size = 5)\nsys.getrefcount(x)  # 2 \n\n2\n\ny = x\nsys.getrefcount(x)  # 3\n\n3\n\nsys.getrefcount(y)  # 3\n\n3\n\ndel y\nsys.getrefcount(x)  # 2\n\n2\n\ny = x\nx = np.random.normal(size = 5)\nsys.getrefcount(y)  # 2\n\n2\n\nsys.getrefcount(x)  # 2\n\n2\n\n\nThis notion of reference counting occurs in other contexts, such as shared pointers in C++ and in how R handles copying and garbage collection."
  },
  {
    "objectID": "units/unit5-programming.html#strategies-for-saving-memory",
    "href": "units/unit5-programming.html#strategies-for-saving-memory",
    "title": "Programming concepts",
    "section": "Strategies for saving memory",
    "text": "Strategies for saving memory\nA frew basic strategies for saving memory include:\n\nAvoiding unnecessary copies.\nRemoving objects that are not being used, at which point the Python garbage collector should free up the memory.\n\nIf you’re really trying to optimize memory use, you may also consider:\n\nUsing types that take up less memory (e.g., Bool, Int16, Float32) when possible.\n\nx = np.array(np.random.normal(size = 5), dtype = \"float32\")\nx.itemsize\n\n4\n\nx = np.array([3,4,2,-2], dtype = \"int16\")\nx.itemsize\n\n2\n\n\nReading data in from files in chunks rather than reading the entire dataset (more in Unit 7).\nExploring packages such as arrow for efficiently using memory, as discussed in Unit 2."
  },
  {
    "objectID": "units/unit5-programming.html#example",
    "href": "units/unit5-programming.html#example",
    "title": "Programming concepts",
    "section": "Example",
    "text": "Example\nLet’s work through a real example where we keep a running tally of current memory in use and maximum memory used in a function call. We’ll want to consider hidden uses of memory, when copies are made, and lazy evaluation. This code (translated from the original R code) comes from a PhD student’s research. For our purposes here, let’s assume that xvar and yvar are very long numpy arrays using a lot of memory.\n\ndef fastcount(xvar, yvar):\n    naline = np.isnan(xvar)\n    naline[np.isnan(yvar)] = True\n    localx = xvar.copy()\n    localy = yvar.copy()\n    localx[naline] = 0\n    localy[naline] = 0\n    useline = ~naline\n    ## We'll ignore the rest of the code.\n    ## ...."
  },
  {
    "objectID": "units/unit5-programming.html#interpreters-and-compilation",
    "href": "units/unit5-programming.html#interpreters-and-compilation",
    "title": "Programming concepts",
    "section": "Interpreters and compilation",
    "text": "Interpreters and compilation\n\nWhy are interpreted languages slow?\nCompiled code runs quickly because the original code has been translated into instructions (machine language) that the processor can understand (i.e., zeros and ones). In the process of doing so, various checking and lookup steps are done once and don’t need to be redone when running the compiled code.\nIn contrast, when one runs code in an interpreted language such as Python or R, the interpreter needs to do all the checking and lookup each time the code is run. This is required because the types and locations in memory of the variables could have changed.\nWe’ll focus on Python in the following discussion, but most of the concepts apply to other interpreted languages.\nFor example, consider this code:\n\nx = 3\nabs(x)\nx*7\nx = 'hi'\nabs(x)\nx*3\n\nBecause of dynamic typing, when the interpreter sees abs(x) it needs to check if x is something to which the absolute value function can be applied, including dealing with the fact that x could be a list or array with many numbers in it. In addition it needs to (using scoping rules) look up the value of x. (Consider that x might not even exist at the point that abs(x) is called.) Only then can the absolute value calculation happen. For the multiplication, Python needs to lookup the version of * that can be used, depending on the type of x.\nLet’s consider writing a loop with some ridiculous code:\n\nx = np.random.normal(10)\nfor i in range(10):\n    if np.random.normal(size = 1) &gt; 0:\n        x = 'hi'\n    if np.random.normal(size = 1) &gt; 0.5:\n        del x\n    x[i]= np.exp(x[i])\n\nThere is no way around the fact that because of how dynamic this is, the interpreter needs to check if x exists, if it is a vector of sufficient length, if it contains numeric values, and it needs to go retrieve the required value, EVERY TIME the np.exp() is executed. Now the code above is unusual, and in most cases, we wouldn’t have the if statements that modify x. So you could imagine a process by which the checking were done on the first iteration and then not needed after that – that gets into the idea of just-in-time compilation, discussed later.\nThe standard Python interpreter (CPython) is a C function so in some sense everything that happens is running as compiled code, but there are lots more things being done to accomplish a given task using interpreted code than if the task had been written directly in code that is compiled. By analogy, consider talking directly to a person in a language you both know compared to talking to a person via an interpreter who has to translate between two languages. Ultimately, the same information gets communicated (hopefully!) but the number of words spoken and time involved is much greater.\nWhen running more complicated functions, there is often a lot of checking that is part of the function itself. For example scipy’s solve_triangular function ultimately calls out to the trtrs Lapack function, but before doing so, there is a lot of checking that can take time. To that point, the documentation suggests you might set check_finite=False to improve performance at the expense of potential problems if the input matrices contain troublesome elements.\nWe can flip the question on its head and ask what operations in an interpreted language will execute quickly. In Python, these include:\n\noperations that call out to compiled C code,\nlinear algebra operations (these call out to compiled C or Fortran code provided by the BLAS and LAPACK software packages), and\nvectorized calls rather than loops:\n\nvectorized calls generally run loops in compiled C code rather than having the loop run in Python, and\nthat means that the interpreter doesn’t have to do all the checking discussed above for every iteration of the loop.\n\n\n\n\nCompilation\n\nOverview\nCompilation is the process of turning code in a given language (such a C++) into machine code. Machine code is the code that the processor actually executes. The machine code is stored in the executable file, which is a binary file. The history of programming has seen ever great levels of abstraction, so that humans can write code using syntax that is easier for us to understand, re-use, and develop building blocks that can be put together to do complicated tasks. For example assembly language is a step above machine code. Languages like C and Fortran provide additional abstraction beyond that. The Statistics 750 class at CMU has a nice overview if you want to see more details.\nNote that interpreters such as Python are themselves programs – the standard Python interpreter (CPython) is a C program that has been compiled. It happens to be a program that processes Python code. The interpreter doesn’t turn Python code into machine code, but the interpreter itself is machine code.\n\n\nJust-in-time (JIT) compilation\nStandard compilation (ahead-of-time or AOT compilation) happens before any code is executed and can involve a lot of optimization to produce the most efficient machine code possible.\nIn contrast, just-in-time (JIT) compilation happens at the time that the code is executing. JIT compilation is heavily used in Julia, which is very fast (in some cases as fast as C). JIT compilation involves translating to machine code as the code is running. One nice aspect is that the results are cached so that if code is rerun, the compilation process doesn’t have to be redone. So if you use a language like Julia, you’ll see that the speed can vary drastically between the first time and later times you run a given function during a given session.\nOne thing that needs to be dealt with is type checking. As discussed above, part of why an interpreter is slow is because the type of the variable(s) involved in execution of a piece of code is not known in advance, so the interpreter needs to check the type. In JIT systems, there are often type inference systems that determine variable types.\nJIT compilation can involve translation from the original code to machine code or translation of bytecode (see next section) to machine code.\n\n\nByte compiling (optional)\nFunctions in Python and Python packages may byte compiled. What does that mean? Byte-compiled code is a special representation that can be executed more efficiently because it is in the form of compact codes that encode the results of parsing and semantic analysis of scoping and other complexities of the Python source code. This byte code can be executed faster than the original Python code because it skips the stage of having to be interpreted by the Python interpreter.\nIf you look at the file names in the directory of an installed Python package you may see files with the .pyc extension. These files have been byte-compiled.\nWe can byte compile our own functions using either the py_compile or compileall modules. Here’s an example (silly since as experienced Python programmers, we would use vectorized calculation here rather than this unvectorized code.)\n\nimport time\n\ndef f(vals):\n    x = np.zeros(len(vals))\n    for i in range(len(vals)):\n        x[i] = np.exp(vals[i])\n    return(x)\n\nx = np.random.normal(size = 10**6)\nt0 = time.time()\nout = f(x)\ntime.time() - t0\n\n0.7474195957183838\n\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n\n0.011847734451293945\n\n\n\nimport py_compile\npy_compile.compile('vec.py')\n\n'__pycache__/vec.cpython-311.pyc'\n\n\n\ncp __pycache__/vec.cpython-311.pyc vec.pyc\nrm vec.py    # make sure non-compiled module not loaded\n\n\nimport vec\nvec.__file__\n    \n\n'/accounts/vis/paciorek/teaching/243fall23/stat243-fall-2023/units/vec.pyc'\n\nt0 = time.time()\nout = vec.f(x)\ntime.time() - t0\n\n0.714287519454956\n\n\nUnfortunately, as seen above byte compiling may not speed things up much. I’m not sure why."
  },
  {
    "objectID": "units/unit5-programming.html#benchmarking-and-profiling",
    "href": "units/unit5-programming.html#benchmarking-and-profiling",
    "title": "Programming concepts",
    "section": "Benchmarking and profiling",
    "text": "Benchmarking and profiling\nRecall that it’s a waste of time to optimize code before you determine (1) that the code is too slow for how it will be used and (2) which are the slow steps on which to focus your attempts to speed the code up. A 100x speedup in a step that takes 1% of the time will speed up the overall code by very little.\n\nTiming your code\nThere are a few ways to time code:\n\nimport time\nt0 = time.time()\nx = 3\nt1 = time.time()\n\nprint(f\"Execution time: {t1-t0} seconds.\")\n\nExecution time: 0.005488395690917969 seconds.\n\n\nIn general, it’s a good idea to repeat (replicate) your timing, as there is some stochasticity in how fast your computer will run a piece of code at any given moment.\nUsing time is fine for code that takes a little while to run, but for code that is really fast, it may not be very accurate. Measuring fast bits of code is tricky to do well. This next approach is better for benchmarking code (particularly faster bits of code).\n\nimport timeit\n\ntimeit.timeit('x = np.exp(3.)', setup = 'import numpy as np', number = 100)\n\n0.00019408203661441803\n\ncode = '''\nx = np.exp(3.)\n'''\n\ntimeit.timeit(code, setup = 'import numpy as np', number = 100)\n\n0.00018205121159553528\n\n\nThat reports the total time for the 100 replications.\nWe can run it from the command line.\n\npython -m timeit -s 'import numpy' -n 1000 'x = numpy.exp(3.)'\n\n1000 loops, best of 5: 987 nsec per loop\n\n\ntimeit ran the code 1000 times for 5 different repetitions, giving the average time for the 1000 samples for the best of the 5 repetitions.\n\n\nProfiling\nThe Cprofile module will show you how much time is spent in different functions, which can help you pinpoint bottlenecks in your code.\nI haven’t run this code when producing this document as the output of the profiling can be lengthy.\n\ndef lr_slow(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    inv = np.linalg.inv(xtx)\n    return inv @ xty\n\n## generate random observations and random matrix of predictors\ny = np.random.normal(size = 5000)\nx = np.random.normal(size = (5000,1000))\n\nt0 = time.time()\nregr = lr_slow(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\nimport cProfile\ncProfile.run('lr_slow(y,x)')\n\nThe cumtime column includes the time spent in nested calls to functions while the tottime column excludes it.\nAs we’ll discuss in detail in Unit 10, we almost never want to explicitly invert a matrix. Instead we factorize the matrix and use the factorized result to do the computation of interest. In this case using the Cholesky decomposition is a standard approach, followed by solving triangular systems of equations.\n\nimport scipy as sp\n\ndef lr_fast(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    L = sp.linalg.cholesky(xtx)\n    out = sp.linalg.solve_triangular(L.T, \n          sp.linalg.solve_triangular(L, xty, lower=True),\n          lower=False)\n    return(out)\n\nt0 = time.time()\nregr = lr_fast(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\ncProfile.run('lr_fast(y,x)')\n\nThe Cholesky now dominates the computational time (but is much faster than inv), so there’s not much more we can do in this case.\nYou might wonder if it’s better to use x.T or np.transpose(x). Try using timeit to decide.\nThe Python profilers (cProfile and profile (not shown)) use deterministic profiling – calculating the interval between events (i.e., function calls and returns). However, there is some limit to accuracy – the underlying ‘clock’ measures in units of about 0.001 seconds.\n(In contrast, R’s profiler works by sampling (statistical profiling) - every little while during a calculation it finds out what function R is in and saves that information to a file. So if you try to profile code that finishes really quickly, there’s not enough opportunity for the sampling to represent the calculation accurately and you may get spurious results.)"
  },
  {
    "objectID": "units/unit5-programming.html#writing-efficient-python-code",
    "href": "units/unit5-programming.html#writing-efficient-python-code",
    "title": "Programming concepts",
    "section": "Writing efficient Python code",
    "text": "Writing efficient Python code\nWe’ll discuss a variety of these strategies, including:\n\nPre-allocating memory rather than growing objects iteratively\nVectorization and use of fast matrix algebra\nConsideration of loops vs. map operations\nSpeed of lookup operations, including hashing\n\n\nPre-allocating memory\nLet’s consider whether we should pre-allocate space for the output of an operation or if it’s ok to keep extending the length of an array or list.\n\nn = 100000\nz = np.random.normal(size = n)\n\n## Pre-allocation\n\ndef fun_prealloc(vals):\n   n = len(vals)\n   x = [0] * n\n   for i in range(n):\n       x[i] = np.exp(vals[i])\n   return(x)\n\n## Appending to a list\n\ndef fun_append(vals):\n   x = []\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n   return(x)\n\n## Appending to a numpy array\n\ndef fun_append_np(vals):\n   x = np.array([])\n   for i in range(n):\n       x = np.append(x, np.exp(vals[i]))\n   return(x)\n\n\nt0 = time.time()\nout1 = fun_prealloc(z)\ntime.time() - t0\n\n0.0720217227935791\n\nt0 = time.time()\nout2 = fun_append(z)\ntime.time() - t0\n\n0.0720512866973877\n\nt0 = time.time()\nout3 = fun_append_np(z)\ntime.time() - t0\n\n2.4477429389953613\n\n\nSo what’s going on? First let’s consider what is happening with the use of np.append. Note that it is a function, rather than a method, and we need to reassign to x. What must be happening in terms of memory use and copying when we append an element?\n\nx = np.random.normal(size = 5)\nid(x)\n\n140228665678064\n\nid(np.append(x, 3.34))\n\n140228665678160\n\n\nWe can avoid that large cost of copying and memory allocation by pre-allocating space for the entire output array. (This is equivalent to variable initialization in compiled languages.)\nOk, but how is it that we can append to the list at apparently no cost?\nIt’s not magic, just that Python is clever. Let’s get an idea of what is going on:\n\ndef fun_append2(vals):\n   n = len(vals)\n   x = []\n   print(f\"Initial id: {id(x)}\")\n   sz = sys.getsizeof(x)\n   print(f\"iteration 0: size {sz}\")\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n       if sys.getsizeof(x) != sz:\n           sz = sys.getsizeof(x)\n           print(f\"iteration {i}: size {sz}\")\n   print(f\"Final id: {id(x)}\")\n   return(x)\n\nz = np.random.normal(size = 1000)\nout = fun_append2(z)\n\nInitial id: 140228665954048\niteration 0: size 56\niteration 0: size 88\niteration 4: size 120\niteration 8: size 184\niteration 16: size 248\niteration 24: size 312\niteration 32: size 376\niteration 40: size 472\niteration 52: size 568\niteration 64: size 664\niteration 76: size 792\niteration 92: size 920\niteration 108: size 1080\niteration 128: size 1240\niteration 148: size 1432\niteration 172: size 1656\niteration 200: size 1912\niteration 232: size 2200\niteration 268: size 2520\niteration 308: size 2872\niteration 352: size 3256\niteration 400: size 3704\niteration 456: size 4216\niteration 520: size 4792\niteration 592: size 5432\niteration 672: size 6136\niteration 760: size 6936\niteration 860: size 7832\niteration 972: size 8856\nFinal id: 140228665954048\n\n\nSurprisingly, the id of x doesn’t seem to change, even though we are allocating new memory at many of the iterations. What is happening is that x is an wrapper object that contains within it a reference to an array of references (pointers) to the list elements. The location of the wrapper object doesn’t change, but the underlying array of references/pointers is being reallocated.\nSide note: our assessment of size above does not include the actual size of the list elements.\n\nprint(sys.getsizeof(out))\n\n8856\n\nout[2] = np.random.normal(size = 100000)\nprint(sys.getsizeof(out))\n\n8856\n\n\nOne upshot of this is that if you need to grow an object use a Python list. Then once it is complete, you can always convert it to another type, such as a numpy array.\n\n\nVectorization and use of fast matrix algebra\nOne key way to write efficient Python code is to take advantage of numpy’s vectorized operations.\n\nn = 10**6\nz = np.random.normal(size = n)\nt0 = time.time()\nx = np.exp(z)\nprint(time.time() - t0)\n\n0.03270316123962402\n\nx = np.zeros(n)  # Leave out pre-allocation timing to focus on computation.\nt0 = time.time()\nfor i in range(n):\n    x[i] = np.exp(z[i])\n\n\nprint(time.time() - t0)\n\n0.808849573135376\n\n\nSo what is different in how Python handles the calculations above that explains the huge disparity in efficiency? The vectorized calculation is being done natively in C in a for loop. The explicit Python for loop involves executing the for loop in Python with repeated calls to C code at each iteration. This involves a lot of overhead because of the repeated processing of the Python code inside the loop. For example, in each iteration of the loop, Python is checking the types of the variables because it’s possible that the types might change, as discussed earlier.\nYou can usually get a sense for how quickly a Python call will pass things along to C or Fortran by looking at the body of the relevant function(s) being called.\nUnfortunately seeing the source code in Python often involves going and finding it in a file on disk, whereas in R, printing a function will show its source code. However you can use ?? in IPython to get the code for non-builtin functions. Consider numpy.linspace??.\nHere I found the source code for the scipy triangular_solve function, which calls out to a Fortran function trtrs, found in the LAPACK library.\n\n## On an SCF machine:\ncat /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/scipy/linalg/_basic.py\n\nWith a bit more digging around we could verify that trtrs is a LAPACK funcion by doing some grepping:\n./linalg/_basic.py:    trtrs, = get_lapack_funcs(('trtrs',), (a1, b1))\nMany numpy and scipy functions allow you to pass in arrays, and operate on those arrays in vectorized fashion. So before writing a for loop, look at the help information on the relevant function(s) to see if they operate in a vectorized fashion. Functions might take arrays for one or more of their arguments.\nOutside of the numerical packages, we often have to manually do the looping:\n\nx = [3.5, 2.7, 4.6]\ntry:\n    math.cos(x)\nexcept Exception as error:\n    print(error)\n\nmust be real number, not list\n\n[math.cos(val) for val in x]\n\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n\nlist(map(math.cos, x))\n\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n\n\nChallenge: Consider the chi-squared statistic involved in a test of independence in a contingency table:\n\\[\n\\chi^{2}=\\sum_{i}\\sum_{j}\\frac{(y_{ij}-e_{ij})^{2}}{e_{ij}},\\,\\,\\,\\, e_{ij}=\\frac{y_{i\\cdot}y_{\\cdot j}}{y_{\\cdot\\cdot}}\n\\]\nwhere \\(y_{i\\cdot}=\\sum_{j}y_{ij}\\) and \\(y_{\\cdot j} = \\sum_{i} y_{ij}\\) and \\(y_{\\cdot\\cdot} = \\sum_{i} \\sum_{j} y_{ij}\\). Write this in a vectorized way without any loops. Note that ‘vectorized’ calculations also work with matrices and arrays.\nSometimes we can exploit vectorized mathematical operations in surprising ways, though sometimes the code is uglier.\n\nx = np.random.normal(size = n)\n\n## List comprehension\ntimeit.timeit('truncx = [max(0,val) for val in x]', number = 10, globals = {'x':x})\n\n0.17521439492702484\n\n\n\n## Vectorized slice replacement\ntimeit.timeit('truncx = x.copy(); truncx[x &lt; 0] = 0', number = 10, globals = {'x':x})\n\n0.005411941558122635\n\n\n\n## Vectorized math trick\ntimeit.timeit('truncx = x * x&gt;0', number = 10, globals = {'x':x})\n\n0.0010863672941923141\n\n\nWe’ll discuss what has to happen (in terms of calculations, memory allocation, and copying) in the two vectorized approaches to try to understand which is more efficient.\nAdditional tips:\n\nIf you do need to loop over dimensions of a matrix or array, if possible loop over the smallest dimension and use the vectorized calculation on the larger dimension(s). For example if you have a 10000 by 10 matrix, try to set up your problem so you can loop over the 10 columns rather than the 10000 rows.\nIn general, in Python looping over rows is likely to be faster than looping over columns because of numpy’s row-major ordering (by default, matrices are stored in memory as a long array in which values in a row are adjacent to each other). However how numpy handles this is more complicated (see more in the Section on cache-aware programming), such that it may not matter for numpy calculations.\nYou can use direct arithmetic operations to add/subtract/multiply/divide a vector by each column of a matrix, e.g. A*b does element-wise multiplication of each column of A by a vector b. If you need to operate by row, you can do it by transposing the matrix.\n\nCaution: relying on Python’s broadcasting rule in the context of vectorized operations, such as is done when direct-multiplying a matrix by a vector to scale the columns relative to each other, can be dangerous as the code may not be easy for someone to read and poses greater dangers of bugs. In some cases you may want to first write the code more directly and then compare the more efficient code to make sure the results are the same. It’s also a good idea to comment your code in such cases.\n\n\nVectorization, mapping, and loops\nNext let’s consider when loops and mapping would be particularly slow and how mapping and loops might compare to each other.\nFirst, the potential for inefficiency of looping and map operations in interpreted languages will depend in part on whether a substantial part of the work is in the overhead involved in the looping or in the time required by the function evaluation on each of the elements.\nHere’s an example, where the core computation is very fast, so we might expect the overhead of looping (in its various forms seen here) to be important.\n\nimport time\nn = 10**6\nx = np.random.normal(size = n)\n\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n\n0.012585163116455078\n\nt0 = time.time()\nvals = np.zeros(n)\nfor i in range(n):\n    vals[i] = np.exp(x[i])\n\n\ntime.time() - t0\n\n0.8506276607513428\n\nt0 = time.time()\nvals = [np.exp(v) for v in x]\ntime.time() - t0\n\n0.6357121467590332\n\nt0 = time.time()\nvals = list(map(np.exp, x))\ntime.time() - t0\n\n0.5772781372070312\n\n\nRegardless of how we do the looping (an explicit loop, list comprehension, or map), it looks like we can’t avoid the overhead unless we use the vectorized call, which is of course the recommended approach in this case, both for speed and readability (and conciseness).\nSecond, is it faster to use map than to use a loop? In the example above it is somewhat faster to use map. That might not be surprising. In the loop case, the interpreter needs to do the checking we discussed earlier in this section at each iteration of the loop. What about in the map case? For mapping over a numpy array, perhaps not, but what if mapping over a list? So without digging into how map works, it’s hard to say.\nHere’s an example where the bulk of time is in the actual computation and not in the looping itself. We’ll run a bunch of regressions on a matrix X (i.e., each column of X is a predictor) using each column of the matrix mat to do a separate regression.\n\nimport time\nimport statsmodels.api as sm\n\nn = 500000;\nnr = 10000\nnCalcs = int(n/nr)\n\nmat = np.random.normal(size = (nr, nCalcs))\n\nX = list(range(nr))\nX = sm.add_constant(X)\n\ndef regrFun(i):\n    model = sm.OLS(mat[:,i], X)\n    return(model.fit().params[1])\n\nt0 = time.time()\nout1 = list(map(regrFun, range(nCalcs)))\ntime.time() - t0\n\n0.05824542045593262\n\nt0 = time.time()\nout2 = np.zeros(nCalcs)\nfor i in range(nCalcs):\n    out2[i] = regrFun(i)\n\n\ntime.time() - t0\n\n0.07360720634460449\n\n\nHere the looping is faster. I don’t have any particular explanation for that result.\n\n\nMatrix algebra efficiency\nOften calculations that are not explicitly linear algebra calculations can be done as matrix algebra. If our Python installation has a fast (and possibly parallelized) BLAS, this allows our calculation to take advantage of it.\nFor example, we can sum the rows of a matrix by multiplying by a vector of ones.\n\nmat = np.random.normal(size=(500,500))\n\ntimeit.timeit('mat.dot(np.ones(500))', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n\n0.033969756215810776\n\ntimeit.timeit('np.sum(mat, axis = 1)', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n\n0.1168225146830082\n\n\nGiven the extra computation involved in actually multiplying each number by one, it’s surprising that this is faster than numpy sum function. One thing we’d want to know is whether the BLAS matrix multiplication call is being done in parallel.\nOn the other hand, big matrix operations can be slow.\nChallenge: Suppose you want a new matrix that computes the differences between successive columns of a matrix of arbitrary size. How would you do this as matrix algebra operations? It’s possible to write it as multiplying the matrix by another matrix that contains 0s, 1s, and -1s in appropriate places. Here it turns out that the for loop is much faster than matrix multiplication. However, there is a way to do it faster as matrix direct subtraction.\n\n\nOrder of operations and efficiency\nWhen doing matrix algebra, the order in which you do operations can be critical for efficiency. How should I order the following calculation?\n\nn = 5000\nA = np.random.normal(size=(n, n))\nB = np.random.normal(size=(n, n))\nx = np.random.normal(size=n)\n\nt0 = time.time()\nres1 = (A @ B) @ x\nprint(time.time() - t0)\n\n1.7937490940093994\n\nt0 = time.time()\nres1 = A @ (B @ x)\nprint(time.time() - t0)\n\n0.08377599716186523\n\n\nWhy is the second order much faster?\n\n\nAvoiding unnecessary operations\nWe can use the matrix direct product (i.e., A*B) to do some manipulations much more quickly than using matrix multiplication. Challenge: How can I use the direct product to find the trace of a matrix, \\(XY\\)?\nFinally, when working with diagonal matrices, you can generally get much faster results by being smart. The following operations: \\(X+D\\), \\(DX\\), \\(XD\\) are mathematically the sum of two matrices and products of two matrices. But we can do the computation without using two full matrices. Challenge: How?\n\nn = 1000\nX = np.random.normal(size=(n, n))\ndiagvals = np.random.normal(size=n)\nD = np.diag(diagvals)\n\n# The following lines are very inefficient\nsummedMat = X + D\nprodMat1 = D @ X\nprodMat2 = X @ D\n\nMore generally, sparse matrices and structured matrices (such as block diagonal matrices) can generally be worked with MUCH more efficiently than treating them as arbitrary matrices. The scipy.sparse package (for both structured and arbitrary sparse matrices) can help, as can specialized code available in other languages, such as C and Fortran packages.\n\n\nSpeed of lookup operations\nThere are lots of situations in which we need to retrieve values for subsequent computations. In some cases we might be retrieving elements of an array or looking up values in a dictionary.\nLet’s compare the speed of some different approaches to lookup.\n\nn = 1000\nx = list(np.random.normal(size = n))\nkeys = [str(v) for v in range(n)]\nxD = dict(zip(keys, x))\n\ntimeit.timeit(\"x[500]\", number = 10**6, globals = {'x':x})\n\n0.014431843534111977\n\ntimeit.timeit(\"xD['500']\", number=10**6, globals = {'xD':xD})\n\n0.026302199810743332\n\n\nHow is it that Python can look up by key in the dictionary at essentially the same speed as jumping to an index position? It uses hashing, which allows O(1) lookup. In contrast, if one has to look through each key in turn, that is O(n), which is much slower:\n\ntimeit.timeit(\"x[keys.index('500')]\", number = 10**6, globals = {'x':x, 'keys':keys})\n\n5.733805952593684\n\n\nAs a further point of contrast, if we look up elements by name in R in named vectors or lists, that is much slower than looking up by index, because R doesn’t use hashing in that context and has to scan through the objects one by one until it finds the one with the name it is looking for. This stands in contrast to R and Python being able to directly go to the position of interest based on the index of an array, or to the hash-based lookup in a Python dictionary or an R environment.\n\n\nHashing (including name lookup)\nAbove I mentioned that Python uses hashing to store and lookup values by key in a dictionary. I’ll briefly describe what hashing is here, because it is a commonly-used strategy in programming in general.\nA hash function is a function that takes as input some data and maps it to a fixed-length output that can be used as a shortened reference to the data. (The function should be deterministic, always returing the same output for a given input.) We’ve seen this in the context of git commits where each commit was labeled with a long base-16 number. This also comes up when verifying files on the Internet. You can compute the hash value on the file you get and check that it is the same as the hash value associated with the legitimate copy of the file.\nWhile there are various uses of hashing, for our purposes here, hashing can allow one to look up values by their name via a hash table. The idea is that you have a set of key-value pairs (sometimes called a dictionary) where the key is the name associated with the value and the value is some arbitrary object. You want to be able to quickly find the value/object quickly.\nHashing allows one to quickly determine an index associated with the key and therefore quickly find the relevant value based on the index. For example, one approach is to compute the hash as a function of the key and then take the remainder when dividing by the number of possible results (here the fact that the result is a fixed-length output is important) to get the index. Here’s the procedure in pseudocode:\n    hash = hashfunc(key) \n    index = hash %% array_size \n    ## %% is modulo operator - it gives the remainder\nIn general, there will be collisions – multiple keys will be assigned to the same index. However with a good hash function, usually there will be a small number of keys associated with a given bucket. So each bucket will contain a list of a small number of values and the associated keys. (The buckets might contain the actual values or they might contain the addresses of where the values are actually stored if the values are complicated objects.) Then determining the correct value (or the required address) within a given bucket is fast even with simple linear search through the items one by one. Put another way, the hash function distributes the keys amongst an array of buckets and allows one to look up the appropriate bucket quickly based on the computed index value. When the hash table is properly set up, the cost of looking up a value does not depend on the number of key-value pairs stored.\nPython uses hashing to look up the value based on the key in a given dictionary, and similarly when looking up variables in namespaces. This allows Python to retrieve objects very quickly."
  },
  {
    "objectID": "units/unit5-programming.html#additional-general-strategies-for-efficiency",
    "href": "units/unit5-programming.html#additional-general-strategies-for-efficiency",
    "title": "Programming concepts",
    "section": "Additional general strategies for efficiency",
    "text": "Additional general strategies for efficiency\nIt’s also useful to be aware of some other strategies for improving efficiency.\n\nCache-aware programming\nIn addition to main memory (what we usually mean when we talk about RAM), computers also have memory caches, which are small amounts of fast memory that can be accessed very quickly by the processor. For example your computer might have L1, L2, and L3 caches, with L1 the smallest and fastest and L3 the largest and slowest. The idea is to try to have the data that is most used by the processor in the cache.\nIf the next piece of data needed for computation is available in the cache, this is a cache hit and the data can be accessed very quickly. However, if the data is not available in the cache, this is a cache miss and the speed of access will be a lot slower. Cache-aware programming involves writing your code to minimize cache misses. Generally when data is read from memory it will be read in chunks, so values that are contiguous will be read together.\nHow does this inform one’s programming? For example, if you have a matrix of values stored in row-major order, computing on a row will be a lot faster than computing on a column, because the row can be read into the cache from main memory and then accessed in the cache. In contrast, if the matrix is large and therefore won’t fit in the cache, when you access the values of a column, you’ll have to go to main memory repeatedly to get the values for the row because the values are not stored contiguously.\nThere’s a nice example of the importance of the cache at the bottom of this blog post.\nIf you know the size of the cache, you can try to design your code so that in a given part of your code you access data structures that will fit in the cache. This sort of thing is generally more relevant if you’re coding in a language like C. But it can matter sometimes in interpreted languages too.\nLet’s see what happens in Python. By default, matrices in numpy are row-major, also called “C order”. I’ll create a long matrix with a small number of very long columns and a wide matrix with a small number of very long rows.\n\nnr = 800000\nnc = 100\n\nA = np.random.normal(size=(nr, nc))   # long matrix\ntA = np.random.normal(size=(nc, nr))  # wide matrix\n\n## Verify that A is row-major using `.flags` (notice the `C_CONTIGUOUS: True`).\nA.flags\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n\nNote that I didn’t use A.T or np.transpose as that doesn’t make a copy in memory and so the transposed matrix doesn’t end up being row-major. You can use A.flags and A.T.flags` to see this.\nNow let’s time calculating the sum by column in the long matrix vs. the sum by row in the wide matrix. Exactly the same number of arithmetic operations needs to be done in an equivalent manner for the two cases. We want to use a large enough matrix so the entire matrix doesn’t fit in the cache, but not so large that the example takes a long time or a huge amount of memory. We’ll use a rectangular matrix, such that the summation for a single column of the long matrix or a single row of the wide matrix involves many numbers, but there are a limited number of such summations. This focuses the example on the efficiency of the column-wise vs. row-wise summation rather than any issues that might be involved in managing large numbers of such summations (e.g., doing many, many summations that involve just a few numbers).\n\n# Define the sum calculations as functions\ndef sum_by_column():\n    return np.sum(A, axis=0)\n\ndef sum_by_row():\n    return np.sum(tA, axis=1)\n\ntimeit.timeit(sum_by_column, number=10)  # potentially slow\n\n0.5056534707546234\n\ntimeit.timeit(sum_by_row, number=10)\n\n0.408780699595809\n\n\nSuppose we instead do the looping manually.\n\ntimeit.timeit('[np.sum(A[:,col]) for col in range(A.shape[1])]',\n    setup = 'import numpy as np', number=10, globals = {'A': A})   \n\n5.12814655713737\n\ntimeit.timeit('[np.sum(tA[row,:]) for row in range(tA.shape[0])]',\n    setup = 'import numpy as np', number=10, globals = {'tA': tA})  \n\n0.41643139719963074\n\n\nIndeed, the row-wise calculations are much faster when done manually. However, when done with the axis argument in np.sum there is little difference. So that suggests numpy might be doing something clever in its implementation of sum with the axis argument.\n\nChallenge: suppose you were writing code for this kind of use case. How could you set up your calculations to do either row-wise or column-wise operations in a way that processes each number sequentially based on the order in which the numbers are stored. For example suppose the values are stored row-major but you want the column sums.\n\nWhen we define a numpy array, we can choose to use column-major order (i.e., “Fortran” order) with the order argument.\n\n\nLoop fusion\nLet’s consider this (vectorized) code:\n\nx = np.exp(x) + 3*np.sin(x)\n\nThis code has some downsides.\n\nThink about whether any additional memory has to be allocated.\nThink about how many for loops will have to get executed.\n\nContrast that to running directly as a for loop (e.g., here in Julia or in C/C++):\n\nfor i in 1:length(x)\n    x[i] = exp(x[i]) + 3*sin(x[i])\nend\n\nHow does that affect the downsides mentioned above?\nCombining loops is called ‘fusing’ and is an important optimization that Julia can do, as shown in this demo. It’s also a key optimization done by XLA, a compiler used with Tensorflow, so one approach to getting loop fusion in Python is to use Tensorflow for such calculations within Python rather than simply using numpy.\n\n\nLazy evaluation\nWhat’s strange about this R code?\n\nf &lt;- function(x) print(\"hi\")\nsystem.time(mean(rnorm(1000000)))\n\n   user  system elapsed \n  0.058   0.000   0.059 \n\nsystem.time(f(3))\n\n[1] \"hi\"\n\n\n   user  system elapsed \n      0       0       0 \n\nsystem.time(f(mean(rnorm(1000000)))) \n\n[1] \"hi\"\n\n\n   user  system elapsed \n  0.001   0.000   0.001 \n\n\nLazy evaluation is not just an R thing. It also occurs in Tensorflow (particularly version 1), the Python Dask package, and in Spark. The basic idea is to delay executation until it’s really needed, with the goal that if one does so, the system may be able to better optimize a series of multiple steps as a joint operation relative to executing them one by one.\nHowever, Python itself does not have lazy evaluation."
  },
  {
    "objectID": "units/unit1-intro.html",
    "href": "units/unit1-intro.html",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "units/unit1-intro.html#some-useful-editors",
    "href": "units/unit1-intro.html#some-useful-editors",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "Some useful editors",
    "text": "Some useful editors\n\nvarious editors available on all operating systems:\n\ntraditional editors born in UNIX: emacs, vim\nsome newer editors: Atom, Sublime Text (Sublime is proprietary/not free)\n\nWindows-specific: WinEdt\nMac-specific: Aquamacs Emacs, TextMate, TextEdit\nRStudio provides a built-in editor for R code and Quarto/R Markdown files. One can actually edit and run Python code chunks quite nicely in RStudio. (Note: RStudio as a whole is an IDE (integrated development environment. The editor is just the editing window where you edit code (and Markdown) files.)\nVSCode has a powerful code editor that is customized to work with various languages, and it has a Quarto extension.\n\nAs you get started it’s ok to use a very simple text editor such as Notepad in Windows, but you should take the time in the next few weeks to try out more powerful editors such as one of those listed above. It will be well worth your time over the course of your graduate work and then your career.\nBe careful in Windows - file suffixes are often hidden."
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-emacs",
    "href": "units/unit1-intro.html#optional-basic-emacs",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic emacs",
    "text": "(Optional) Basic emacs\nEmacs is one option as an editor. I use Emacs a fair amount, so I’m including some tips here, but other editors listed above are just as good.\n\nEmacs has special modes for different types of files: Python code files, R code files, C code files, Latex files – it’s worth your time to figure out how to set this up on your machine for the kinds of files you often work on\n\nIf working with Python and R, one can start up a Python or R interpreter in an additional Emacs buffer and send code to that interpreter and see the results of running the code.\nFor working with R, ESS (emacs speaks statistics) mode is helpful. This is built into Aquamacs Emacs.\n\nTo open emacs in the terminal window rather than as a new window, which is handy when it’s too slow (or impossible) to pass (i.e., tunnel) the graphical emacs window through ssh: emacs -nw file.txt"
  },
  {
    "objectID": "units/unit1-intro.html#optional-emacs-keystroke-sequence-shortcuts.",
    "href": "units/unit1-intro.html#optional-emacs-keystroke-sequence-shortcuts.",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Emacs keystroke sequence shortcuts.",
    "text": "(Optional) Emacs keystroke sequence shortcuts.\n\nNote Several of these (Ctrl-a, Ctrl-e, Ctrl-k, Ctrl-y) work in the command line, interactive Python and R sessions, and other places as well.\n\n\n\n\n\n\n\n\nSequence\nResult\n\n\n\n\nCtrl-x,Ctrl-c\nClose the file\n\n\nCtrl-x,Ctrl-s\nSave the file\n\n\nCtrl-x,Ctrl-w\nSave with a new name\n\n\nCtrl-s\nSearch\n\n\nESC\nGet out of command buffer at bottom of screen\n\n\nCtrl-a\nGo to beginning of line\n\n\nCtrl-e\nGo to end of line\n\n\nCtrl-k\nDelete the rest of the line from cursor forward\n\n\nCtrl-space, then move to end of block\nHighlight a block of text\n\n\nCtrl-w\nRemove the highlighted block, putting it in the kill buffer\n\n\nCtrl-y (after using Ctrl-k or Ctrl-w)\nPaste from kill buffer (‘y’ is for ‘yank’)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "Submit your solutions on Gradescope and (for problem sets but not assignments) via your Git repository.\n\n\n\nProblem Set\nDate\nTime\n\n\n\n\nPS 1 (HTML) (PDF)\nThursday Sep. 7\n10 am\n\n\nPS 2 (HTML) (PDF)\nFriday Sep. 15\n10 am\n\n\nPS 3 (HTML) (PDF) (code)\nWednesday Sep. 27\n10 am\n\n\nPS 4 (HTML) (PDF)\nMonday Oct. 9\n10 am\n\n\nPS 5 (HTML) (PDF)\nFriday Oct. 27\n10 am\n\n\nPS 6 (HTML) (PDF)\nFriday Nov. 3\n10 am\n\n\n\nProblem set solutions need to follow the rules discussed in Lab 1 (Sep. 1) and documented here.\n\n\n\nAssignment\nDate\nTime\n\n\n\n\nbash shell tutorial and bash shell problems (first 10) (reading some of tutorial before Wed. Aug. 30 recommended); see Details below\nFriday Sep. 1\n10 am\n\n\nregular expression reading and problems; See Details below\nFriday Sep. 8\n10 am\n\n\n\nAssignments will generally be turned in on Gradescope but don’t need to follow the rules for problem set solutions and will be graded complete/incomplete."
  },
  {
    "objectID": "schedule.html#problem-sets-and-assignments",
    "href": "schedule.html#problem-sets-and-assignments",
    "title": "Statistics 243 Fall 2023",
    "section": "",
    "text": "Submit your solutions on Gradescope and (for problem sets but not assignments) via your Git repository.\n\n\n\nProblem Set\nDate\nTime\n\n\n\n\nPS 1 (HTML) (PDF)\nThursday Sep. 7\n10 am\n\n\nPS 2 (HTML) (PDF)\nFriday Sep. 15\n10 am\n\n\nPS 3 (HTML) (PDF) (code)\nWednesday Sep. 27\n10 am\n\n\nPS 4 (HTML) (PDF)\nMonday Oct. 9\n10 am\n\n\nPS 5 (HTML) (PDF)\nFriday Oct. 27\n10 am\n\n\nPS 6 (HTML) (PDF)\nFriday Nov. 3\n10 am\n\n\n\nProblem set solutions need to follow the rules discussed in Lab 1 (Sep. 1) and documented here.\n\n\n\nAssignment\nDate\nTime\n\n\n\n\nbash shell tutorial and bash shell problems (first 10) (reading some of tutorial before Wed. Aug. 30 recommended); see Details below\nFriday Sep. 1\n10 am\n\n\nregular expression reading and problems; See Details below\nFriday Sep. 8\n10 am\n\n\n\nAssignments will generally be turned in on Gradescope but don’t need to follow the rules for problem set solutions and will be graded complete/incomplete."
  },
  {
    "objectID": "schedule.html#quizzes",
    "href": "schedule.html#quizzes",
    "title": "Statistics 243 Fall 2023",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are in-person only.\n\nQuiz 1: Monday, October 23 in class.\n\nReview session Friday October 20 in section.\n\nQuiz 2: Monday, November 20 in class.\n\nReview session Friday November 17 in section."
  },
  {
    "objectID": "schedule.html#project",
    "href": "schedule.html#project",
    "title": "Statistics 243 Fall 2023",
    "section": "Project",
    "text": "Project\nDue date: TBD."
  },
  {
    "objectID": "schedule.html#first-few-weeks-activities-and-assignments",
    "href": "schedule.html#first-few-weeks-activities-and-assignments",
    "title": "Statistics 243 Fall 2023",
    "section": "First few weeks activities and assignments",
    "text": "First few weeks activities and assignments\n\n\n\nWeek\nDay\nDate\nTime\nActivity/Assignment\n\n\n\n\n1\nThursday\n2023-08-24\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nFriday\n2023-08-25\nnoon\nRequired (part of your class participation): class survey\n\n\n\n\n\nnoon\nRequired: office hour time survey\n\n\n\n\n\nnoon\nRequired: Go to github.berkeley.edu and login with your Calnet credentials (that’s all – it will allow me to create your repository)\n\n\n\n\n\nlab (1:00-4:30 pm)\nOptional: Lab 0 help session for software installation/setup and UNIX-style command line basics (see Details below)\n\n\n2\nMonday\n2022-08-28\n10 am\nRequired: read first three sections of Unit 2 (sections before ‘Webscraping’)\n\n\n\n\n\nnone\nOptional: work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n\n\n\n\n\n7:00-8:15 pm\nOptional: Python workshop (see Details below); perhaps worthwhile if you have no Python background and if you couldn’t attend the Aug 16-17 workshop we held\n\n\n\nWednesday\n2023-08-30\n5:00-6:15 pm\nOptional: Python workshop (see Details below); perhaps worthwhile if you have no Python background and if you couldn’t attend the Aug 16-17 workshop we held\n\n\n\nFriday\n2022-09-01\n10 am\nRequired: Bash shell tutorial and exercises (see Details below)\n\n\n\n\n\nlab\nRequired: Lab 1 on using Git and Quarto and problem set submission (see Details below)\n\n\n3\nWednesday\n2022-09-06\n10 am\nRequired: PS1 due on Gradescope and via GitHub commit\n\n\n\n\n\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nFriday\n2022-09-08\n10 am\nRequired: Regular expression tutorial and exercises (see Details below)\n\n\n\n\n\nlab\nRequired: Lab 2 on exceptions and testing"
  },
  {
    "objectID": "schedule.html#notes-on-assignments-and-activities",
    "href": "schedule.html#notes-on-assignments-and-activities",
    "title": "Statistics 243 Fall 2023",
    "section": "Notes on assignments and activities",
    "text": "Notes on assignments and activities\n\nOptional library LaTeX session: I highly recommend (in particular if you are a Statistics graduate student) that you know how to create equations in LaTeX. Even if you develop your documents using Quarto, Jupyter notebooks, R Markdown, etc. rather than LaTeX-based documents, LaTeX math syntax is the common tool for writing math syntax that will render beautifully.\nOptional Lab 0 software/command line help session: (August 25 in lab room) Help session for installing software, accessing a UNIX-style command line, and basic command line usage (e.g., the UNIX basics tutorial). You can show up at any time (unlike all remaining labs). You should have software installed, be able to accesss the command line, and have started to become familiar with basic command line usage before class on Wednesday August 30.\nBash shell tutorial and exercises: (by September 1) Read through this tutorial on using the bash shell. You can skip the pages on Regular Expressions and Managing Processes. Work through the first 10 problems in the exercises and submit your answers via Gradescope. This is not a formal problem set, so you don’t need to worry about formatting nor about explaining/commenting your answers, nor do you need to put your answers in your GitHub class repository. In fact it’s even fine with me if you hand-write the answers and scan them to an electronic document. I just want to make sure you’ve worked through the tutorial. I’ll be doing demonstrations on using the bash shell in class starting on Wednesday August 30, so that will be helpful as you work through the tutorial.\nLab 1: (September 1) First section/lab on using Git, setting up your GitHub repository for problem sets, and using Quarto to generate dynamic documents. Please come only to the section you are registered for given space limits in the room, unless you have talked with Chris and have his permission.\nRegular expression reading and exercises: (by September 8), read the regular expression material in the tutorial on using the bash shell. Then answer the regular expressions (regex) practice problems and submit your answers on Gradescope. This is not one of the graded problem sets but rather an assignment that will simply be noted as being completed or not."
  }
]